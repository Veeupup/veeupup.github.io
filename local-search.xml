<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>An Empirical Evaluation of Columnar Storage Formats - 阅读笔记</title>
    <link href="/2024/12/29/An-Empirical-Evaluation-of-Columnar-Storage-Formats-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/29/An-Empirical-Evaluation-of-Columnar-Storage-Formats-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><a href="https://15721.courses.cs.cmu.edu/spring2024/papers/02-data1/p148-zeng.pdf">An Empirical Evaluation of Columnar Storage Formats</a> 主要对比介绍了 Parquet 和 ORC 两种列存存储格式，通过一个基于真实世界数据集的基准测试，评估了它们的性能和空间效率。</p><h1 id="列存格式的演进"><a href="#列存格式的演进" class="headerlink" title="列存格式的演进"></a>列存格式的演进</h1><ul><li><strong>早期大数据生态系统中的文件格式</strong>：2010 年代初期，大数据生态系统催生了多种开源文件格式。Apache Hadoop 最先引入了两种面向行的格式：<strong>SequenceFile</strong>，以键值对的形式组织；以及基于 JSON 的 <strong>Avro</strong>。</li><li><strong>列式数据库系统的兴起</strong>：与此同时，<strong>C-Store</strong>、<strong>MonetDB</strong> 和 <strong>VectorWise</strong> 等面向列的数据库管理系统（DBMS）开发了高效分析查询处理的基础方法，包括列式压缩、矢量化处理和延迟物化。</li><li><strong>Hadoop 社区采纳列式存储思想</strong>：Hadoop 社区随后从列式系统中借鉴了这些思想，并开发了更高效的格式。</li><li><strong>RCFile 的发布</strong>：2011 年，Facebook&#x2F;Meta 发布了用于 Hadoop 的面向列的格式 <strong>RCFile</strong>。</li><li><strong>ORC 格式的出现</strong>：两年后，Meta 对 RCFile 进行了改进，并发布了基于 PAX（Partition Attribute Across）模型的 <strong>ORC</strong>（Optimized Record Columnar File）格式。</li><li><strong>Parquet 格式的出现</strong>：在 ORC 发布一个月后，Twitter 和 Cloudera 发布了第一个版本的 <strong>Parquet</strong>。Parquet 借鉴了早期列式存储研究的见解，例如 PAX 模型以及 Google Dremel 的记录切分和组装算法。</li><li><strong>Parquet 和 ORC 的发展现状</strong>：此后，Parquet 和 ORC 都成为 Apache 基金会的顶级项目。它们也得到了大多数数据处理平台的支持，包括 Hive、Presto&#x2F;Trino 和 Spark。即使是具有专有存储格式的数据库产品（例如 Redshift、Snowflake、ClickHouse 和 BigQuery）也通过外部表支持 Parquet 和 ORC。</li><li><strong>其他开源列式格式</strong>：华为的 <strong>CarbonData</strong> 是另一种开源列式格式，它提供了内置的倒排索引和列组。但由于其与 Spark 的关系更为密切，之前的研究未能独立评估该格式。最近的研究表明，与 Parquet 和 ORC 相比，CarbonData 的性能较差，并且社区活跃度较低。</li><li><strong>专有列式格式</strong>：许多大型公司在过去十年中开发了自己的专有列式格式。Google 的 <strong>Capacitor</strong> 格式被其许多系统使用，包括 BigQuery 和 Napa。它基于 Dremel 和 Abadi 等人 的技术，这些技术根据工作负载行为优化布局。YouTube 在 2019 年为 Procella DBMS 开发了 <strong>Artus</strong> 格式，该格式支持自适应编码，无需块压缩，并针对嵌套模式的查找时间进行了优化。Meta 的 <strong>DWRF</strong> 是 ORC 的变体，可更好地支持读取和加密嵌套数据。Meta 最近开发了 <strong>Alpha</strong>，以改善机器学习（ML）应用程序的训练工作负载。</li><li><strong>Arrow 格式</strong>：<strong>Arrow</strong> 是一种内存中的列式格式，旨在实现不同应用程序进程之间或库 API 边界之间的高效数据交换，且序列化程度有限或没有序列化。与 Parquet 或 ORC 不同，Arrow 支持随机访问，因此在读取时不需要基于块的解码。由于 Arrow 不适用于长期磁盘存储，因此本文不对其进行评估。</li><li><strong>Lakehouse 趋势</strong>：最近的 lakehouse 趋势导致了格式的扩展，以支持更好的元数据管理（例如，ACID 事务）。代表性项目包括 Delta Lake、Apache Iceberg 和 Apache Hudi。它们添加了一个辅助元数据层，并且不直接修改底层的列式文件格式。</li><li><strong>科学数据存储格式</strong>：还有用于 HPC 工作负载的科学数据存储格式，包括 <strong>HDF5</strong>、<strong>BP5</strong>、<strong>NetCDF</strong> 和 <strong>Zarr</strong>。它们的目标是具有复杂文件结构、类型和组织的异构数据。它们的数据通常是多维数组，不支持按列编码。尽管它们公开了多个语言 API，但由于缺乏列式存储功能，很少有 DBMS 支持这些格式。</li></ul><h1 id="Parquet-和-ORC-的核心特性比较"><a href="#Parquet-和-ORC-的核心特性比较" class="headerlink" title="Parquet 和 ORC 的核心特性比较"></a>Parquet 和 ORC 的核心特性比较</h1><p>这里主要介绍了列式存储格式（如 Parquet 和 ORC）的<strong>核心功能</strong>，并详细对比了这两种格式在不同功能上的设计差异。该部分内容旨在<strong>建立一个用于理解和比较列式存储格式的框架</strong>，为后续的性能评估和未来格式的设计提供基础。</p><p><img src="/parquet-orc-layout.png" alt="parquet orc layout"></p><ul><li><strong>内部布局（Internal Layout）</strong><ul><li><strong>PAX 格式</strong>：Parquet 和 ORC 都采用 <strong>PAX（Partition Attribute Across）</strong> 格式。这种格式首先将表水平划分为行组（Row Group），然后在每个行组内按列存储数据，每个属性形成一个列块（Column Chunk）。这种混合列式布局使得 DBMS 可以使用<strong>矢量化查询处理</strong>，并减轻行组中元组重建的开销。</li><li><strong>逻辑块到物理存储的映射</strong>：虽然 Parquet 和 ORC 的布局相似，但它们在逻辑块到物理存储的映射方式上有所不同。Parquet 使用基于<strong>行数</strong>的行组大小（例如 100 万行），而 ORC 使用固定的<strong>物理存储大小</strong>（例如 64MB）。Parquet 试图保证一个行组内有足够的条目来利用矢量化查询处理，但它可能导致较大的内存占用，尤其是在宽表的情况下。另一方面，ORC 限制了行组的物理大小以更好地控制内存使用，但它可能导致具有大属性的条目不足。</li><li><strong>压缩单元</strong>：Parquet 将其压缩单元映射到最小的区域映射（smallest zone map）。ORC 在调整块压缩算法的性能空间权衡方面提供了灵活性。但是，最小的区域映射和压缩单元之间的不对齐会在查询处理期间增加额外的复杂性。</li></ul></li><li><strong>编码变体（Encoding Variants）</strong><ul><li><strong>轻量级压缩方案</strong>：Parquet 和 ORC 都支持标准的 OLAP 压缩技术，例如字典编码（Dictionary Encoding）、游程编码（Run-Length Encoding，RLE）和位打包（Bitpacking）。</li><li><strong>字典编码</strong>：<strong>Parquet 默认对每个列应用字典编码</strong>，无论数据类型如何，而 <strong>ORC 仅将其用于字符串</strong>。它们都在字典代码上应用另一层整数编码。Parquet 为每个列块的字典大小设置了一个限制（默认情况下为 1MB），当字典已满时，后续值将回退到“plain”（即，无编码）。另一方面，<strong>ORC 计算列的 NDV 比率（即，NDV&#x2F;行数）以确定是否对其应用字典编码</strong>。如果列的 NDV 比率大于预定义的阈值（例如，0.8），则 ORC 将禁用编码。</li><li><strong>整数编码</strong>：对于整数列，Parquet 首先进行字典编码，然后对字典代码应用 RLE 和位打包的混合。如果相同的值连续重复 ≥ 8 次，则使用 RLE；否则，使用位打包。<strong>ORC 的整数编码器使用基于规则的贪婪算法为每个值子序列选择最佳方案</strong>。ORC 的整数编码方案包括 RLE、Delta 编码、位打包和 PFOR 的变体。ORC 的整数编码算法比 Parquet 的更复杂，能够抓住更多的压缩机会，但是<strong>在四种编码方案之间切换会减慢解码过程</strong>。</li></ul></li><li><strong>压缩（Compression）</strong><ul><li><strong>块压缩</strong>：Parquet 和 ORC 默认都启用块压缩。每个格式支持的算法在表1中列出。块压缩算法与类型无关，它将任何数据视为字节流。大多数块压缩算法包含用于配置“压缩级别”的参数，以在压缩率和压缩&#x2F;解压缩速度之间进行权衡。</li><li><strong>Parquet 直接向用户公开这些调整旋钮，而 ORC 为每个算法提供了两个预配置的选项，“针对速度优化”和“针对压缩优化”</strong>。</li></ul></li><li><strong>类型系统（Type System）</strong><ul><li><strong>Parquet 的类型系统</strong>：Parquet 提供了一组最小的原始类型（例如，INT32、FLOAT、BYTE_ARRAY）。Parquet 中所有其他支持的类型（例如，INT8、日期、时间戳）都是使用这些原始类型实现的。</li><li><strong>ORC 的类型系统</strong>：ORC 中的每个类型都有一个单独的实现，具有专用的读取器和写入器。尽管这可以带来更多特定于类型的优化，但会使实现变得臃肿。</li><li><strong>复杂类型</strong>：Parquet 和 ORC 都支持 Struct、List 和 Map，但 Parquet 不像 ORC 那样提供 Union 类型。</li></ul></li><li><strong>区域映射&#x2F;索引（Zone Map &#x2F; Index）</strong><ul><li><strong>区域映射</strong>：Parquet 和 ORC 都包含区域映射和可选的布隆过滤器以启用选择修剪。区域映射包含文件中预定义范围内最小值、最大值和行数。如果区域的值范围不满足谓词，则可以在表扫描期间跳过整个区域。Parquet 和 ORC 都在文件级别和行组级别包含区域映射。</li><li><strong>最小区域映射粒度</strong>：Parquet 中最小区域映射粒度是物理页（即压缩单元），而 ORC 中是表示行数的可配置值（默认情况下为 10000 行）。是否构建最小区域映射在 Parquet 中是可选的。</li><li><strong>Parquet 的 PageIndex</strong>：在 Parquet 的早期版本中，最小的区域映射存储在页面标头中，这导致了大量的随机 I&#x2F;O。在 Parquet 的最新版本（2.9.0）中，通过一个名为 PageIndex 的可选组件解决了这个问题，该组件存储在文件页脚之前，以保存所有最小的区域映射。ORC 将其最小的区域映射存储在每个行组的开头。</li><li><strong>Bloom Filter</strong>：Bloom Filter 在 Parquet 和 ORC 中都是可选的。ORC 中的 Bloom Filter 与最小的区域映射具有相同的粒度，并且它们彼此位于同一位置。但是，Parquet 中的 Bloom Filter 仅在列块级别创建，部分原因是 Parquet 中的 PageIndex（即，最小的区域映射）是可选的。</li></ul></li><li><strong>嵌套数据模型（Nested Data Model）</strong><ul><li><strong>Parquet 的 Dremel 模型</strong>：Parquet 中的嵌套数据模型基于 Dremel。Parquet 将每个原子字段的值（图 3a 中分层模式中的叶节点）存储为单独的列。每个列都与两个相同长度的整数序列相关联，称为重复级别（R）和定义级别（D），以编码结构。R 将值链接到它们对应的“重复字段”，而 D 跟踪“非必需字段”中的 NULL。</li><li><strong>ORC 的长度和存在模型</strong>：另一方面，ORC 采用基于长度和存在的更直观的模型来编码嵌套数据。ORC 将一个布尔列与每个可选字段相关联，以指示值的存在。对于每个重复字段，ORC 包括一个额外的整数列来记录重复长度。</li></ul></li></ul><h1 id="Lessions-Learned"><a href="#Lessions-Learned" class="headerlink" title="Lessions Learned"></a>Lessions Learned</h1><p>实验评估部分总结的经验教训和未来方向，为下一代列式存储格式的设计提供了重要的指导，主要包括以下几点：</p><ul><li><strong>字典编码（Dictionary Encoding）</strong>：<strong>字典编码对于各种数据类型（甚至是浮点数值）都非常有效</strong>，因为大多数真实世界的数据都具有较低的 NDV（不同值的数量）比率。 未来格式应继续积极应用此技术，如 Parquet 所做的那样。</li><li><strong>编码方案的简洁性</strong>：<strong>列式格式的编码方案保持简单至关重要</strong>，以确保具有竞争力的解码性能。未来的格式设计者应注意解码过程中从多个编解码算法中进行选择的性能成本。</li><li><strong>减少块压缩和重量级编码的使用</strong>：在现代硬件上，<strong>查询处理的瓶颈正在从存储 I&#x2F;O 转移到计算 CPU</strong>。除非在特定情况下证明有好处，否则未来格式应限制使用块压缩和其他重量级编码。</li><li><strong>元数据布局的集中化和随机访问优化</strong>：未来格式中的<strong>元数据布局应集中化</strong>，并且对随机访问友好，以更好地支持 ML 训练中常见的宽（特征）表。基本 I&#x2F;O 块的大小应针对高延迟云存储进行优化。</li><li><strong>索引和过滤结构的增强</strong>：随着存储成本的降低，未来格式可以<strong>存储更复杂的索引和过滤结构</strong>，以加快查询处理速度。</li><li><strong>嵌套数据模型与内存格式的亲和性</strong>：嵌套数据模型的设计应与现代内存格式具有亲和性，以减少转换开销。</li><li><strong>支持机器学习工作负载</strong>：常见机器学习工作负载的特性要求未来格式有效地支持<strong>宽表投影和低选择性选择</strong>。 这需要更好的元数据组织和更有效的索引。此外，未来格式应为大型二进制对象分配单独的区域，并采用专门为浮点数设计的压缩技术。</li><li><strong>GPU 解码效率</strong>：未来的格式应考虑使用 GPU 的解码效率。 这不仅需要在文件级别有足够的并行数据块，还需要<strong>编码算法可以并行化</strong>，以充分利用 GPU 线程块内的计算。</li></ul><p>这些经验教训表明，未来的列式存储格式需要<strong>更加注重解码性能、元数据管理、索引和过滤机制，以及对现代硬件和机器学习工作负载的支持</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2025阅读笔记</title>
    <link href="/2024/12/29/2025%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/29/2025%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>今年决定看完一本书就得整理一下读书笔记，博客中只整理最精华的部分。</p><h1 id="控糖革命"><a href="#控糖革命" class="headerlink" title="控糖革命"></a>控糖革命</h1><blockquote><p>最近在健身，减肥，朋友推荐了这本书，马上就把这本书读完了</p></blockquote><ul><li><strong>血糖水平变化越小，健康状况就会越好（使得血糖曲线平稳化）</strong></li><li><strong>最优的吃饭顺序：吃前喝点醋，先吃纤维（蔬菜），再吃蛋白质、脂肪，最后吃淀粉和糖类，吃完锻炼十分钟</strong></li><li>葡萄糖的三类储存位置：糖原（依赖肝脏，存储大概 100g），肌肉（存储 400g），脂肪（果糖只能在脂肪中）</li><li>没有好糖坏糖，糖就是糖，所以 去吃你喜欢的糖（果汁也是糖，纤维素被破坏了）</li></ul><h1 id="鱼不存在"><a href="#鱼不存在" class="headerlink" title="鱼不存在"></a>鱼不存在</h1><blockquote><p>也是朋友推荐的这本书</p></blockquote><p>一本讲斯坦福大学建校校长大卫乔丹的书，是一本不一样的传记与回忆录，整本书从头到尾有一个戏剧性的翻转。</p><p>从前面讲述大卫的成长经历，一开始觉得大卫是一个特别正面的人物，执着于鱼类标本的收集，到成为大学校长走上成功的道路开始。到后来随着作者逐渐深入到他弘扬“优生学”的理念的故事之后，整个故事发生的巨大的翻转。</p><p>我也很喜欢后面作者讲述自己的生活，她一开始从一段失败的婚姻中一蹶不振，随着发现 “鱼不存在” 之后，她的生活也逐渐走向了正轨。</p><p>这里回答一下书里最后提到的几个问题：</p><ol><li><p>你是否对大卫·斯塔尔·乔丹的戏剧性转变感到吃惊？一开始你是不是有点喜欢他？如果真是这样，作者的叙述方式如何影响了你对大卫的理解？为什么作者没有在一开始揭露他那令人不安的行径？</p><ul><li>吃惊。作者的叙述方式属于 欲抑先扬，如果作者从一开始就揭露大卫的罪行，那么我也许就不会读下去这本书，也会认为作者就是大卫的一个憎恨者写的一本诋毁他的书。但是作者前面对于大卫人生中成功部分的描述让我对他产生了兴趣，让我继续有动力读下去……</li></ul></li><li><p>你第一次听到本书书名的时候，是什么感觉？现在，读完整本书之后，你觉得鱼类是否存在？为什么？</p><ul><li>开始完全不知道是什么意思，读的过程也一直纠结为什么作者一直不给答案。。鱼不存在，用作者的一句话来说就是：”他一直担心直觉这一对手过分强大，担心人们不会为了寻求真相跳出舒适圈”。</li></ul></li></ol><p>另外两个比较震惊的事实：</p><ul><li><p>大卫的“优生学”给了希特勒民粹的灵感</p></li><li><p>“鱼”这种生物学分类并不存在</p></li></ul><h1 id="一如既往"><a href="#一如既往" class="headerlink" title="一如既往"></a>一如既往</h1><h1 id="为什么我们要睡觉"><a href="#为什么我们要睡觉" class="headerlink" title="为什么我们要睡觉"></a>为什么我们要睡觉</h1><p>好好睡觉，越来越认识到睡觉的重要性了。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据库论文阅读笔记</title>
    <link href="/2024/12/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>今年的一个计划是用 NotebookLLM 来读完 <a href="https://15721.courses.cs.cmu.edu/spring2024/schedule.html">cmu15-721 spring2024</a> 中的标星论文，新工具带来的生产力的提高真是太强了……</p><p>博客里面就不放太多细碎的笔记内容了，这里就放一个最精华的 NotebookLLM 的总结和一些要点了……</p><h2 id="Lakehouse-A-New-Generation-of-Open-Platforms-that-Unify-Data-Warehousing-and-Advanced-Analytics"><a href="#Lakehouse-A-New-Generation-of-Open-Platforms-that-Unify-Data-Warehousing-and-Advanced-Analytics" class="headerlink" title="Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics"></a><a href="https://15721.courses.cs.cmu.edu/spring2024/papers/01-modern/armbrust-cidr21.pdf">Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics</a></h2><p>这篇论文主要讨论了<strong>Lakehouse</strong>架构，这是一种结合了数据湖和数据仓库优势的新型数据平台。Lakehouse 基于<strong>开放的直接访问数据格式</strong> (如 Parquet 和 ORC)，并为<strong>机器学习和数据科学</strong>提供了一流的支持。这种架构通过<strong>元数据层</strong>实现了 ACID 事务和版本控制等数据管理功能，并利用<strong>缓存、索引和数据布局优化</strong>来提高 SQL 性能。此外，Lakehouse 还支持<strong>声明式 DataFrame API</strong>，以加速 ML 工作负载的数据访问。通过这些技术，Lakehouse 旨在解决传统数据湖和数据仓库的局限性，例如数据陈旧、可靠性问题和对高级分析的有限支持。Lakehouse 为企业提供了更<strong>高效、灵活和经济的数据管理解决方案</strong>。</p><p>下面放一张论文中的好图，讲 Data Platform 的演进过程：</p><ul><li>第一阶段： <ul><li>计算和存储耦合</li><li>数据集快速增长，越来越多非结构化数据集无法处理</li></ul></li><li>第二阶段：<ul><li>将所有原始数据offload到数据湖</li><li>使用开放文件格式（Parquet, ORC 等）</li><li>schema-on-read</li><li>数据质量和治理的问题被推到了下游</li><li>仍然有数据湖中的一小部分数据稍后会被 ETL 到下游数据仓库中，用于最重要的决策支持和 BI 应用</li><li>云数据湖：S3 开始取代 HDFS，持久性高，跨地域复制，成本极低</li><li>当前的四个问题：<ul><li>可靠性</li><li>数据过时</li><li>对高级分析的支持有限</li><li>总体拥有成本</li></ul></li></ul></li></ul><p><img src="/evolution_data_platform.png" alt="数据平台的演进"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2024总结</title>
    <link href="/2024/12/23/2024%E6%80%BB%E7%BB%93/"/>
    <url>/2024/12/23/2024%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>24年快要过完了，得吃吃思考的苦，想想今年又是怎么虚度的，今年是正式工作的第二年，心态相比去年刚毕业时的紧张的感觉，要缓解了很多。</p><h1 id="工作上的感受"><a href="#工作上的感受" class="headerlink" title="工作上的感受"></a>工作上的感受</h1><p>在工作上，自己独立承担了一些事情，能够端到端做一些事情了，这是好事。当前参与的项目从最开始的草台班子到现在逐渐变得比较成熟，这个过程感觉自己学到了很多，从身边很多很棒的同事身上能学到了不少，这里指的不仅仅是在技术能力上的提升，还有更多学到了做事的方式以及视野的开阔。</p><ul><li>从做好一个扎实的 MVP 开始<ul><li>这是我这两年在当前项目里感受最深的一个点：在做大的方向选型的时候，不要一开始就想做的又大又全，好像每个方面都做到了，但是每个方面都做的好像用不了，这里也是问题，那里也是问题，最后会完完全全变成连优化、重构都无法进行的屎山。</li><li>在做严肃工程时，正确做事方法应该是：先做最小功能的集合，确保这个最小功能的集合，除了在设计本身之外，工程上还需要有较为完备的测试手段（unit test、e2e、fuzz等），用 CI&#x2F;CD 来确保这一点，确保有一定的可观测性，确保有 benchmark 的方式和手段，确保有产品做发布、做运维的 SOP 等。</li><li>做上述这些的目的是确保每一个功能都能高质量的发布，代码不至于太快演化成屎山，也能让你睡更多好觉。之后的不论是在该严肃的工程项目上的演化，都能够尽量稳健的走好每一步，而不至于走两步退三步的情况出现。</li><li>感觉是血泪的教训。。。</li></ul></li><li>打破砂锅问到底：搞清楚真实的问题和需求，不一定是最初发现的源头。<ul><li>工作中不免会有一些 adhoc 的需求产生， 这种需求可能是自上而下的，也可能是同级的同事提出的需求。</li><li>最开始我收到这些请求的时候，经常会<strong>直接</strong>从技术上的角度去思考、去回答如何解决，但是面对背后用户真实的需求确完全不了解。举个同事用过的例子：“我能用微波炉烧水吗？”，其实用户也许只是想泡一碗泡面，但是不知道热水器在哪，这种情况下如果直接硬上，就是显得特别滑稽。但如果你深入挖掘一些他人真实的需求，也许你给告诉用户，热水器就在那边，你可以用那边的热水泡面，问题就特别简单的解决了。</li><li>这种方式可以避免非常多 “硬上” 方案的苟合，如果不搞清楚真实需求，“硬上”这种方式不仅仅会让用户感到难受，也会让产品&#x2F;技术提供方感觉到特别别扭。所以往往当你觉得某些事情非常不优雅的时候，也许从一开始的切入点就错了。</li><li>更重要的是，如果某些事情特别难做，或者特别别扭，也许本身就不应该存在，也许这个问题或者需求本身就是一个伪需求。我现在深刻认识到了技术解决问题的局限性，很多事情从其他的方面也许一句话就解决了，但是换成技术“硬上”，就是给自己找罪受。</li></ul></li><li>全面的思考问题，站在其他人的角度思考问题<ul><li>这里不仅仅是指技术问题，也是工作中的各种问题，不过我觉得我现在也做得不是特别好，感觉全面的思考问题更多的是需要从事情中去学习。</li><li>我觉得这里更多的是经验层面上的，很多事情遇到过了，了解过事情可能发生的各种可能性，下次遇到类似的问题你才会多一个角度去思考。</li><li>这点需要不断的学习和反思，不论哪次如果发生意料之外的事情或者发现一些没料想过的新事物，那就是一些新的视角的产生，都能帮助我更好的去思考下一个问题。</li></ul></li><li>公开&#x2F;高效&#x2F;全面的沟通<ul><li>工作里不可能每个人都有完善的信息，一定要尽可能做好上下文的共享，<strong>尽可能采用公开的形式来讨论问题和交流方案</strong>，可以非常大程度降低沟通的复杂度，降低所有人做决策的难度。</li></ul></li></ul><h1 id="收获的好的习惯"><a href="#收获的好的习惯" class="headerlink" title="收获的好的习惯"></a>收获的好的习惯</h1><p>今年稍微拨乱反正，重新开始找回了一些好的习惯。</p><ul><li><p>健身</p><ul><li>今年年初的时候自己已经胖到了一百七十多斤了，一拍照发现自己已经胖成球了，于是开始减肥、健身。</li><li>现在自己回到了一百五十多斤，现在基本上一周四练能够保证，身材也重新开始回来了一些，但是距离健康和自己想要的目标还是有一些距离，明年要继续努力。</li></ul></li><li><p>阅读</p><ul><li>依稀记得23年刚毕业自己完全放飞自我了… 今年开始又开始看了一些书，参见 <a href="https://tanweime.com/2024/12/14/2024%E5%B9%B4%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">2024阅读笔记</a>，现在看书的功利性没有那么强了，不像在学校找工作的时候只看技术书 🤡</li></ul></li><li><p>记账</p><ul><li>其实从毕业开始就开始记账了，今年找到了更好更适合自己的方法，发现自己的消费太多了… 还是得制定预算系统，花钱如流水啊，沪币也确实名副其实……</li></ul></li><li><p>PARA 做笔记</p><ul><li>找到了更好的做笔记方法，这个真值得单独拿出来说，目前自己已经完全迁移到该方法上了</li></ul></li></ul><h1 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h1><p>今年年尾的时候又开始听B哥，陶喆和李宗盛，因为发生了一些意料之外的事情，心态发生了一些变化吧。</p><p>如果说今年在生活中学到的几件事，那就是：</p><ul><li>逃避问题不会解决任何问题，必须勇敢的面对。</li><li>需要坦诚的面对自己，面对自己的内心，不要欺骗自己，不要活在自己的幻想中。</li><li>要能够有允许一切事情发生的勇气（勇气，被讨厌的勇气里提到最多的也就是勇气）。</li></ul><hr><p>不论怎么说，每一年都是我最好的一年，希望明年能更好！</p>]]></content>
    
    
    <categories>
      
      <category>年度总结</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2024年阅读笔记</title>
    <link href="/2024/12/14/2024%E5%B9%B4%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/14/2024%E5%B9%B4%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>今年终于再次开始拾起来看书的念头，看的比较杂，这里整理记录下今年阅读的书单。有一些书是第二次读，发现和第一次读起来的感受完全不一样了，人的人生经历会让你在看待同一件事物的时候会有不同的感受。</p><p>现在自己看书的方法已经发生了一些变化，并不贪图要阅读的量，而是争取让读到的每一本书，自己至少能够从中学习并且在生活中应用一个点，那么算是一次成功的阅读了。</p><p>我会首先回忆自己还能记得的印象最深的点，然后再去翻阅之前的笔记来做好这份总结。</p><p>下面是自己今年的读的一些书</p><ul><li><p>《打造第二大脑》</p></li><li><p>《金钱心理学》</p></li><li><p>《被讨厌的勇气》</p></li><li><p>《蛤蟆先生去看心理医生》</p></li><li><p>《边城》</p></li><li><p>《我与地坛》</p></li><li><p>《投资中最重要的事》</p></li><li><p>《费曼讲物理：入门》</p></li><li><p>《人性的弱点》</p></li><li><p>《10倍法则》</p></li><li><p>《认知觉醒》</p></li><li><p>《纳瓦尔宝典》</p></li></ul><p>除此之外，还有一些书，读到一半发现对自己来说有一些困难，或者是读不下去的书，希望自己以后能再把他们读完</p><ul><li>《奈飞文化手册》</li><li>《逃避自由》</li><li>《硅谷之火》</li><li>《置身事内》</li></ul><h2 id="打造第二大脑"><a href="#打造第二大脑" class="headerlink" title="打造第二大脑"></a>打造第二大脑</h2><p>PARA 笔记法</p><ul><li>P：Project<ul><li>正在从事的各项活动</li></ul></li><li>A: Area<ul><li>长期坚持的事项，例如阅读、健身、理财等</li></ul></li><li>R: Resouces<ul><li>搜集到的一些资料，具有潜在的参考价值的信息</li></ul></li><li>A: Archieved<ul><li>完结或者搁置的项目、领域、资源</li></ul></li></ul><p>有两个点我认为最重要：</p><ul><li>用项目 project 的方式来整理笔记，这样不用思考自己当前的笔记应该放在哪里，避免了分类决策带来的很重的思维负担。</li><li>通过笔记的方式放空自己的大脑，让大脑能够思考更加重要的事情</li></ul><p>每次同时进行的项目数维持在个位数以内，一旦完成，那么就从 projects 移动到 archieved。</p><p>我自己目前已经完全迁移该笔记方法上来了</p><ul><li>PARA + logseq + 周报+ 备份（坚果云&#x2F;github）</li></ul><h2 id="金钱心理学"><a href="#金钱心理学" class="headerlink" title="金钱心理学"></a>金钱心理学</h2><p>已经读了好久了，现在想起来印象最深的几个点就是：</p><ul><li>存钱是财富积累关键因素</li><li>学会知足：保持舒适的生活&#x2F;消费状态就好，不要因为收入的短暂提升而大幅提高消费水平</li><li>自由是财富最大的红利，财富能够给人掌控自己时间的能力</li></ul><p>另外还有一些其他重要的点：</p><ul><li>尾部的胜利：真正的财务成功不需要每一笔投资都胜利，往往是少数的投资决定了大多数的财富</li><li>需要留给自己一些容错的空间</li></ul><h2 id="被讨厌的勇气"><a href="#被讨厌的勇气" class="headerlink" title="被讨厌的勇气"></a>被讨厌的勇气</h2><p>这本书是我读的第一本阿德勒心理学的书籍，是一位工作里很厉害的前辈推荐的书籍，和我之前阅读到的所有其他的心理学书籍都不一样，很感谢这本书在我的心智还不够成熟的时候带给我的力量，让我在不少怀疑自己的时候给我一些心理支持。</p><p>目的论和课题分离给了我很大的面对现实和接纳自我的“勇气”。</p><ul><li><p>阿德勒的心理学是“个体心理学”</p></li><li><p><strong>目的论：所有决定我们自己当前行为的不是过去的“原因”，而是现在的“目的”</strong></p><ul><li>由此推断出心里创伤并不存在、愤怒也是不存在的</li><li>问题不在于“发生了什么”，而在于“如何诠释”</li><li>所有的不幸都是自己选择的</li><li>之所以无法改变，是因为下了无法改变的决心</li></ul></li><li><p>人的一切烦恼都是人际关系的烦恼</p><ul><li>自卑、自我价值的怀疑</li><li>做出改变的最重要的东西是“勇气”</li></ul></li><li><p><strong>课题分离</strong></p><ul><li>不要活在别人的期待中</li><li>每个人都有自己的课题，无法决定其他人的课题</li><li>自由就是被其他人“讨厌”</li></ul></li><li><p>被讨厌的勇气</p><ul><li>有价值就有勇气，有鼓励才有勇气</li></ul></li><li><p>活在当下</p><ul><li>要完全诚实的接纳自我，不对自己撒谎，这也是勇气</li><li>也要有甘于平凡的勇气</li><li>人生没有普遍的意义</li></ul></li></ul><h2 id="蛤蟆先生去看心理医生"><a href="#蛤蟆先生去看心理医生" class="headerlink" title="蛤蟆先生去看心理医生"></a>蛤蟆先生去看心理医生</h2><p>这是一个故事，也和被讨厌的勇气一样，是蛤蟆先生和心理医生苍鹭的十次心理咨询，用对话式的方式讲述了悲伤、自卑的蛤蟆先生逐渐认识到他的坏情绪的来源，以及最终他在心理上的成长与成熟的过程。</p><p>苍鹭先生关于心理学的想法和阿德勒心理学正好相关，认为过去对于人非常重要，将人类的心里状态分为：</p><ul><li><p>三种生活状态</p><ul><li><p>儿童自我状态</p><ul><li><p>“儿童自我状态”是指个体在成长过程中，从童年时期内化而来的情感、冲动和行为模式。这种状态通常表现为天真、好奇、情绪化或叛逆。</p></li><li><p>儿童自我状态分成两种</p><ul><li>自然型儿童：最基本的情绪组成，由童年的所有体验构成</li><li>适应性儿童：面对初始状态，所有婴儿学会如何调整行为来面对其他的事务</li></ul></li><li><p>父母自我状态</p><ul><li>“父母自我状态”是指个体在成长过程中，从父母或其他权威人物那里内化而来的态度、价值观、行为模式和情感反应。这种状态通常表现为一种权威性、指导性或保护性的行为模式。</li></ul></li><li><p>成人自我状态</p><ul><li>指个体以理性、客观、现实的方式处理信息和做出决策的状态。它不带有情感色彩，专注于事实和逻辑。</li><li>（这里和目的论很相似）我们可以选择自己的情绪，是愤怒或者是如何反应</li></ul></li></ul></li></ul></li><li><p>我们对世界的看法是在人生的最初阶段里形成的</p></li></ul><h2 id="边城"><a href="#边城" class="headerlink" title="边城"></a>边城</h2><p>边城这本书，我在上学的时候就读过一次，我这个人是比较感性的人，所以对沈从文笔下的那种纯粹的人与人之间的情感非常向往。</p><p>很喜欢沈从文的文笔，记得高中还买过一本他的散文集，里面好多都是讲吃的东西，写的都特别。尤其记得他笔下的高邮咸鸭蛋，我小时候从来没有吃过咸鸭蛋，后来长大了，记忆里他的故事总是给鸭蛋加分，所以我经常在吃面的时候会买了吃。</p><h2 id="我与地坛"><a href="#我与地坛" class="headerlink" title="我与地坛"></a>我与地坛</h2><p>太喜欢我与地坛这本书，尤其喜欢《幸运设计》这篇文章，读过好多遍，特别当人生不顺或者亦有挫折难受的时候，读读这篇文章，能够给你带来太多的力量。</p><p>铁生是一位伟大的人。</p><p>当他反思自己对于母亲的各种苛责和烦恼时，当他在地坛里消磨对于命运的不公时，当他在医院接受自己最终瘫痪的命运的时候……</p><h2 id="投资中最重要的事"><a href="#投资中最重要的事" class="headerlink" title="投资中最重要的事"></a>投资中最重要的事</h2><p>读的第一本关于价值投资的书，是一个自己做二级市场投资的同学推荐给我的，里面有很多非常经典的价值投资的理论值得实践，并且到今天看来也仍不过时。</p><p>这本书讲了很多投资的意识，我这里只记录下我认为最重要的几个点，其中也有一些合并和简单的归纳。</p><ul><li><p>第二层思维 与 逆向投资</p><ul><li>需要想的更多，考虑未来的多种可能性，成功是简单的反面</li><li>简单理解就是巴菲特所说的：“在别人恐惧时我贪婪，在别人贪婪时我恐惧“</li></ul></li><li><p>成功的投资不在于“买好的”，而在于“买得好”</p><ul><li>无论多好的资产，如果买的价格过高，都会变成失败的投资</li></ul></li><li><p>万物皆有周期 与 钟摆意识</p><ul><li><p>贪婪与恐惧的循环是对风险的态度改变所致</p></li><li><p>极端的市场将会发生逆转，钟摆不会永远只向着一个方向摆动</p><blockquote><p>想起来前两天看到其他人的一句话：市场只有 震荡 和 趋势 两种形态</p></blockquote></li><li><p>投资 与 软件工程一样 都没有银弹</p></li></ul></li><li><p>寻找便宜货</p><ul><li>评估价值与价格，找到那些便宜的投资标的</li></ul></li><li><p>耐心等待机会</p><ul><li>等待投资机会到来，而不是一直追逐</li></ul></li><li><p>要重视运气，投资在很大程度上受到运气支配</p></li></ul><h2 id="费曼讲物理：入门"><a href="#费曼讲物理：入门" class="headerlink" title="费曼讲物理：入门"></a>费曼讲物理：入门</h2><p>这本书是费曼讲物理的全集中的一些节选，没想到我会把这本书给看了。这本书并没有涉及到费曼学习法之类的东西，而是费曼用生活化的语言、每个人都能听得懂的话，讲明白了一些非常复杂的物理原理。</p><h2 id="人性的弱点"><a href="#人性的弱点" class="headerlink" title="人性的弱点"></a>人性的弱点</h2><p>经典，不要被书名所误导，有很多能够学习到的视角。</p><ol><li><strong>不要批评、谴责或者抱怨</strong></li><li><strong>要给他人真诚的、发自内心的赞美</strong></li><li>在他人心中激起强烈的欲望</li><li>要真诚的对他人感兴趣</li><li>微笑</li><li>记住一个人的名字！</li><li><strong>做一个好的听众，鼓励别人谈论自己</strong></li><li>谈论别人感兴趣的话题</li><li><strong>要让别人感觉自己很重要，真诚的这样做</strong></li><li>避免辩论</li><li>永远不要说：你错了</li><li>如果自己错了就马上承认</li><li>立即让别人说“是的，是的”。</li><li>尽量把说话的机会让给别人</li><li>需要想的更多，考虑未来的多种可能性，成功是简单的反面</li><li>让别人感觉这个想法是他们自己想到的。</li><li>要真诚的站在别人的观点上看问题</li><li>开始的时候要表扬别人，衷心的表扬</li><li>间接的让别人注意到自己的错误</li><li>批评别人之前，先说自己的错误</li><li>询问问题，而不是下命令</li><li>给别人留面子</li><li>表扬最小的提高，表扬任何进步。要“诚心诚意地赞扬，并且要不吝赞美之辞。</li><li>给他人一个好的名声，让别人把这个名声当做榜样来学习。</li><li>使用鼓励，让那些错误看起来容易纠正。</li></ol><h2 id="10倍法则"><a href="#10倍法则" class="headerlink" title="10倍法则"></a>10倍法则</h2><p>这本书感觉有点拖沓和重复，但是有一个核心思想但是挺新颖的，挺不错的。</p><ul><li>设置相当于自己预期10倍的目标，然后付出认为完成目标10倍的努力。<ul><li>这样才能让自己付出大量的行动，有一点像：“法乎其上，得乎其中，法乎其中，仅得其下” 的含义</li></ul></li></ul><h2 id="认知觉醒"><a href="#认知觉醒" class="headerlink" title="认知觉醒"></a>认知觉醒</h2><p>这本书比想象的好，没有那么鸡汤，反而有很多观点我感觉到非常认同，是一本我读完开始部分实践的书。这本书不仅仅在抽象的思想层面上提出一些观点，包括消除模糊、专注力等</p><p>整本书最精华的一句话我觉得就是：</p><ul><li>人生就是一场消除模糊的比赛，模糊是一切焦虑的根源<ul><li>只有消除了模糊，才能让你的行动变得干脆果敢，专注的去完成每一件你应该做的事情</li></ul></li></ul><p>另外，作者提到的：“早冥读写跑” 五项也是非常值得学习实践的点。</p><h2 id="纳瓦尔宝典"><a href="#纳瓦尔宝典" class="headerlink" title="纳瓦尔宝典"></a>纳瓦尔宝典</h2><p>又想再看一遍这本书了… 这本书实在值得分享的点太多…，还是只讲讲自己最有收获和感受的点吧。</p><ul><li>积累财富<ul><li>赚钱和工作的努力程度没关系，最重要的是理解和思考</li><li><strong>依靠出租时间是不可能致富的，必须拥有股份，才能实现财务自由</strong></li><li>勇于以个人名义承担商业风险</li><li>找到杠杆：劳动力杠杆、资本杠杆 和 复制边际成本为零的产品</li><li>分清主次，聚焦重点</li><li>如何获得运气：不期而遇的运气、坚持不懈的运气、善于发现好运的运气和打造独特个性的运气</li><li>保持耐心</li></ul></li><li>增强判断力<ul><li>清晰地思考，增强判断力</li><li>学会热爱阅读</li></ul></li><li>学习幸福<ul><li>幸福需要活在当下，需要心境平和</li></ul></li><li>自我救赎<ul><li>做自己，关爱自己，冥想+精神力量</li><li>选择自我成长</li></ul></li><li>哲学<ul><li>生命的意义是一个私人问题，每个人都需要找到自己生命的意义</li></ul></li></ul><p>另外，纳瓦尔还推荐了一些书单，包括上面的费曼讲物理，我就是从这本书中去看的。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2023总结</title>
    <link href="/2023/12/28/2023%E6%80%BB%E7%BB%93/"/>
    <url>/2023/12/28/2023%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>竟然又到了年底，今年是从学校毕业到社会上的第一年，自己的社会身份发生了从学生到打工人的转变，尽管自己对于职场和独立生活已经有一些准备，但还是有些事在预期之外。</p><span id="more"></span><p>先记录下今年在自己身上发生的一些值得记录的事情：</p><ol><li>毕业从北京来到上海独自生活，落户上海</li><li>工作转正了，正式成为打工人</li><li>女朋友从国外回来了，在国内呆了大半年，并且都回家见家长了</li><li>养了一只猫，八戒，肥肥的大橘白</li><li>今年出行距离大大提升，去了好些城市旅行，比起在学校的时候没那么宅了</li></ol><p>在开始写总结之前，<code>daneswdAZS </code> &lt;- 来自己刚才八戒过来给我留下的印记，八戒总是在你干正事的时候冲过来🐱；好了，今年整体的风格或者评价就是大部分事情都在某种程度上的按部就班在预期内生活着，比较遗憾的就是自己并没有踏出太多自己的舒适区做一些很有价值的事情，希望明年在这个方面能做出一些改变。</p><p>下面简单分成生活、生活和学习三个方面来总结记录一下</p><blockquote><p>写这篇总结的时候比较浮躁，感觉最近都很难静下心来，也可能是自己的写作能力不太行，还没想好要写什么东西</p></blockquote><h1 id="流水账"><a href="#流水账" class="headerlink" title="流水账"></a>流水账</h1><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>今年的生活里面主要是完成了下面这些事情</p><ol><li>毕业从北京来到上海独自生活，落户上海</li><li>女朋友从国外回来了，在国内呆了大半年，并且都回家见家长了</li><li>养了一只超大的肥猫橘白八戒</li><li>今年出行距离大大提升，去了好些城市旅行，比起在学校的时候没那么宅了</li></ol><p>从年初开始那个时候在家里，家里发生了一些难过的事情，后来二月底中下旬来到了上海，在公司附近找了个房子安顿下来。刚上班的那段时间，每天去健身节食减肥，一个多月减了十多斤，那个时候效果还是很显著的，后来没坚持节食和健身…还是反弹了。三月份的时候曾X休假，来上海梅开了一段时间，到五月份的时候女朋友终于从国外回来，这个学期她gap了半年，直到24年才会回去，这段时间很快乐，总算是结束了两年多的异国恋。后来也见两次家长，总体也还是比较顺利，虽然因为自己嘴笨导致了一些误会的发生，但是最终没啥大事，还是过来了。</p><p>在下半年的时候，在一些机缘巧合下，领养了现在这只猫咪🐱；事情说来很巧，当时公司楼下开了一个宠物领养店，然后就领养了现在八戒，当时是因为长得搞笑而不是可爱决定领养的，哈哈，后来也证明这个选择是对的。讲讲和猫咪的缘分吧，在最开始领养的时候，每天和八戒斗智斗勇，被猫咪抓了咬了好多次，甚至一度想把他送走，好在后来最终还是过的比较愉快，八戒不再咬人抓人，也顺利晋升为铲屎官。</p><p>另外就是今年出行了好几趟，韩国、成都、南京、西安、广州…… 虽然这些城市的风光本身大差不差，虽然自己不是特别热衷于旅行的一个人，但是出去走走总是感觉舒服很多，主要是从自己的现实生活中抽离出来体验一种新生活的感受，能够暂时从日常生活中换个心态感受世界。</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><p>工作转正了，正式成为打工人。工作的内容不适合聊太多，只能说职场和学校差别非常大，虽然自己之前实习很久，但是正式成为打工人还是不一样，特别是经历了一些人事变动之后……自己的心态也发生了变化，深刻感受到很多事情只有亲身经历才能够明白。</p><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>今年早些时候断断续续看完了 15721，考了证券从业资格证，看完了一些经济学书《货币银行学》、《金融市场学》，</p><p>技术上的学习：</p><ul><li>15721 断断续续把论文过了一遍，对于 AP 的系统</li><li>在家的时候做了 nand2teris 硬件部分，做了 CS144</li><li>给 arrow-datafusion 贡献了一些代码，写了三篇技术文章发在知乎，知乎竟然也有1000关注了</li></ul><p>经济学方面，自己在进入这个行业之前几乎没太多了解，今年断断续续入门了一些：</p><ul><li>过了一些经典教材《货币银行学》、《金融市场学》，建立了基本的经济学框架</li><li>看了 有知有行的投资第一课和一些文章，一个对于投资入门概念比较有帮助的平台</li><li>开始买了些基金感受市场的波动</li></ul><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1><p>看了看自己年初的目标，才发现自己竟然完成了大部分的目标，虽然有些比较浅尝辄止，但是方向上却是没有偏离太多，接下来就是定下明年的一些目标继续去冲吧。</p><p>得多写写文章了，感觉已经完全不会写字了…</p><p>冲！</p>]]></content>
    
    
    <categories>
      
      <category>年度总结</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>树莓派3b搭建openwrt科学上网</title>
    <link href="/2023/05/03/%E6%A0%91%E8%8E%93%E6%B4%BE3b%E6%90%AD%E5%BB%BAopenwrt%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"/>
    <url>/2023/05/03/%E6%A0%91%E8%8E%93%E6%B4%BE3b%E6%90%AD%E5%BB%BAopenwrt%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<p>本文记录下采用树莓派来搭建软路由实现科学上网的过程，中间没有想象的那么顺利…折腾了大概两个小时才弄完，这里记录下整个流程和一些坑。</p><span id="more"></span><h1 id="树莓派刷-openwrt"><a href="#树莓派刷-openwrt" class="headerlink" title="树莓派刷 openwrt"></a>树莓派刷 openwrt</h1><h2 id="下载-openwrt-镜像"><a href="#下载-openwrt-镜像" class="headerlink" title="下载 openwrt 镜像"></a>下载 openwrt 镜像</h2><p>首先去下载固件给树莓派刷系统，下载的地址为：</p><blockquote><p>定制固件下载地址：<a href="https://links.jianshu.com/go?to=https://doc.openwrt.cc/2-OpenWrt-Rpi/1-Download/">https://doc.openwrt.cc/2-OpenWrt-Rpi/1-Download/</a></p><p>作者项目地址：<a href="https://links.jianshu.com/go?to=https://github.com/SuLingGG/OpenWrt-Rpi">https://github.com/SuLingGG/OpenWrt-Rpi</a></p></blockquote><p>目前我使用的型号是 3B+，下载 <code>immortalwrt-bcm27xx-bcm2710-rpi-3-squashfs-sysupgrade.img.gz</code>即可。</p><h2 id="刷写固件"><a href="#刷写固件" class="headerlink" title="刷写固件"></a>刷写固件</h2><p>下载 <a href="https://links.jianshu.com/go?to=https://www.balena.io/etcher/">BalenaEtcher</a>，安装，然后插上TF卡转接器，它会自动检测到USB设备，开始刷系统</p><blockquote><p>我这里遇到了刻录失败的情况，原因是 TF 卡未格式化完全，去这里下载 TF 卡格式化软件并且进行完全格式化</p><p><a href="https://doc.embedfire.com/openwrt/user_manal/zh/latest/User_Manual/quick_start/imageflash.html">https://doc.embedfire.com/openwrt/user_manal/zh/latest/User_Manual/quick_start/imageflash.html</a></p></blockquote><p>刷写完成后，弹出”U盘”，取下TF卡，装到树莓派中，通电等待30秒<strong>（不要插网线）</strong>。</p><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><p>等树莓派启动之后会默认开启一个名叫 <code>OpenWRT</code> 的WIFI，可以直接连接，连接成功之后访问 <a href="http://192.168.1.1/">http://192.168.1.1。</a></p><p>默认账号：<code>root</code>，默认密码：<code>password</code>。进去之后可以先修改密码。</p><p>在 <code>网络 -&gt; 接口</code> 中可以看到当前有一个 LAN 口设置，这是树莓派 3B+ 本身的无线 AP 的设置，也就是最终外部设备访问树莓派网络的接口，进去进行一些设置：</p><ul><li><strong>然后点击 <code>高级设置</code> ，取消勾选 <code>以太网适配器: &quot;eth0&quot;</code>，点击保存与应用。</strong></li></ul><p><img src="/LAN_setting.png" alt="LAN口设置"></p><blockquote><p>除此之外，由于我从另外一个路由器的 LAN 口接出来，本身的网段也是 <code>192.168.0.x</code>，所以这里把 LAN 口的网段改成了 <code>192.168.2.x</code>，避免冲突</p></blockquote><p>随后进行 WAN 的设置，也就是树莓派本身接入网络的设置，这里我是直接从另外一个路由器的 LAN 口接出来的网线，所以直接选择 DHCP 客户端即可。</p><p><img src="/WAN_setting.png" alt="image-20230503213619032"></p><p>到这里 LAN 口和 WAN 口都配置完毕，可以在 web 界面中的 <code>系统 -&gt; TTYD终端</code> 来尝试 ping baidu.com 看是否连接上了 WAN 口；电脑连接 OpenWRT WIFI 看是否能连接以及上网。</p><p>到这里其实应该结束了，但是我发现我这里的 LAN 口和 WAN 之间并没有路由规则，因为两个属于不同的网段，所以我在 iptable 里添加了这么一条规则</p><p><code>iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE</code></p><blockquote><p>来自 chatGPT 的解释：</p><p>这个命令的作用是在Linux系统中使用iptables工具配置NAT(Network Address Translation，网络地址转换)，将私有网络IP地址转换为公有网络IP地址，以实现互联网访问的功能。</p><p>具体来说，这个命令添加了一个POSTROUTING规则，在NAT表中（-t nat），用于转发通过eth0网络接口出去的所有IP数据报（-o eth0）。添加MASQUERADE目标（-j MASQUERADE）意味着源IP地址被掩盖成eth0的IP地址，从而实现了内网地址（如192.168.x.x）到外网地址（如公网IP地址）的映射，这样就可以访问公网。</p></blockquote><p>到此为止树莓派已经能够正常作为一个路由器使用了。</p><h1 id="openclash"><a href="#openclash" class="headerlink" title="openclash"></a>openclash</h1><p>之后就是配置 openclash 的过程，在这个镜像里面已经内置了 openclash，所以不需要安装，如果需要安装参考这里：<a href="https://github.com/vernesong/OpenClash/wiki/%E5%AE%89%E8%A3%85">openclash 安装</a></p><p>进去添加 clash 订阅即可，记得勾选 “在线订阅转换”，才能将订阅链接替换成配置文件。</p><p>但是我到这里又不行了，openclash 启动不了，报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup: failed to run the command `/etc/openclash/clash`: Exec format error<br></code></pre></td></tr></table></figure><p>这里是因为 clash 内核的版本对不上，可以直接尝试在页面中更新</p><p><code>服务 -&gt; openclash -&gt; 插件设置 -&gt; 版本更新</code>，选择已经编译版本为 armv7，点下面的检查并更新，但是又因为网络问题下载不了……难蚌</p><p>然后解决方式是手动下载然后替换掉 <code>/etc/openclash/clash</code> 文件即可，记得 <code>chmod +x clash</code>。</p><p>手动下载的地址为：<a href="https://github.com/vernesong/OpenClash/releases/tag/Clash%EF%BC%8C%E8%AE%B0%E5%BE%97%E8%BF%99%E9%87%8C%E9%80%89%E4%B8%AD">https://github.com/vernesong/OpenClash/releases/tag/Clash，记得这里选中</a> armv7 版本下载即可，之后替换成功了，就能看到成功启动的日志了。</p><h2 id="连接-WAN-LAN"><a href="#连接-WAN-LAN" class="headerlink" title="连接 WAN LAN"></a>连接 WAN LAN</h2><p>我这边设置的 WAN 口和 LAN 口的网络属于两个网段，默认的防火墙配置是无法访问到的，这里需要配置 <code>网络 -&gt; 防火墙</code> 中来自 WAN 的流量都接受，同时设置端口转发规则将</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul><li>basic 参考 <a href="https://www.jianshu.com/p/1d9f45197627">https://www.jianshu.com/p/1d9f45197627</a></li><li><a href="https://github.com/vernesong/OpenClash">https://github.com/vernesong/OpenClash</a></li><li>LAN口和WAN的 route 规则 <a href="https://www.right.com.cn/forum/thread-3218146-1-1.html">https://www.right.com.cn/forum/thread-3218146-1-1.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/451788328">https://zhuanlan.zhihu.com/p/451788328</a></li><li>flash 刷写不成功 <a href="https://doc.embedfire.com/openwrt/user_manal/zh/latest/User_Manual/quick_start/imageflash.html">https://doc.embedfire.com/openwrt/user_manal/zh/latest/User_Manual/quick_start/imageflash.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>折腾</category>
      
    </categories>
    
    
    <tags>
      
      <tag>科学上网</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DuckDB(0): 整体介绍</title>
    <link href="/2023/05/02/DuckDB-0-%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D/"/>
    <url>/2023/05/02/DuckDB-0-%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="DuckDB"><a href="#DuckDB" class="headerlink" title="DuckDB"></a>DuckDB</h1><h2 id="What-is-DuckDB"><a href="#What-is-DuckDB" class="headerlink" title="What is DuckDB"></a>What is DuckDB</h2><p>DuckDB 是一个 In-Process 的 OLAP 数据库，可以理解为 AP 版本的 SQLite，采用 MIT 协议开源，是荷兰 CWI 数据库组的一个项目，学术气息比较浓厚，项目的组织很有教科书的感觉，架构很清晰，所以非常适合阅读学习。</p><h2 id="Why-DuckDB-come-out"><a href="#Why-DuckDB-come-out" class="headerlink" title="Why DuckDB come out"></a>Why DuckDB come out</h2><p><a href="https://www.cwi.nl/en/groups/database-architectures/">CWI 数据库组</a>非常厉害，像 MonetDB、Vectorwise 都是该组出来的项目。所以在该组早期推出过一个 MonetDBLite 的项目用于嵌入式的数据分析，这是一个基于 MonetDB 实现了想要的 In-Process 的 OLAP，这个过程中发现搞边缘计算 AP 分析很有市场，也发现了做嵌入式AP数据库的各种要求和限制，所以才开始做 DuckDB。</p><p>下面是 DuckDB 团队发现对于嵌入式 In-Process OLAP 的一些要求，并尝试解决它们：</p><ul><li>组合 OLAP 和 ETL 的 workload：能够处理 AP workload 的同时不完全牺牲 OLTP 的性能，同时有效支持批量 append 和批量 update。</li><li>传输效率：需要很方便的在 application 和 DBMS 之间传递数据，因为不是所有任务都能在嵌入式数据库中完成，例如机器学习、画图等，所以需要很方便的在 DBMS 和 application 之间传递数据。而由于 In-Process OLAP 的好处是在同一地址空间内，可以非常方便的来传递数据。</li><li>弹性（Resilience）：边缘计算的 OLAP 所在的硬件和服务器级别的硬件差别非常大而且各异，更容易出现硬件问题，嵌入式DBMS要能够检测这些问题并防止数据损坏</li><li>Cooperation：系统需要优雅地适应资源争用（CPU or RAM），由于嵌入式数据库不再是机器的唯一使用者，因此它不能像以前那样持续使用所有硬件，否则会导致底层应用程序资源匮乏</li></ul><h1 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h1><p>先来看一个整体的架构图：</p><blockquote><p>来自 Mark 的 15721 slide</p></blockquote><p><img src="/overview.png" alt="image-20230501182908424"></p><p>DuckDB 各个组件之间的架构非常的 “textbook”，也就是说会把整个数据库分为：<strong>Parser, logical planner, optimizer, physical planner, execution engine，transaction and storage managers</strong>。</p><p>作为嵌入式数据库，DuckDB没有客户端协议接口或服务器进程，而是使用C&#x2F;C++ API进行访问。此外，DuckDB提供了SQLite兼容层，允许以前使用SQLite的应用程序通过重新链接或库 overload 来使用DuckDB。</p><ul><li><p>Parser</p><ul><li>SQL Parser 源自 Postgres SQL Parser</li></ul></li><li><p>Logical Planner </p><ul><li>binder， 解析所有引用的 schema 中的对象（如 table 或 view）的表达式，将其与列名和类型匹配。</li><li>plan generator，将 binder 生成的 AST 转换为由基本 logical query 查询运算符组成的树，就得到了一颗  type-resolved logical query plan。 DuckDB 还保留存储数据的统计信息，并将其作为规划过程中不同表达式树的一部分进行传播。这些统计信息用于优化程序本身，并且也用于防止整数溢出</li></ul></li><li><p>Optimizer</p><ul><li>使用动态规划进行 join order 的优化，针对复杂的 join graph 会 fallback 到贪心算法</li><li>会消除所有的 subquery</li><li>有一组 rewrite rules 来简化 expression tree，例如执行公共子表达式消除和常量折叠。</li><li>Cardinality estimation 是使用采样和<code>HyperLogLog</code> 的组合完成的，这个过程将优化 logical plan。</li><li>physical planner 将 logical plan 转换为 physical plan，在适用时选择合适的实现方式，例如 sort-merge join or hash join。</li></ul></li><li><p>execution engine</p><ul><li>DuckDB 最开始采用了基于 Pull-based 的 <code>Vector Volcano</code> 的执行引擎，后来切换到了 Push-based 的 pipelines 执行方法</li><li>DuckDB 采用了向量化计算来来加速计算，具有内部实现的多种类型的 vector 以及向量化的 operator</li><li>另外出于可移植性原因，没有采用 JIT，因为 JIT引擎依赖于大型编译器库（例如LLVM），具有额外的传递依赖。</li></ul></li><li><p>Transactions: </p><ul><li>DuckDB 通过 MVCC 提供了 ACID 的特性，实现了<a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">HyPer专门针对混合OLAP &#x2F; OLTP系统定制的可串行化MVCC 变种</a> 。该变种立即 in-place 更新数据，并将先前状态存储在单独的 undo buffer 中，以供并发事务和 abort 使用</li></ul></li><li><p>Persistent Storage</p><ul><li>单文件存储</li><li>DuckDB 使用<strong>面向读取优化</strong>的 DataBlocks 存储布局（单个文件）。逻辑表被水平分区为 chunks of columns，并使用轻量级压缩方法压缩成 physical block 。每个块都带有每列的<code>min/max</code> 索引，以便快速确定它们是否与查询相关。此外，每个块还带有每列的轻量级索引，可以进一步限制扫描的值数量。</li></ul></li></ul><p> vector, execution, storage 多做一些介绍，其他部分暂时略过…</p><h1 id="Vectors"><a href="#Vectors" class="headerlink" title="Vectors"></a>Vectors</h1><p>DuckDB 内部是一个 <strong>vectorized push-based model</strong>，在执行过程中，vector 会在各个操作符之间流转，而不是一个个 tuple。</p><p>DuckDB 具有非常多自定义的  vector format，非常类似 Arrow，但是对于执行更加友好，是和 volox team 一起设计的，所以很 volex team 的 vector 有很多相似之处。</p><p>每个 vector 只保存单个类型的数据，这里有 logical array 和实际物理实现的区别，对外的操作符看来就是一致的 logical array。</p><p>可以看到下面举例了四种类型的 vector，各自都有自己的逻辑和物理表达形式。</p><p><img src="/duckdb_vector_1.png" alt="image-20230502113453116"></p><p>而为了统一起来上层访问不同类型的 vector（避免 operator 算子处理不同 vector 时出现组合爆炸等问题），DuckDB 使用了一种统一的访问格式去访问底层不同的物理格式，也就是新增了一个 selection 数组，通过 DATA 和 Selection 去访问一个数据（类似 dictionary vector 结构）：</p><p><img src="/duckdb_vector_unified.png" alt="image-20230502162914316"></p><p>在针对字符串的额存储的时候，DuckDB 针对长短字符串做了不同的格式优化，短字符串直接 inline 存储，长字符串保存 4 byte 的prefix，另外保存一个 8 byte 的 pointer 指向剩下的内容。</p><blockquote><p>保存 4 byte 的 prefix 有利于在比较字符串的时候提前进行比较</p></blockquote><p><img src="/duckdb_string.png" alt="image-20230502163259475"></p><p>作为一款现代的 OLAP， DuckDB 也支持非结构化的类型存储，包括了 <code>struct</code> 和 <code>list</code> 这两种结构的 native 存储，DuckDB 不是直接存储 BLOB 或者 JSON 格式，而是使用已有的 vector 去递归表示两种结构。</p><p>像 Struct 包含了三个 vector 去表达一个 struct 结构，list 通过 offset 和 length 去表达 list 中的每一个元素。</p><p>这样可以直接利用已有的高性能算子去计算 nested type 的计算，而不是单独开发一套新的针对结构化类型的算子。</p><p><img src="/duckdb_nested_types.png" alt="image-20230502163655601"></p><h1 id="Execution"><a href="#Execution" class="headerlink" title="Execution"></a>Execution</h1><p>最开始 DuckDB 采用 pull-based 的执行引擎，每个算子都实现了 <code>GetChunk</code> 算子，非常经典的 valcano 模型。</p><p>在这个模型中，单线程执行是非常自然的，多线程并行的能力不是很好；在 volcano 模型下，DuckDB 实现并行的方式是添加了一个 <code>Exchange Operator</code>，由优化器将执行计算分成多个 partition，每个partition内部的执行仍然是单线程的 pull-based 的执行，不感知并行，通过 <code>Exchange</code> 算子将多个并行执行的算子的结果给组合起来。</p><p><img src="/duckdb_exec_pull_exec.png" alt="image-20230502164422656"></p><p>这么做有不少问题：</p><ul><li>优化器在做 plan explosion 的时候很难找到最好的 plan，因为需要感知并行，来做不同的 partition 切分</li><li>就算找到了合适的 plan 去执行，在真正执行的过程中不同 partition 之间容易出现 data skew 的情况</li><li>另外还有就是中间结果物化的成本，例如在 <code>Exchange Operator</code> 执行的时候需要将下面的执行结果给物化下来</li></ul><p>在 2021 年 DuckDB 切换到了 push-based 的执行引擎，并采用了 Morsel-Driven 的并行实现方式。</p><blockquote><p><a href="https://github.com/duckdb/duckdb/issues/1583">Move to push-based execution model #1583</a></p><p><a href="https://www.youtube.com/watch?v=MA0OsvYFGrc">Push-Based Execution in DuckDB - Mark Raasveldt</a></p></blockquote><p>DuckDB 会将一个 plan 切分成多个 pipeline，每个 pipeline 采用 push-based 的方式进行数据传递和调用。</p><p>DuckDB 在实现 push-based 的执行引擎时，将不同的算子抽象成三种类型 <code>Source</code>，<code>Operator</code> 和 <code>Sink</code>，其 <code>Source</code> 和 <code>Sink</code> 能够感知到全局状态以及并行，中间的每个 <code>Operator</code> 算子都是不感知并行，只是做计算。</p><blockquote><p>可以参照<a href="https://zhuanlan.zhihu.com/p/614907875">这篇文章了解 pipeline</a> </p></blockquote><p><img src="/duckdb_pipeline.png" alt="image-20230502165931398"></p><p>pull-based 的执行的控制流其实隐含在函数调用中（非常灵活和方便），但是 push-based 的执行能够显式的去控制执行流，而不是只靠函数调用，也就是说 push-based 的控制流由 DuckDB 手动控制，有很多好处</p><ul><li>可以进行更多优化，因为了解每个算子的执行状态<ul><li>vector cache，在不同的 operator 之间增加小的 cache，更加 cache friendly 去执行</li><li>scan sharing，每个算子可以将数据发送到多个算子中去，可以有多个 output</li><li>backpressure，因为了解了全局状态，可以设定一个 buffer size，在buffer 不够的时候暂停执行，有了足够空间再次执行</li><li>async IO，当算子执行 blocking IO 操作的时候暂停执行，等有了数据再继续执行</li></ul></li></ul><h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>DuckDB 采用了 single-file block-based 的存储格式，WAL 写到了另外一个文件中，通过 header 来实现了 ACID， 每个 block 是 4K 的大小。</p><p><img src="/duckdb_storage_block.png" alt="image-20230502170753542"></p><p>每张表存储的时候会先水平切分成不同的 row group（每个大约 120K~ rows），每个 row group 内部的存储是列存的格式。</p><p><img src="/duckdb_storage_table.png" alt="image-20230502171223613"></p><p>在每个 row group 内部的列存的时候 DuckDB 使用了压缩技术，可以提升 IO 的速度，同时加速执行（vector 中的一些类似方法）。</p><p>compression 能够让数据变得更小同时加速 query。</p><blockquote><p>有两种类型的压缩：</p><ul><li>General-purpose, 重量级压缩<ul><li>gzip, zstd, snappy, lz4</li><li>寻找 bits 中的 pattern 压缩</li><li>优点<ul><li>实现简单</li><li>压缩率很高，空间占用很小</li></ul></li><li>缺点<ul><li>高压缩率会降低执行时候的速度，CPU 花时间解压</li><li>需要整个解压缩，不能只读取部分或者 seek</li></ul></li></ul></li><li>Special purpose, 轻量级压缩<ul><li>RLE, bitpacking, dictionary, FOR, delta</li><li>寻找 data 中的 pattern</li><li>优点<ul><li>非常快</li><li>在执行的时候能够发现某个 pattern</li></ul></li><li>缺点<ul><li>如果找不到 pattern，那么就不能压缩</li><li>需要实现不同的压缩算法</li></ul></li></ul></li></ul></blockquote><p>DuckDB 团队采用了轻量级的压缩方案，在执行的时候寻找 pattern。</p><p>压缩的粒度会在 row group 中的 column 级别上进行，整个过程分成两步：</p><ol><li>Analyze，找到最合适的压缩方法</li><li>Compress，采用该压缩方法进行压缩</li></ol><p><img src="/duckdb_compression.png" alt="image-20230502171838768"></p><h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><p>这里留下一些坑，以后再学习 DuckDB 的过程中仔细学习再写博客总结。</p><h2 id="Buffer-Manager"><a href="#Buffer-Manager" class="headerlink" title="Buffer Manager"></a>Buffer Manager</h2><p>DuckDB 的 buffer manager 是 lock-free 的（类似<a href="https://db.in.tum.de/~leis/papers/leanstore.pdf">Lean Store </a>），粒度是 256KB 的级别，实现了下面这些功能：</p><ul><li>限制内存使用量</li><li>当计算需要的时候 pin blocks 在内存中</li><li>当不需要的时候 unpin blocks</li></ul><h2 id="Out-Of-Core"><a href="#Out-Of-Core" class="headerlink" title="Out-Of-Core"></a>Out-Of-Core</h2><p>DuckDB 支持在  <strong>larger-than-memory execution</strong>，因为是嵌入式 OLAP，所以需要考虑资源不够的情况。</p><ul><li>streaming engine</li><li>当内存不够的时候从 hash join -&gt; sort merge join ，或者是一些 window 算法</li></ul><p>目标是为了达到优雅的性能下降，避免性能的急剧下降</p><blockquote><p>记得在哪看过这句话：“稳定的慢要比不稳定的快” 更好……</p></blockquote><h2 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h2><p>DuckDB 实现了 ACID 特性的事务，<a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">基于 Hyper 的 MVCC 模型</a>，而且特别为了 vector processing 做了优化，DuckDB 支持到了 snapshot isolation 的隔离级别，采用了乐观的并发控制，如果修改了相同的行，那么就会 abort 当前的 transaction。</p><p><img src="/duckdb_mvcc.png" alt="image-20230502172541708"></p><h2 id="External-Formats"><a href="#External-Formats" class="headerlink" title="External Formats"></a>External Formats</h2><p>DuckDB 还支持了直接从很多其他的格式中进行查询：Parquet, CSV, JSON, Arrow, Pandas, SQLite, Postgres, </p><h2 id="Pluggable-Catalog"><a href="#Pluggable-Catalog" class="headerlink" title="Pluggable Catalog"></a>Pluggable Catalog</h2><p>DuckDB 能够支持直接 attach 不同数据库作为自己的 catalog 执行。</p><h2 id="Pluggable-File-System-HTTP-x2F-Object-Store-Reads"><a href="#Pluggable-File-System-HTTP-x2F-Object-Store-Reads" class="headerlink" title="Pluggable File System + HTTP&#x2F;Object Store Reads"></a>Pluggable File System + HTTP&#x2F;Object Store Reads</h2><p>DuckDB 有一个可插拔的文件系统，能够直接从 HTTP&#x2F;S3&#x2F;Object store 中来做查询。</p><h2 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h2><p>DuckDB 支持很多不同的插件，能够通过 INSTALL 和 LOAD来进行开关，可以使用 shared library 的方式进行加载。</p><p>很多核心特性都是通过插件来实现的，例如：time zone, json, sqlite_scanner 等…</p><h2 id="WASM"><a href="#WASM" class="headerlink" title="WASM"></a>WASM</h2><p>DuckDB 还有一个 WASM 的build，可以直接在浏览器中进行查询。</p><h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><ul><li><a href="https://www.youtube.com/watch?v=bZOvAKGkzpQ&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=22">Mark Raasveldt Talk at CMU 15721</a>, with <a href="https://15721.courses.cs.cmu.edu/spring2023/slides/22-duckdb.pdf">slides</a></li><li>M. Raasveldt, et al., <a href="https://15721.courses.cs.cmu.edu/spring2023/papers/22-duckdb/2019-duckdbdemo.pdf">DuckDB: an Embeddable Analytical Database</a>, in <em>SIGMOD</em>, 2019</li><li>M. Raasveldt, et al., <a href="https://15721.courses.cs.cmu.edu/spring2023/papers/22-duckdb/p23-raasveldt-cidr20.pdf">Data Management for Data Science Towards Embedded Analytics</a>, in <em>CIDR</em>, 2020</li><li><a href="https://github.com/duckdb/duckdb/issues/1583">Move to push-based execution model #1583</a></li><li><a href="https://www.youtube.com/watch?v=MA0OsvYFGrc">Push-Based Execution in DuckDB - Mark Raasveldt</a></li><li><a href="https://zhuanlan.zhihu.com/p/614907875">pipeline执行引擎和一些工程优化</a> </li><li><a href="https://db.in.tum.de/~leis/papers/leanstore.pdf">Lean Store </a></li><li><a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">基于 Hyper 的 MVCC 模型</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>DuckDB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Databricks Photon | Native C++ Query Engine for Lakehouse Systems</title>
    <link href="/2023/04/30/Photon/"/>
    <url>/2023/04/30/Photon/</url>
    
    <content type="html"><![CDATA[<h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><h2 id="Spark-简介"><a href="#Spark-简介" class="headerlink" title="Spark 简介"></a>Spark 简介</h2><p>来自伯克利的高性能和更具表现力的 Hadoop 替代品。</p><ul><li>计算&#x2F;存储分离</li><li>支持对同一数据集进行多次迭代算法。</li></ul><p>使用 Scala 编写，可以在 JVM 上运行。</p><p>最初只支持 low-level 的 RDD API，后来添加了 DataFrame API 以实现更高级别的抽象。</p><h2 id="SHARK-（2013）"><a href="#SHARK-（2013）" class="headerlink" title="SHARK （2013）"></a>SHARK （2013）</h2><p>Facebook的Hive中间件的修改版本，将SQL转换为Spark API程序。</p><p>仅支持在Hive目录中注册的数据文件上运行SQL。Spark程序无法在API调用之间执行SQL。</p><p>Shark依赖于Hive查询优化器，该优化器专为在Hadoop上运行map-reduce作业而设计。</p><ul><li>Spark具有更丰富的本地API功能</li></ul><h2 id="Spark-SQL-（2015）"><a href="#Spark-SQL-（2015）" class="headerlink" title="Spark SQL （2015）"></a>Spark SQL （2015）</h2><p>基于行的SQL引擎原生地嵌入Spark runtime 里面， Spark SQL 将使用基于Scala的 query codegen。</p><ul><li>以原始字节缓冲区为中间结果的内存列式表示</li><li>Dictionary encoding, RLE, bitpacking compressions</li><li>查询阶段之间的内存 shuffle</li></ul><p>DBMS将查询的WHERE子句表达式树转换为Scala AST。然后它编译这些AST以生成JVM字节码。</p><h2 id="JVM-Problems"><a href="#JVM-Problems" class="headerlink" title="JVM Problems"></a>JVM Problems</h2><p>Databricks的 workload 是 CPU bound 的。</p><ul><li>由于 NVMe SSD 缓存和自适应的 shuffle，disk stall 减少了。</li><li>Better filtering to skip reading data</li></ul><p>他们发现很难进一步优化基于JVM的Spark SQL执行引擎：</p><ul><li>对于大于64GB的堆而言，GC会很慢</li><li>针对大型方法，JIT代码生成存在局限性</li></ul><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="DataBricks-Photon（2022）"><a href="#DataBricks-Photon（2022）" class="headerlink" title="DataBricks Photon（2022）"></a>DataBricks Photon（2022）</h2><p>photon 是 databricks 为了增强 spark 计算而写的 Native C++ 计算引擎，通过 JNI 和 JVM 进行结合，实现了spark的向量化处理、微批adaptivity，与已有DBR和spark兼容。</p><p>一些特点：</p><ul><li>Shared-Disk &#x2F; Disaggregated Storage</li><li>Pull-based Vectorized Query Processing</li><li>Precompiled Primitives + Expression Fusion</li><li>Shuffle-based Distributed Query Execution</li><li>Sort-Merge + Hash Joins</li><li>Unified Query Optimizer + Adaptive Optimizations</li></ul><h2 id="Query-Execution"><a href="#Query-Execution" class="headerlink" title="Query Execution"></a>Query Execution</h2><p><img src="/databricks_query_exec.png" alt="image-20230430180259757"></p><blockquote><p>和 Dremel 区别是没有 dedicated shuffle nodes</p></blockquote><h2 id="Photon-Vectorized-Query-Processing"><a href="#Photon-Vectorized-Query-Processing" class="headerlink" title="Photon: Vectorized Query Processing"></a>Photon: Vectorized Query Processing</h2><p>每次在 Photon 操作符上调用 GetNext 都会生成 column batch。</p><ul><li>一个或多个带有 position list vectors的 column vectors。</li><li>每个列向量都包括空位图。</li></ul><p>Databricks：与间接引用相比，“位置列表向量”表现更好。</p><p><img src="/databricks_photon_getnexr.png" alt="image-20230430180733503"></p><p>Photon不支持HyPer风格的操作符融合，以便DBMS可以收集每个操作符的metrics，帮助用户了解查询行为。</p><ul><li>在 pipeline 中对多个操作符进行 vertical fusion。</li></ul><p>相反，Photon的工程师会融合表达式来避免过多的函数调用。</p><ul><li>Horizontal fusion within a single op</li></ul><blockquote><p>相当于某种程度上的编译查询，将多个 primitive operator 组合起来，防止过多的函数调用</p></blockquote><p>下面是 hyper 的 operator fusion：</p><p><img src="/databricks_hyper_operator_fusion.png" alt="image-20230430181023303"></p><p>下面是 vectorwise 的 precompiled primitives</p><p><img src="/databricks_vectorwise_precompiled.png" alt="image-20230430181143547"></p><h2 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h2><p>所有的内存分配都会进入由JVM中的DBR（databricks runtime）管理的内存池</p><ul><li>Single source of truth for runtime memory usage</li></ul><p>因为没有数据统计，所以DBMS在其内存分配方面必须更加动态</p><ul><li><p>Opera 不再将自己的内存 spill out 到磁盘上，而是向 manager 请求更多内存，然后管理员决定释放哪些 operator 的内存</p></li><li><p>简单启发式算法从已分配最少但足以满足请求的操作员中释放内存</p></li></ul><h2 id="Catalyst-Query-Optimizer"><a href="#Catalyst-Query-Optimizer" class="headerlink" title="Catalyst Query Optimizer"></a>Catalyst Query Optimizer</h2><p>Spark SQL 的 Cascades-style query optimizer ，用Scala编写，在pre-defined stages执行转换，类似于Microsoft SQL Server。三种类型的转换：</p><ul><li><em><em>Logical</em> Logical</em>* (“Analysis &amp; Optimization Rules”)</li><li><em><em>Logical</em> Physical</em>* (“Strategies”)</li><li><em><em>Physical</em> Physical</em>* (“Preparation Rules”)</li></ul><h2 id="PHYSICAL-PLAN-TRANSFORMATION"><a href="#PHYSICAL-PLAN-TRANSFORMATION" class="headerlink" title="PHYSICAL PLAN TRANSFORMATION"></a><strong>PHYSICAL PLAN TRANSFORMATION</strong></h2><p>从下往上遍历原始查询计划，将其转换为新的 Photon 特定物理计划。</p><ul><li>New Goal: Limit the number of runtime switches between old engine and new engine.</li></ul><p><img src="/databricks_photon_phy_plan.png" alt="image-20230430181638258"></p><h1 id="ADAPTIVITY"><a href="#ADAPTIVITY" class="headerlink" title="ADAPTIVITY"></a>ADAPTIVITY</h1><h2 id="RUNTIME-ADAPTIVITY"><a href="#RUNTIME-ADAPTIVITY" class="headerlink" title="RUNTIME ADAPTIVITY"></a>RUNTIME ADAPTIVITY</h2><p><strong>Query-Level Adaptivity (更宏观一点)</strong></p><ul><li><p>在每个shuffle 阶段结束时重新评估查询计划决策。</p></li><li><p>类似于我们上一节课讨论的 Dremel 方法。</p></li><li><p>这由 DBR wrapper 提供。</p></li></ul><p><strong>Batch-Level Adaptivity (更微观一点)</strong></p><ul><li>operator 内部的专用代码路径，以处理单个 tuple batch 的内容。</li><li>这是在查询执行期间由 Photon 完成的。</li></ul><h2 id="Spark-Dynamic-Query-Optimization"><a href="#Spark-Dynamic-Query-Optimization" class="headerlink" title="Spark: Dynamic Query Optimization"></a>Spark: Dynamic Query Optimization</h2><p>Spark在阶段开始之前根据前一阶段的观察结果改变查询计划。</p><ul><li>避免优化器使用不准确（或不存在）的数据统计信息做出决策的问题。</li></ul><blockquote><p>大家都学聪明了，刚开始没有统计信息的时候不好做优化，等执行过程中有了统计信息再来做优化…</p></blockquote><p>优化示例：</p><ul><li>Dynamically switch between <strong>shuffle vs. broadcast join</strong>.</li><li>Dynamically <strong>coalesce partitions</strong></li><li>先分配足够数量的 partitions，这个时候某些 partition 可能比较小，等这个过程全部结束，再来合并小的 partitions，</li><li>Dynamically <strong>optimize skewed joins</strong></li></ul><h2 id="PHOTON-BATCH-LEVEL-ADAPTIVITY"><a href="#PHOTON-BATCH-LEVEL-ADAPTIVITY" class="headerlink" title="PHOTON: BATCH-LEVEL ADAPTIVITY"></a>PHOTON: BATCH-LEVEL ADAPTIVITY</h2><p>将ASCII和UTF-8数据分开处理</p><ul><li>ASCII编码的数据始终是1字节字符，而UTF-8数据可以使用1到4字节字符。</li></ul><p><strong>No NULL values in a column vector</strong></p><ul><li>省略检查空向量的分支</li></ul><p><strong>No inactive rows in column batch</strong></p><ul><li>省略位置列表中间查找</li></ul><h1 id="BenchMark"><a href="#BenchMark" class="headerlink" title="BenchMark"></a>BenchMark</h1><p>用 C++ 重写带来的提升太猛了。。当然也不只是语言的功劳……</p><p><img src="/databricks_bench_tpch.png" alt="image-20230430182329828"></p><h1 id="Delta-Lake"><a href="#Delta-Lake" class="headerlink" title="Delta Lake"></a>Delta Lake</h1><p>缺乏统计数据使得对数据湖上的查询优化更加困难。适应性在某些方面有所帮助，但如果DBMS了解数据，它总是可以做得更好。如果有一个存储服务支持增量变化以便于DBMS计算统计信息会怎么样呢？</p><h2 id="Delta-Lake（2019）"><a href="#Delta-Lake（2019）" class="headerlink" title="Delta Lake（2019）"></a>Delta Lake（2019）</h2><blockquote><p>可以看这篇文章<a href="http://tanweime.com/2021/11/17/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-Delta-Lake-High-Performance-ACID-Table-Storage-over-Cloud-Object-Stores2/">Delta Lake</a></p></blockquote><p>Delta Lake 提供了基于对象存储的 structured data incremental ingestion Transactional CRUD接口。</p><p>DBMS将写入记录到到面向 JSON 的日志中。后台工作程序定期将日志转换为Parquet文件（带有计算统计信息）。</p><h2 id="Kudu（2015）"><a href="#Kudu（2015）" class="headerlink" title="Kudu（2015）"></a>Kudu（2015）</h2><p>分布式文件系统中，用于结构化数据文件  low-latency random access 的存储引擎。</p><ul><li>2015年在Cloudera开始，以增强 Impala。</li></ul><p>无SQL接口（必须使用Impala）。仅支持低级CRUD操作。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://15721.courses.cs.cmu.edu/spring2023/slides/20-databricks.pdf">Andy 15-721</a></li><li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/20-databricks/sigmod_photon.pdf">photon paper</a></li><li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/20-databricks/p975-armbrust.pdf">delta lake paper</a></li><li>建议看看：<a href="https://zhuanlan.zhihu.com/p/511400714">论文解读 Photon: A Fast Query Engine for Lakehouse Systems</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pipeline执行引擎以及一些工程优化</title>
    <link href="/2023/04/30/pipeline%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B7%A5%E7%A8%8B%E4%BC%98%E5%8C%96/"/>
    <url>/2023/04/30/pipeline%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B7%A5%E7%A8%8B%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><h2 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h2><p>pipeline 是一种<strong>执行引擎模型</strong>，是通过将复杂的计算链路拆分成多个小部分，通过各种手段来执行 pipeline 中的任务完成高效率的计算。</p><blockquote><p>在 Morsel, Clickhouse, Databend, Datafusion, DuckDB 中对于 pipeline 都有不同程度的实现</p></blockquote><p>pipeline 本质上就是将计算任务抽象成一个 DAG，然后将每个节点抽象为一个 TASK，pipeline 将完成任务之间的调度执行顺序和数据传输。</p><p>下图描述了 clickhouse 中的 pipeline 如何 pipeline 的抽象的：</p><blockquote><p><a href="https://presentations.clickhouse.com/meetup24/5.%20Clickhouse%20query%20execution%20pipeline%20changes/#parallel-execution">来源 Clickhouse Presentation meetup24</a></p></blockquote><p><img src="/image-20230310224134113.png" alt="image-20230310224134113"></p><p>图中表示了一个计算任务之间的有向无环图，也就是一个 pipeline </p><p>整个计算链路串起来叫做 pipeline，图中的每个方框内的算子叫做 processor，可以来处理一个计算任务，Ports 代表 processor 之间的数据连接。</p><p>注意每个 processor 都可以有一个或者多个 input ，可以有一个或者多个输出，也就是 pipeline 的变换是非常灵活的，可以 1 -&gt; 1, N -&gt; 1, 1 -&gt; N。</p><p>这里介绍图中没有的另一个概念：pipeline breaker，所谓 pipeline breaker 就是在多个 pipeline 的执行的时候不能顺畅的执行下去，需要等这个同步的点执行完毕才能继续执行。</p><p>例如一个 hash join 的 build hashtable 阶段，就是一个 pipeline breaker，下面的 pipe2 需要等待 pipe1 的 build hashtable 结束之后才能开始 probe 阶段，就阻碍的 pipe2 的继续执行</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xl"><span class="hljs-function"><span class="hljs-title">pipe1</span>: source1 -&gt;</span> <span class="hljs-function"><span class="hljs-title">project</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">build</span> hashtable -&gt;</span><br>  -&gt;<br>  -&gt;---&gt; transform1<br><span class="hljs-function"><span class="hljs-title">pipe2</span>: source2 -&gt;</span> <span class="hljs-function"><span class="hljs-title">project</span>  ----------------------------&gt;</span> <span class="hljs-function"><span class="hljs-title">probe</span>  ---&gt;</span> transform2<br>---&gt; transform3<br></code></pre></td></tr></table></figure><h2 id="如何更快的进行执行计算任务"><a href="#如何更快的进行执行计算任务" class="headerlink" title="如何更快的进行执行计算任务"></a>如何更快的进行执行计算任务</h2><p>这是一个很大的问题，这里总结一下在计算任务中提高执行计算效率的一些方法：</p><p>要注意要提高计算性能，借用 Andy 课中的总结，核心就是三个点：</p><ol><li>减少指令数量<ol><li>向量化，每条指令处理更多数据</li><li>……</li></ol></li><li>减少每条指令的平均 cycle<ol><li>branchless, make cpu pipeline prediction happy</li><li>……</li></ol></li><li>并行执行<ol><li>将执行<strong>水平切分</strong>，<strong>竖直切分</strong></li></ol></li></ol><p>本文不详细展开了，每个点展开又是一个很大的话题。就是要<strong>让CPU一直转起来，而且要转在更有效率的地方</strong>，而不是画这个时间在调度，function call，memory access上面。</p><p>而一个好的调度程序就是能够通过上面的三种方式来提高执行效率</p><ul><li>1: 通过调度尽可能减少 function call 的指令以及额外的调度</li><li>2: 通过 Numa-aware 的方式减少 memory access 的 cycle</li><li>3: 通过并行手段将完整的计算任务水平与竖直进行切分提高效率</li></ul><h2 id="和-volcano-执行模型的对比"><a href="#和-volcano-执行模型的对比" class="headerlink" title="和 volcano 执行模型的对比"></a>和 volcano 执行模型的对比</h2><p>不同于传统的 valcano 的执行引擎，最大的不同就是 pipeline 具有<strong>显式的计算控制流程</strong>（也就是实现了手动调度任务），而 volcano 执行引擎是通过函数调用来实现隐式的计算控制流。</p><blockquote><p><strong><a href="https://github.com/alexey-milovidov">alexey-milovidov</a></strong> ：The main difference is that volcano has implicit control flow (calculations are controlled by function call stack) and pipeline has explicit control flow (there is query execution graph and external scheduler can select what computation blocks to run in what threads).</p><p><a href="https://github.com/ClickHouse/ClickHouse/issues/34045">https://github.com/ClickHouse/ClickHouse/issues/34045</a></p></blockquote><h1 id="一些工程优化"><a href="#一些工程优化" class="headerlink" title="一些工程优化"></a>一些工程优化</h1><p>由于 pipeline 的本质概念上比较简单，工程上有很多可以优化的点</p><h2 id="batch-execution"><a href="#batch-execution" class="headerlink" title="batch-execution"></a>batch-execution</h2><p> 一个前提：注意这里的 pipeline 执行一般都是以 batch 为单位执行的，和 row-oriented 的执行引擎对比来说</p><p>execution row by row:</p><ul><li>simple</li><li>High overhead</li></ul><p>Batch execution:</p><ul><li>small overhead (control overhead)<ul><li>以 MonetDB 论文中的数据说明，MySQL 真正用于计算的只小于 10% 的时间，更多的时间花在了各种不同的</li><li><img src="/MonetDBx100.png" alt="image-20230310215338663"></li></ul></li><li>vectorized execution</li><li>greater memory consumption</li></ul><h3 id="explict-schedule"><a href="#explict-schedule" class="headerlink" title="explict schedule"></a>explict schedule</h3><p>总的来说，pipeline 将一个大的计算流程表示为一个 <strong>DAG 有向无环图</strong>，图中的每个节点转化为作为 pipeline 计算中的一个个的 task 或者叫 processor, 每个 processor 只负责其中的一个子任务，然后将这些 processor 逻辑上连接起来，所以<strong>如何调度这个 DAG 的执行是影响执行效率很大的一个因素</strong>。</p><ul><li>最朴素的实现里面可以直接将每个 task 抽象出来放到一个线程池里面去跑 或者是 thread per prossor，<strong>不做任何显式或者隐式的调度</strong>，任务之间既不 pull 也不 push，轮到谁跑谁就尝试去跑，如果发现不能继续就 yield，然后尝试，每个线程不断轮询 DAG 中的任务，哪个能够执行就去执行，直到最终 task queue 中没有任务即可停止。<ul><li>优点：实现简单</li><li>缺点：OS or 线程池不知道任务之间的依赖关系，可能产生非常多的调度开销，而且不利于做后续的 numa-aware 优化</li></ul></li></ul><blockquote><p>在 Rust 中也有不少用异步的实现直接把任务丢到 tokio 中去跑，让 tokio 来做调度</p></blockquote><ul><li>另一种方法可以采用 pull-based schedule，同样也放到 线程池中去跑，只不过这个时候的调度顺序从下游节点向上游节点发起，也是隐式的调度（通过 function call）<ul><li>优点：<ul><li>实现简单</li><li>容易可以做一些算子的下推：例如 limit 算子</li></ul></li><li>缺点：<ul><li>仍然有不少的调度开销</li></ul></li></ul></li><li>效率比较高的一种调度就是 push-based schedule，由 bottom-to-top 的执行，上游节点执行完毕就通知相邻的下游 processor 执行，把下游节点放到 task queue 中去，这个时候用于调度的时间会更少，真正执行的时间占比会更高<ul><li>优点：<ul><li>减少非常多无用的调度开销</li><li>以数据为中心计算，刚才算子计算的数据仍然在 cpu cahce 中，效率更高<ul><li>所以很多最开始水平切分最小的 batch 的时候尽可能和 CPU cache 大小相等，这样 cache miss 更少</li></ul></li></ul></li><li>缺点：<ul><li>实现稍微复杂一些</li><li>不容易做一些算子的下推</li></ul></li></ul></li><li>还有一种方式就是用一个显式的 watchdog &#x2F; scheduler 线程来调度，这个线程不做执行，专门来做调度，哪个算子能够执行，应该去哪个线程执行（线程和哪个 CPU 更亲和），都能够安排的明明白白的<ul><li>优点：<ul><li>更精细化的调度，对于各种计算中的 skew（例如一条 pipeline 中的 task 非常重，另一个空闲下来可以将任务调度过去） 能够做非常好的 schedule</li><li>可以对于执行有全局视角，更容易做优化</li></ul></li><li>缺点：<ul><li>额外的调度 overhead</li><li>实现更复杂一点</li></ul></li></ul></li></ul><h2 id="parallel-execution"><a href="#parallel-execution" class="headerlink" title="parallel execution"></a>parallel execution</h2><p>提升执行效率另一个非常重要的点就是并行，在 pipeline 里面的并行分成两个方面的并行：</p><ul><li><p>水平方向</p><ul><li>这里通常指将一个完整的 range 的数据水平切几刀，然后分成多个 pipeline 去执行这一小部分数据，最终再聚合起来，类似 map reduce，每个 pipeline 只有一部分数据</li></ul></li><li><p>竖直方向</p><ul><li><p>用前面 ck 的图举例，对于一个很长的 pipeline，可以同时执行一条 pipeline 上的多个算子，例如</p><p><code>source1 -&gt; transform1 -&gt; transform2 -&gt; transform3 -&gt; ...</code></p></li></ul></li></ul><p>可以同时执行一条 pipe 上的 source1 算子和 transform2 算子，这就是所谓的竖直方向的并行</p><h2 id="NUMA-aware-data-centric"><a href="#NUMA-aware-data-centric" class="headerlink" title="NUMA-aware (data centric)"></a>NUMA-aware (data centric)</h2><p>在现代的多核CPU架构中，由于 NUMA 架构的特点，不同CPU访问不同内存区域的速率有比较大的差异（通过 CPU socket 传输数据）。</p><p>所以希望能够尽可能以数据为中心，将针对同一条 pipeline 中的多个算子的都调度到同一个 CPU 上执行（CPU 亲和性），这样同一条 pipeline 上的多个算子所实际运行的 thread 都尽可能在同一个 CPU 上执行，所访问的数据也都在这部分 CPU 更近的内存中，会有更高执行效率。</p><h2 id="work-stealing-x2F-delay-scheduling"><a href="#work-stealing-x2F-delay-scheduling" class="headerlink" title="work-stealing &#x2F; delay scheduling"></a>work-stealing &#x2F; delay scheduling</h2><p>在 Pipeline 执行的时候，每个 pipe 中的任务执行上可能出现各种情形的 skew，虽然在 pipeline 开始水平切分的时候尽可能将数据进行了平均切分，但是通过每个算子之后，剩余的数据可能不同，例如通过一个 filter 算子之后，pipeline 中的两个 pipe 之间的数据出现了极度不均衡，例如下面这种情况，第一个 pipe 的 filter 之后只有 1% 的数据，那么很快这个pipe 就计算完毕了。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs xl"><span class="hljs-function"><span class="hljs-title">pipe1</span>: source1 -&gt;</span> <span class="hljs-function"><span class="hljs-title">filter</span> -- selectity: 1% ---&gt;</span> <span class="hljs-function"><span class="hljs-title">transform2</span> -&gt;</span> <br><br><br><span class="hljs-function"><span class="hljs-title">pipe2</span>: source2 -&gt;</span> <span class="hljs-function"><span class="hljs-title">filter</span> -- selectity: 99% ---&gt;</span> <span class="hljs-function"><span class="hljs-title">transform2</span> -&gt;</span> <br><br><br><span class="hljs-function"><span class="hljs-title">pipe3</span>:  source3 -&gt;</span> <span class="hljs-function"><span class="hljs-title">filter</span> -- selectity: 50% ---&gt;</span> <span class="hljs-function"><span class="hljs-title">transform2</span> -&gt;</span> <br><br></code></pre></td></tr></table></figure><p>在这种情况下，pipe1 就可以尝试 steal pipe3 中的 task queue 中的任务来执行，不让每个 CPU 闲置，提高 CPU 的利用率。</p><p>working stealing 的一个简单优化是 delay scheduling：有的时候并不需要立即去执行 working stealing，而是稍微等待一会，再去尝试拿任务，这种方式在工程实践上被证明是非常有效的，因为有的时候可能隔壁的 CPU 只是稍微慢了那么一点点，多等一下等ok了。</p><p>但是这个方式也不是绝对的，有的时候，work stealing 机制也会带来副作用，例如在 SAP HANA 中，他们提出，他们使用的机器有 256 nodes，这个时候 work stealing 反而会降低效率，因为需要从跨 numa 的 cpu socket 去更远的内存中拿到数据，work stealing 得不偿失。 </p><h2 id="complication"><a href="#complication" class="headerlink" title="complication"></a>complication</h2><p>编译执行，在不少高性能的 AP 中有实现，例如：MS hekaton，sqlite，presto，pelton 等，实现的方式也是有多种：</p><ul><li>手动改写成 C&#x2F;C++ or 某种 DSL</li><li>JVM-based JIT</li><li>LLVM-based</li></ul><p>在 pipeline 中，当可以将任务表达成一个 DAG 之后，就可以编译生成机器代码执行，不再做显式的解释执行调度了。编译执行的效果很好，但是实现起来也通常比解释执行更加困难。</p><h1 id="一个-naive-的实现"><a href="#一个-naive-的实现" class="headerlink" title="一个 naive 的实现"></a>一个 naive 的实现</h1><p>说了这么多，还没有介绍 pipeline 具体的实现是怎样的，在工业实践中，比较好的实现有</p><ul><li><a href="https://github.com/ClickHouse/ClickHouse/tree/c2611c3ba940cedf7ddc225e805e45fd77c59977/src/QueryPipeline">Clickhouse</a> ，<a href="https://github.com/datafuselabs/databend/tree/main/src/query/pipeline">databend</a> ，<a href="https://github.com/apache/arrow-datafusion/tree/ac5676f74bfac89707642f9221d35899b7c2c321/datafusion/core/src/scheduler/pipeline">datafusion</a>，<a href="https://github.com/duckdb/duckdb/tree/88b1bfa74d2b79a51ffc4bab18ddeb6a034652f1/src/parallel">DuckDB</a></li></ul><p>感兴趣的读者可以自己阅读这些 pipeline 的实现。</p><p>我自己也简单实现一个的版本来学习 pipeline，并没有实现太多先进的特性，只是出于学习理解的目的实现一个基础的版本：<a href="https://github.com/Veeupup/naive-pipeline-execution">https://github.com/Veeupup/naive-pipeline-execution</a></p><h1 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h1><ul><li><p><a href="https://www.cidrdb.org/cidr2005/papers/P19.pdf">MonetDB&#x2F;X100: Hyper-Pipelining Query Execution</a></p></li><li><p><a href="https://15721.courses.cs.cmu.edu/spring2019/papers/14-scheduling/p743-leis.pdf">Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age</a></p></li><li><p><a href="https://presentations.clickhouse.com/meetup24/5.%20Clickhouse%20query%20execution%20pipeline%20changes/">ClickHouse Query Execution Pipeline PPT</a></p><ul><li><p><a href="https://www.bilibili.com/video/BV147411U7A3/?vd_source=2be11642e5a095e9c7f08f3b64cc4b1a">bilibili 搬运对应的演讲</a></p></li><li><p><a href="https://github.com/ClickHouse/ClickHouse/issues/34045">Clickhouse CTO Alexey 解释 pipeline</a></p></li></ul></li><li><p><a href="https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/">By 虎哥 ClickHouse和他的朋友们（4）Pipeline处理器和调度器</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/401658490"># ClickHouse 源码学习之Pipeline执行引擎</a> \</p></li><li><p><a href="https://io-meter.com/2020/01/04/olap-distributed/"><a href="https://io-meter.com/2020/01/04/olap-distributed/">OLAP 任务的并发执行与调度</a></a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
      <tag>执行引擎</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dremel | A Decade of Interactive SQL Analysis at Web Scale</title>
    <link href="/2023/04/30/Dremel/"/>
    <url>/2023/04/30/Dremel/</url>
    
    <content type="html"><![CDATA[<h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><p>google 内部有很多 Data Systems，每当 google 发布了他们系统的论文之后总会出现外部的一些开源版本，因为大家都认为 google 很成功，</p><p>NoSQL：</p><ul><li>MapReduce, 2004 -&gt; Hadoop, Spark</li><li>BigTable, 2005 -&gt; HBase, Accumulo, Hypertable</li><li>Chubby, 2006 -&gt; Zookeeper, etcd</li></ul><p>SQL:</p><ul><li>Megastore, 2010</li><li>Vitness, 2010 -&gt; Vitness, Planetscale</li><li>Dremel, 2011 -&gt; Drill, Impala, Dremio</li><li>Spanner, 2011 -&gt; CockroachDB, TiDB</li><li>F1, 2013</li><li>Mesa, 2014</li><li>Napa, 2021</li></ul><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>最初在2006年作为一个 side-project 开发，用于分析从其他工具生成的数据文件。</p><ul><li><p>“interactive”目标意味着他们希望支持对原位数据文件进行即席查询。</p></li><li><p>第一版不支持 Join 操作。</p></li></ul><p>在2010年代后期重写为基于GFS的共享磁盘架构。</p><p>2012年发布为公共商业产品（BigQuery）。</p><h2 id="In-SITU-Data-Processing"><a href="#In-SITU-Data-Processing" class="headerlink" title="In-SITU Data Processing"></a>In-SITU Data Processing</h2><p>在共享存储（例如对象存储）中执行对数据文件的查询，以其原始格式进行操作，而无需首先将其 Ingest 到DBMS（即托管存储）中。</p><ul><li>这就是人们通常所说的 Data lake。</li><li>data lakehouse 是位于所有这些之上的DBMS。</li></ul><p>目标是减少开始分析数据所需的准备时间。</p><ul><li>用户愿意牺牲查询性能来避免重新编码&#x2F;加载数据文件。</li></ul><h2 id="一些特性"><a href="#一些特性" class="headerlink" title="一些特性"></a>一些特性</h2><ul><li>Shared-Disk &#x2F; Disaggregated Storage</li><li>Vectorized Query Processing</li><li>Shuffle-based Distributed Query Execution</li><li>Columnar Storage<ul><li>Zone Maps &#x2F; Filters</li><li>Dictionary + RLE Compression</li><li>Only Inverted Indexes</li></ul></li><li>Hash Joins Only</li><li>Heuristic Optimizer + Adaptive Optimizations</li></ul><h2 id="Query-Execution"><a href="#Query-Execution" class="headerlink" title="Query Execution"></a>Query Execution</h2><p>DBMS将逻辑计划转换为包含多个并行任务的 stage（pipelines）。</p><ul><li>每个任务必须是确定性和幂等的，以支持重新启动。</li></ul><p>根节点（协调器）检索批处理中目标文件的所有元数据，然后将其嵌入查询计划中。</p><ul><li>避免了数千个工作进程同时访问分布式文件系统获取元数据。避免对 GFS 的压力太大</li><li>后面也提到可以通过合并成大文件来减少小文件的碎片</li></ul><p><img src="/./dremel_query_exec.png" alt="image-20230430130519564"></p><h1 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h1><p>使用专用的节点从每个阶段传输中间结果的生产者&#x2F;消费者模型。</p><ul><li>工作节点将输出发送到 shuffle  节点。</li><li>shuffle  节点在内存中以哈希分区方式存储数据。</li><li>下一阶段的工作节点从 shuffle  节点检索其输入。</li></ul><p>如果需要， shuffle  节点会将此数据存储在内存中，并仅溢出到磁盘存储。</p><p><img src="/./dremel_shuffle_pic.png" alt="image-20230430130653795"></p><p> shuffle  阶段代表查询生命周期中的检查点，协调器确保所有任务都已完成。</p><p><strong>Fault Tolerance &#x2F; Straggler Avoidance</strong>：</p><ul><li>如果一个工作节点在DDL内没有生成任务结果，则协调器会推测性地执行冗余任务。</li></ul><p><strong>Dynamic Resource Allocation</strong>：</p><ul><li>根据阶段输出的大小，扩大&#x2F;缩小下一阶段的工作节点数量。</li></ul><h1 id="Query-Optimization"><a href="#Query-Optimization" class="headerlink" title="Query Optimization"></a>Query Optimization</h1><p>我们讨论了查询优化器如何依赖从数据中提取的统计信息导出的成本模型。但是如果没有统计信息，DBMS如何优化查询呢？</p><ul><li><p>DBMS从未见过的数据文件。</p></li><li><p>来自其他DBMS（Connector）的查询API。</p></li></ul><p>Dremel的优化器采用分层方法，使用基于规则和成本的优化器传递来生成初步物理计划以开始执行。</p><ul><li>谓词下推、星型模式约束传播、主&#x2F;外键提示、连接顺序等规则。</li><li>成本优化仅针对DBMS具有统计信息可用（例如，物化视图）的数据进行。</li></ul><p>为避免坏成本模型估算带来的问题，Dremel使用动态查询优化</p><h2 id="Dynamic-QUERY-OPTIMIZATION"><a href="#Dynamic-QUERY-OPTIMIZATION" class="headerlink" title="Dynamic QUERY OPTIMIZATION"></a>Dynamic QUERY OPTIMIZATION</h2><p>Dremel在阶段开始之前根据前一阶段的观察结果更改查询计划。</p><ul><li>避免优化器使用不准确（或不存在）的数据统计信息做出决策的问题。</li></ul><p>优化示例：</p><ul><li>更改 stages 中 work nodes 的数量。</li><li>在Shuffle和Broadcast Join之间切换。</li><li>更改物理运算符实现方式。</li><li>Dynamic repartitioning。</li></ul><h2 id="Dynamic-Repartition"><a href="#Dynamic-Repartition" class="headerlink" title="Dynamic Repartition"></a>Dynamic Repartition</h2><p>Dremel动态负载平衡并调整中间结果分区以适应 data skew。DBMS检测到 shuffle  分区过于满时，然后指示工作人员调整其 partition 方案。</p><p><img src="/./dremel_repartition.png" alt="image-20230430131202943"></p><h1 id="Storage-Format"><a href="#Storage-Format" class="headerlink" title="Storage Format"></a>Storage Format</h1><p>DBMS依赖于Google的分布式文件系统（Colossus）来扩展存储容量。</p><p>依赖于Capacitor的列编码方案，用于嵌套关系和半结构化数据。</p><ul><li>可以将其视为没有速度缓慢问题的JSON &#x2F; YAML。</li><li>Capacitor还提供了具有基本过滤功能的访问库，embedd 的一个 mini SQL 处理器，可以做一些 filter。</li><li>类似于Parquet vs. Orc格式。</li></ul><p>重复和定义字段嵌入在列中，以避免检索&#x2F;访问祖先属性。</p><p><img src="/./dremel_columnar_format.png" alt="image-20230430123814485"></p><blockquote><p>可以看这篇文章了解具体的结构：</p><ul><li><a href="https://www.kancloud.cn/digest/in-memory-computing/202157">https://www.kancloud.cn/digest/in-memory-computing/202157</a></li><li><a href="https://www.kancloud.cn/digest/in-memory-computing/202158">https://www.kancloud.cn/digest/in-memory-computing/202158</a></li></ul></blockquote><h2 id="SCHEMA-REPRESENTATION"><a href="#SCHEMA-REPRESENTATION" class="headerlink" title="SCHEMA REPRESENTATION"></a>SCHEMA REPRESENTATION</h2><p>Dremel的内部存储格式是自描述的</p><ul><li>数据库管理系统需要理解文件中包含什么，所有信息都在文件中。</li></ul><p>但是每当DBMS想要读取一个文件时，它必须解析该文件的嵌入式模式。</p><ul><li>表可以有数千个属性。大多数查询只需要属性子集。</li></ul><p>DBMS以列格式存储模式，以减少检索元数据时的开销</p><h1 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h1><p>在2010年代初期，谷歌内部的许多DBMS项目都有自己的SQL方言。GoogleSQL项目统一了这些冗余的努力，构建了数据模型、类型系统、语法、语义和函数库。开源版本：ZetaSQL。</p><p>使用这些 SQL 的项目有 cloud spanner 等。</p><blockquote><p>The conventional wisdom at Google was “SQL doesn’t scale”, and with a few exceptions, Google had moved away from SQL almost completely. In solving for scalability, we had given up ease of use and ability to iterate quickly.</p><p>Dremel was one of the first systems to reintroduce SQL for Big Data analysis.Dremel made it possible for the first time to write a simple SQL query to analyze web-scale datasets. Analysis jobs that took hours to write, build, debug, and execute could now be written in minutes and executed in seconds, end-to-end, allowing users to interactively write, refine and iterate on queries. This was a paradigm shift for data analysis. The ability to interactively and declaratively analyze huge datasets, ad hoc, in dashboards, and in other tools, unlocked the insights buried inside huge datasets, which was a key enabler for many successful products over the next decade</p><p>Dremel 开始重新拥抱 SQL 的怀抱并且统一了 Google 的 SQL 前端。</p></blockquote><h1 id="Serverless-Computing"><a href="#Serverless-Computing" class="headerlink" title="Serverless Computing"></a>Serverless Computing</h1><p>Dremel是提供弹性、多租户和按需服务的先驱之一，现在被广泛称为无服务器。</p><blockquote><p>感觉 Snowflake 相似的云数仓都学习了 Dremel 的这种思想</p></blockquote><h2 id="Serverless-roots"><a href="#Serverless-roots" class="headerlink" title="Serverless roots"></a>Serverless roots</h2><p>大多数数据仓库（如IBM Netezza、Teradata和Oracle产品）在Dremel概念形成之时部署在专用服务器上，主要遵循数据库管理系统的模式。MapReduce和Hadoop等大数据框架采用了更灵活的部署模式，利用虚拟机和容器，但仍需要单租户资源配置即每个用户一个作业。</p><p>显然，在Google支持交互式低延迟查询和现场分析，并同时扩展到数千个内部用户且成本较低的情况下，只有通过提供按需资源配置的多租户服务才能实现。最初我们利用三个核心思想来实现无服务器分析：</p><ol><li><p>解耦：解耦计算、存储和内存允许按需缩放并独立共享计算而不受存储影响。因此它可以以更低的成本适应使用情况。正如第3节所述，Dremel从2009年开始将磁盘存储与计算资源解耦，并于2014年最终添加了解耦内存。</p></li><li><p>容错性和可重启性：Dremel查询执行是基于底层计算资源可能缓慢或不可用这一假设构建的，使得工作者固有地不可靠。这种假设对查询运行时和调度逻辑有着强烈的影响：</p><ul><li>查询中的每个子任务必须是确定性和可重复的，以便在失败时只需重新启动另一个工作者上的一小部分工作。</li><li>查询任务调度程序必须支持将多个相同任务副本分派到其他不响应的工作者上。</li></ul></li></ol><p>因此，这些机制使得调度逻辑可以轻松地通过取消和重新安排子任务来调整为查询分配的资源量。</p><ol start="3"><li>虚拟调度单元：Dremel调度逻辑设计为使用称为插槽（slots） 的计算和内存抽象单位而不是依赖于特定类型和形状的机器。这对容器导向Borg计算环境是一个很好匹配模型，该环境支持灵活资源配置形状。这些虚拟调度单元允许解耦服务部署、容器和机器形状以及客户可见资源配置与其之间关系，并继续成为BigQuery中资源管理核心客户可见概念。</li></ol><p>原始Dremel论文中采用了这三种思想，在许多无服务器数据分析系统中成为构建块。行业和学术界广泛采用了解耦技术。Snowflake等其他提供商也采用了虚拟资源单元。在许多领域，行业已经趋于数据湖架构，使用弹性计算服务“按需”分析云存储中的数据。此外，许多数据仓库服务（如Presto、AWS Athena和Snowflake）也采用了按需分析或自动缩放作为无服务器分析的关键因素，导致许多企业选择云而不是本地系统。</p><h2 id="5-2-Evolution-of-serverless-architecture"><a href="#5-2-Evolution-of-serverless-architecture" class="headerlink" title="5.2 Evolution of serverless architecture"></a>5.2 <strong>Evolution of serverless architecture</strong></h2><p>Dremel继续发展其无服务器能力，使它们成为Google BigQuery的关键特征之一。原始Dremel论文中的一些方法演变成了下面描述的新思想，将无服务器方法提升到了一个新水平。</p><ul><li><p><em>Centralized Scheduling</em>。2012年，Dremel转向集中式调度，这允许更精细的资源分配，并开放了预留功能，即为特定客户分配Dremel处理能力的一部分。 集中式调度取代了原始论文中负责在中间服务器之间分配资源的“dispatcher”。 新调度程序使用整个群集状态来进行调度决策，从而实现更好地利用和隔离。</p></li><li><p>*Shufflfle Persistence Layer.*。在2010年论文发表后引入了Shuffle和分布式连接功能。 在最初的 shuffle 实现之后，架构演变为允许解耦查询不同阶段的调度和执行。 使用 shuffle 结果作为查询执行状态检查点时，调度程序具有动态抢占 work node  以减少资源配置以适应计算资源受限时其他工作负载需求等灵活性。</p></li><li><p>*Flexible Execution DAGs.*。原始论文描述了图3所示系统架构。 固定执行树对于聚合运算效果良好，但随着Dremel的发展，固定树对于更复杂的查询计划并不理想。 通过迁移到集中式调度和Shuffle持久层，架构以以下方式改变：</p><ul><li><p>查询协调器是接收查询的第一个节点。它构建查询计划，可以是查询执行树（也称为阶段）的DAG，并使用由调度程序提供给它的 work node  编排查询执行。</p></li><li><p>work node  被分配为没有预定义结构的池。一旦协调员决定执行DAG形状，就会向 work node  发送准备好执行本地查询执行计划（树）。叶子级别上来自存储层读取数据并写入Shuffle持久层；其他级别上来自&#x2F;到Shuffle持久层读取和写入数据。完成整个查询后，在Shuffle持久层中存储最终结果，并将其发送给客户端。</p></li><li><p>考虑图4中所示的示例，该示例说明了在Wikipedia表格上运行top-k查询时如何进行：</p></li><li><p><img src="/./dremel_shuffle_based_execution.png" alt="image-20230430132927022"></p></li><li><p>阶段1（叶子）中的工作人员从分布式存储器中读取数据、应用过滤器、局部预聚合数据然后按语言字段进行哈希分区 shuffle 。</p></li><li><p>由于数据按聚合键 shuffle ，因此阶段2中的工作人员可以进行最终的GROUP BY聚合，然后按不同键排序、截断限制并将结果发送到下一个阶段。</p></li><li><p>在第3个阶段中只有一个 work node  ；它从Shuffle持久层读取输入，进行最终排序和截断，并将结果写入Shuffle层。查询协调器从Shuffle持久层中读取最终的100条记录，并将其发送给客户端。</p></li><li><p>任何Dremel查询（例如上面提供的示例）都可以在任意数量的工作人员上执行，范围从1到数万名工作人员。 Shuffle持久层提供了这种灵活性。</p></li></ul></li><li><p>*Dynamic Query Execution.*。基于数据形状，查询引擎可以应用多种优化。例如，考虑选择连接策略，如广播与哈希连接。广播连接不需要在连接探测端对数据进行 shuffle ，因此速度可能更快，但只有当构建侧足够小以适合内存时才能使用广播。</p><ul><li>通常，在查询计划期间获得准确的基数估计是困难的；众所周知，在联接中错误会指数级传播[27]。Dremel 选择了一条路径，在查询执行过程中根据收集到的统计信息动态更改查询执行计划。这种方法是通过 shuffle 持久层和由查询协调器进行集中式查询编排实现的。在广播与哈希连接方面，Dremel 将从哈希连接开始，并在两侧都进行 shuffle 处理，但如果一侧完成得很快并且低于广播数据大小阈值，则 Dremel 将取消第二次 shuffle 并改为执行广播连接。</li></ul></li></ul><h1 id="其他优化"><a href="#其他优化" class="headerlink" title="其他优化"></a>其他优化</h1><ul><li><em>Stand-by server pool</em>。通过分布式SQL执行引擎，可以启动一个系统并准备好立即处理提交的查询。这消除了用户编写自己的MapReduce或Sawzall作业时存在的机器分配、二进制复制和二进制启动延迟。</li><li><em>Speculative execution</em>。当数百台机器处理一个查询时，最慢的工作者可能比平均值慢一个数量级。如果没有任何补救措施，则最终效果是用户端到端查询延迟高达一个数量级。为解决这个问题，Dremel将查询拆分成数千个小任务，在完成任务时每个工作者都可以接受任务。通过这种方式，慢速机器处理较少的任务而快速机器则处理更多的任务。此外，为了对抗查询执行结束时长尾延迟，Dremel可以针对滞后者发出重复任务，并降低总体延迟时间。因此性能成为可用资源总量而不是最慢组件功能。</li><li><em>Multi-level execution trees</em>。在几秒钟内使用数百台计算机来处理单个查询需要协调性能 。 Dremel使用具有顶部根服务器、中间服务器和叶子服务器的树形结构来解决这个问题。执行从根到叶子，然后返回。该模型最初是从Google的搜索借鉴而来。它很好地并行化了请求的分派和查询结果的组装。</li><li><em>Column-oriented schema representation</em>。Dremel的存储格式被设计为自描述，即数据分区存储嵌入式模式。在Google使用的模式通常包含数千个字段。解析完整模式可能比读取和处理来自分区的数据列更耗时。为了解决这个问题，Dremel内部架构表示本身以列格式存储。</li><li>*Balancing CPU and IO with lightweight compression.*。使用列格式使压缩更有效，因为相似值（单个列的所有值）按顺序存储。这意味着必须从存储层读取较少字节，进而减少查询延迟时间 。另一方面 ，解压数据需要CPU周期 ，因此涉及越多 的 压缩 ， CPU成本就越高 。关键是选择一个平衡数据大小减小与CPU解压缩成本之间关系 的 压缩方案 ， 以便既不会出现CPU瓶颈也不会出现IO瓶颈。</li><li><em>Approximate results.</em> 。许多分析不需要100％的准确性，因此提供处理top-k和count-distinct的近似算法可以降低延迟。Dremel使用一次通过算法，这些算法与多层次执行树结构配合良好。此外 ， Dremel允许用户指定在返回结果之前要处理多少数据百分比 。由于滞后效应，在处理了98％的数据后返回结果已被证明可以将延迟时间提高2-3倍。</li><li>*Query latency tiers.*。为了在共享服务器池中实现高利用率，Dremel必须本地支持多个用户同时发出多个查询。由于数据大小范围广泛，有些查询可以在几秒钟内完成，而其他查询可能需要数十秒钟的时间。为确保“小”查询保持快速，并且不会被具有“大”查询的用户阻塞，Dremel在中间服务器上使用调度程序公平地安排资源。调度程序需要能够抢占部分查询处理以允许处理新用户的查询，以避免先前的用户通过运行数百个并发查询来垄断资源的情况。即使是来自单个用户的查询也可能需要不同的优先级，例如支持交互式仪表板与执行每日ETL管道作业等任务。</li><li>*Reuse of fifile operations.*。对于一个请求，在几秒钟内处理成千上万个文件将给分布式文件系统带来巨大负载压力。这实际上可能成为实现低延迟性能瓶颈所在之处：当数千台Dremel工作者向文件系统主节点发送元数据和向块服务器发送打开和读取操作时就会产生此问题。 Dremel采用了一些技术解决了这个问题：最重要的技术是通过从根服务器批量获取元数据并将其传递到叶服务器进行数据读取来重用从文件系统获得的元数据。另一种技术是创建更大的文件，以便可以通过较少的文件表示相同的表，从而减少了文件元数据操作。</li><li>*Guaranteed capacity.*。在第5节中引入集中式调度程序时介绍了预留概念，这也有助于提高延迟性能。例如，客户可以预留一些容量，并仅将该容量用于对延迟敏感的工作负载。当未充分利用保证容量时，这些资源可供其他人使用；但是当请求这些资源时，则立即授予给客户使用。 Dremel工作者使用自定义线程调度程序，它会立即重新分配CPU以执行已预订任务并暂停非预订任务。</li><li>*Adaptive query scaling.*。在第5节描述的灵活执行DAGs是改善不断增长和多样化工作负载下延迟性能的重要组成部分。根据查询计划为每个查询单独构建执行DAG可能很关键：考虑全局聚合（例如COUNT或SUM）：对于固定聚合树结构而言，在多个中间级别上需要进行多次跳转处理此类查询；但是采用灵活DAGs则无需超过两个聚合级别——叶子级别聚合输入并生成每个文件一个记录，而顶级则执行最终聚合。相反，考虑top-k查询（即ORDER BY … LIMIT）：叶子阶段中的每个工作者都会产生许多记录。在具有大量输入的单个节点上进行最终聚合将成为瓶颈。因此，为了处理这种查询，Dremel动态构建一个聚合树，其深度取决于输入大小。</li></ul><h1 id="Other-Systems"><a href="#Other-Systems" class="headerlink" title="Other Systems"></a>Other Systems</h1><p>自2011年VLDB论文以来，有一些数据库管理系统项目是Dremel的副本或灵感来源。</p><ul><li>Apache Drill<ul><li>Drill是一个基于Hadoop的Dremel开源实现。该项目始于2012年在MapR公司。支持通过Janino嵌入式Java编译器进行查询代码生成。惠普企业宣布他们将不再支持Drill的开发工作，时间为2020年。</li></ul></li><li>Dremio<ul><li>基于Apache Arrow，灵感来自Dremel的开源&#x2F;商业DBMS。由CMU校友于2015年开始。</li><li>利用用户定义的物化视图（“反射”）加速对外部数据文件的查询执行。</li><li>还依赖于基于Java的codeghen和向量化。</li></ul></li><li>Apache Impala、<ul><li>Impala是受Dremel启发的另一个分布式文件系统上执行查询的DBMS。</li><li>由前Google数据库人员于2012年在Cloudera开始开发。</li><li>支持过滤器和解析逻辑的代码生成。</li><li>将执行器组件放置在每个数据节点上以提供解析和谓词下推。</li></ul></li></ul><p>还有<a href="https://uniffle.apache.org/">Apache Uniffle</a>（腾讯）提供分布式 shuffle 服务。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Dremel是一种创新的DBMS，早于所有其他主要的云原生OLAP DBMS。shuffle 阶段似乎很浪费，但它简化了工程，并可以提高性能。这也是将DBMS组件分解为单独服务以抽象原始资源好处的一个很好的例子。</p><p>解耦了计算、内存、存储，元数据存储，在现代云计算时代的先驱。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://15721.courses.cs.cmu.edu/spring2023/slides/19-bigquery.pdf">15-721 spring 2023</a></li><li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/p3461-melnik.pdf">Dremel: A Decade of Interactive SQL Analysis at Web Scale</a>, in <em>VLDB</em>, 2020</li><li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/19-bigquery/melnik-vldb10.pdf">Dremel: Interactive Analysis of Web-Scale Datasets</a>, in <em>VLDB</em>, 2010 <em>(Optional)</em></li><li><a href="https://www.kancloud.cn/digest/in-memory-computing/202157">https://www.kancloud.cn/digest/in-memory-computing/202157</a></li><li><a href="https://www.kancloud.cn/digest/in-memory-computing/202158">https://www.kancloud.cn/digest/in-memory-computing/202158</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SIGMOD&#39;18 | Column Sketches</title>
    <link href="/2023/02/09/SIGMOD%E2%80%9918-Column-Sketches/"/>
    <url>/2023/02/09/SIGMOD%E2%80%9918-Column-Sketches/</url>
    
    <content type="html"><![CDATA[<h1 id="要点总结"><a href="#要点总结" class="headerlink" title="要点总结"></a>要点总结</h1><p>论文原文：</p><ul><li><a href="https://15721.courses.cs.cmu.edu/spring2023/slides/04-olapindexes.pdf">Column Sketches: A Scan Accelerator for Rapid and Robust Predicate Evaluation</a></li></ul><blockquote><p>发现看论文越来越快了，确实是熟能生巧，今天花了大概两个小时读了一篇+笔记，确实对于一般的论文，自己也不需要深究太多证明和实现的细节，先整体了解一些思路即可，之后有需要再看具体的细节证明</p></blockquote><p>本文的贡献：</p><ul><li>本文提出一种新的索引方案，称为 Column Sketch，它提高了各种 workload 下的 predicate evalution  的性能</li><li>介绍了如何使用有损压缩来创建信息位表示，同时保持较低的内存开销<ul><li>主要第三章，看图就大概懂了</li></ul></li><li>提供了在 Column Sketch 上进行高效 scan   的算法（结合SIMD指令加速的帮助），给出了 Column Sketch 性能模型</li><li>通过分析和实验证明，Column Sketches 将数值数据的 scan   性能提高了 3~6 倍，分类数据的 scan 性能提高了 2.7 倍</li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>虽然已经开发了许多索引和存储方案来解决数据系统中 predicate evalution 的核心功能，但它们都需要特定的 workload 属性（query selectivity、data distribution、data-clustering）以提供良好的性能，并在其他情况下的性能不好. 我们提出了一种<strong>新的索引方案，称为 Column Sketch，它提高了独立于 workload 属性的 predicate evalution  的性能</strong>。 Column Sketches 主要通过使用有损压缩方案来工作，这些方案旨在使索引快速摄取数据，高效地评估任何查询，并且内存占用量小。 Column Sketch 通过在逐个值的基础上应用这种有损压缩来工作，将base data 映射到较小的fixed-width codes的表示。 使用压缩数据对绝大多数值的查询进行计算，并且仅在需要时检查基础数据中的剩余值。 Column Sketch 适用于列、行和混合存储布局。</p><p>本文证明，通过使用 Column Sketch，现代分析系统中的选择运算符比最先进的存储和索引方案获得更好的 CPU 效率和更少的 data movement。 与标准 scan   相比，Column Sketches 为 numerical attributes 提供了 3×-6× 的改进，为 categorical attributes 提供了 2.7× 的改进。 与 Column Imprints 和 BitWeaving 等最先进的 scan   加速器相比，Column Sketches 的性能提高了 1.4 - 4.8 倍。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p><strong>Modern Data Access Methods</strong>: 基础数据访问和谓词评估方法对分析数据库性能至关重要。 事实上，由于每个查询都需要索引或全表 scan   ，因此选择运算符的性能可作为系统性能的基准。 因此，存在无数的方法和优化来增强谓词评估。 尽管已完成大量工作，但现有的访问方法在某些情况下都表现不佳。 图 1 显示了一个示例，其中从传统索引（如 B 树）到 scan   加速器（如 Zone Maps和 BitWeaving ）的不同类别的访问方法没有带来任何优于普通 scan   的改进。 在本节中，我们使用图 1 来讨论最先进的索引和 scan   加速器方法的性能特征，并使用它们的性能来对比 Column Sketches。</p><p><img src="/col_sketch_1.png" alt="image-20230209212425975"></p><p>**Traditional Indexes: ** 作为数据库系统的长期样例，传统的二级索引（例如 B 树）将数据访问定位到感兴趣的元组，从而为包含低选择性谓词的查询提供出色的性能。 然而，一旦选择性达到中等水平，B 树的性能就明显比其他方法差。 图 1 显示了这样一个具有 3% 低选择性的示例。</p><p>为了在高选择性查询上实现其性能，B 树引入了几个固有的缺点。 </p><p>首先，传统索引按照域的顺序而不是表的顺序查看数据。 因此，它们的输出留下了一个选择：要么按表的顺序对索引的输出进行排序，要么继续执行其余的查询，查看乱序的值。 其次，排序的顺序索引需要 gap，如 B 树的非完整叶节点的形式，用于新插入以分摊更新成本。 （二级索引 -&gt; 聚簇索引 会变成随机查找）</p><p>其次，这些 gap 需要在谓词评估期间在内存中跳转，不断中断处理器，以便它可以等待数据。 这两者都与现代 scan   形成对比，现代 scan   依赖于对内存中连续数据的紧密循环比较，并按表的顺序查看数据。 </p><p>第三，传统索引的更新分散在它们的整个域中，因此不能与许多分析数据库运行的 append-only 的系统很好地交互。 此外，最近的变化，例如存储布局从面向行到面向列的变化以及内存容量的增加，使得传统 scan   相对于传统索引具有更高的性能。 在 1% 的 selectivity 的上，scan 比 B-tree 的查询效率更高。</p><p>**Lightweight Indices: ** 最近，轻量级索引技术对 scan   性能产生了影响。 这些技术主要用作在进行按顺序 scan   时跳过数据的方法。 zone map 是当今使用最广泛的技术之一，它通过存储少量元数据（例如数据块的最小值和最大值）来工作。 这种少量的元数据利用了数据中的 natural clustering 的属性，并允许 scan   跳过完全 ok 或者完全不符合 的 block。 Column Imprints  或 Feature Based Data Skipping 等其他技术采用更复杂的方法，但高级思想是相同的：它们使用数据组的汇总统计来启用 data skipping。 虽然在正确的情况下非常有用，但在数据不表现出聚类属性的一般情况下，对数据组使用汇总统计的方法没有帮助。 图 1 显示了这种情况，其中列的值与其位置无关； 那么， zone map 没有任何优势，有和没有 zone map 的 scan   性能是一样的。</p><p>**Early Pruning Methods: ** Byte-Slicing, Bit-Slicing , and Approximate and Refine 等早期 prune 方法通过按位分解数据元素来工作。 在物理层面上，这意味着将单个值分成多个子值，沿着每个位、每个字节或沿着任意边界。 在对数据进行物理分区后，每种技术都对值进行谓词并将谓词分解为不相交的子谓词的连接。 例如，检查一个两字节数值是否等于 100 等同于检查高位字节是否等于 0 且低位字节是否等于 100。将谓词分解为不相交的部分后，每种技术都会评估谓词按照最高位到最低位的顺序，如果某些块中的元组组都确定为合格或不合格，则跳过评估顺序中后面的谓词的谓词评估。 如果高位字节中的数据提供信息，则会跳过大量数据，但是这些技术可能会受到 data-skew 的影响。 例如，继续上面的例子，如果高阶字节的重要部分的值为 0，那么第一个字节的谓词在很大程度上是无信息的，而第二个字节的谓词几乎总是被评估。 这就是图 1 的情况； 高阶位偏向于零，使得剪枝非常少。 因此，与传统 scan   相比，早期修剪并没有带来明显的优势。</p><p>**Rapid and Robust Indexing via Column Sketches: ** 我们提出了一种新的索引方案，Column Sketches，与现有技术不同，它对选择性、数据值分布和数据聚类具有鲁棒性。 Column Sketches 通过在逐个值的基础上应用有损压缩来创建辅助代码列，这是基础数据的“草图”。 然后，谓词评估被分解为 (1) 对草图数据的谓词评估，以及，如果有必要，(2) 对来自基础数据的少量数据的谓词评估。</p><p>**Lossy Compression: **通过使用有损而不是无损压缩，Column Sketches 同时实现了三个主要目标。 </p><ol><li>首先，有损压缩允许高效地找到使 code schemes 始终提供信息的编码方案。 </li><li>其次，有损压缩保证了在节省空间的同时可以实现信息编码，生成的 Column Sketch 明显小于基础数据。</li><li>第三，有损压缩允许比基于无损编码技术的索引更快的摄取速度。 这是因为生成的代码很小，因此存储从值到代码的映射所需的结构也很小，从而使从值到代码的转换速度很快。 同样，与字典压缩等无损编码相比，有损压缩意味着此映射不需要是单射的，因此域中的新值不会导致 Column Sketch 重写旧代码值。</li></ol><p>**Technical Distinctions: **Column Sketches 通过在关键方面不同于过去的技术来实现强大的性能。 与传统索引相反，数据按表的顺序显示，而不是按域的顺序显示。 压缩的辅助列位于连续的内存地址中，因此不会发生 pointer chasing。 与轻量级索引相比，Column Sketches 在逐个值的基础上工作，即使当值组混合满足和不满足谓词的值时，也允许跳过数据。 与 early prune 方法相比，有损压缩与物理布局紧密结合，以确保快速谓词评估和摄取速度。 现代 scan   中使用的其他最先进的加速技术，例如 SIMD、多核和 scan   共享，适用于 Column Sketches。 这是因为 Column Sketches 的核心是对一列 code 采用顺序 scan   。 结果如表 1 所示，其中 Column Sketches 为所有场景中的相等查询和范围查询提供了性能优势。 贡献。 本文的贡献如下：</p><ol><li>我们介绍了 Column Sketch，这是一种用于加速 scan   的数据结构，它使用有损压缩来提高性能，而不管selectivity, data-value distribution, and data clustering (§2)。</li><li>我们展示了如何使用有损压缩来创建信息位表示，同时保持较低的内存开销（§3）。</li><li>我们提供了在 Column Sketch 上进行高效 scan   的算法（§4），给出了 Column Sketch 性能模型（§5），并展示了 Column Sketches 如何轻松集成到现代系统架构中（§6）。</li><li>我们通过分析和实验证明，Column Sketches 将数值数据的 scan   性能提高了 3~6 倍，分类数据的 scan   性能提高了 2.7 倍，并改进了当前最先进的 scan   加速器技术（§7）</li></ol><p><img src="/col_sketch_2.png" alt="image-20230209214931927"></p><h1 id="2-Column-Sketches-Overview"><a href="#2-Column-Sketches-Overview" class="headerlink" title="2 Column Sketches Overview"></a>2 Column Sketches Overview</h1><blockquote><p>整体思路很好理解，建立一个有损的压缩列进行对比，虽然比较的次数不变甚至可能更多？但是每次比较的字节数变少了，难道说可以用一些 SIMD 指令或者 cpi 更小的指令来进行增加效率？</p></blockquote><p>我们从一个说明性的例子开始来描述主要思想和存储方案。 为了便于演示，我们在示例中使用了简单的有损压缩函数和 scan   算法。 然后，本文的其余部分以此处涵盖的逻辑概念为基础，并展示 Column Sketches 如何使用这些概念来提供强大、高效的性能。</p><p>**Supported Base Data Format **对基础数据的唯一要求是给定位置 i 和基础属性 B，它能够为该位置生成值 B[i]。 Column Sketches 适用于行、列组或列数据布局，本文的主体重点介绍列数据布局上的 Column Sketches。。 正如最先进的 AP 系统中常见的那样，假设表的所有基列都在位置上对齐，因此位置用于标识跨列的同一元组的值。 对于数字数据类型和字典编码的字符串数据，基础数据是一个固定宽度值的数组，位置 i 的值在数组中的索引 i 处。 对于未编码的可变长度数据（例如字符串），存在一个间接级别，偏移量数组指向包含值的 blob 数据结构。</p><p>**Column Sketch Format ** Column Sketch 由两个结构组成。 Column Sketch 中的第一个结构是 compression map，一个用 S (x) 表示的函数。 第二个结构是 sketched column。 compression map 使用函数 S 将基础数据中的值映射到 sketched column 中的指定代码。 术语 Column Sketch 指的是 compression map 和 sketched column. 的联合配对，图 2 显示了一个示例。</p><p><img src="/col_sketch_overview.png" alt="image-20230209215342847"></p><blockquote><p>图示很清楚表达了 column sketch 的技术实现</p></blockquote><p>(1) Compression Map 压缩图 S 以两种格式之一存储。 如果 S 是保序的，那么我们将生成的 sketched column 称为 order-preserving，并且压缩图存储为排序值的数组。 数组中位置 i 的值给出代码 i 中包含的最后一个元素。 例如，如果位置 i 1 的值为 1000，位置 i 的值为 2400，则代码 i 表示介于 1001 和 2400 之间的值。除了索引 i 处的值外，还有一个位用于表示代码是否为 “独特的”。Unique codes 在第 3 节中讨论。</p><p>对于非保序的Column Sketches，函数S由一个包含唯一码的哈希表和一个哈希函数组成。 在这种格式中，频繁值被赋予唯一代码并存储在哈希表中。 不频繁值不存储它们的代码，而是计算为（单独的）哈希函数的输出。</p><p>(2) Sketch Column。 Sketch Column B<sub>s</sub> 是一个 fixed-width 的 dense array，位置 i 存储函数 S 的输出，该函数 S 应用于基础数据位置 i 的值。 为了区分 base data 和 sketch column 中的值，我们将基础数据中的值称为简单值，将sketch column中的值称为code value。</p><p>示例： Building &amp; Querying a Column Sketch。 考虑图 2 中所示的示例，其中我们使用由中间数组定义的函数 S 将 8 位无符号整数 I8 映射到 2 位无符号整数 I2。 S 是保序的，因此它具有以下属性：</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-number">1.</span> <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> column b, S<span class="hljs-function"><span class="hljs-params">(x)</span> != <span class="hljs-title">S</span><span class="hljs-params">(y)</span> -&gt;</span> x != y<br><span class="hljs-number">2.</span> <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> column b, S<span class="hljs-function"><span class="hljs-params">(x)</span> &lt; <span class="hljs-title">S</span><span class="hljs-params">(y)</span> -&gt;</span> x &lt; y<br></code></pre></td></tr></table></figure><p>此外，S 生成一个 固定宽度（两位）的输出，并将基础数据中相同数量的值分配给每个代码。</p><p>我们使用 S 从基础数据构建一个较小的草图列。 对于基本属性 B 中的每个位置 i，我们将草图列中的位置 i 设置为 S (B[i])。 草图列的大小是原始列的 1&#x2F;4，因此 scan   它需要较少的 data movement。 例如，考虑使用谓词 WHERE B &lt; x 的查询求值。 因为 S 是保序的，所以 Column Sketch 可以将这个谓词翻译成 <code>(Bs &lt;S(x)) OR (Bs =S(x)AND B &lt; x)</code>。</p><p>为了评估谓词，Column Sketch 首先计算 S (x )。 然后，它 scan   绘制的列 Bs 并检查 Bs &lt; S (x ) 和 Bs &#x3D; S (x )。 对于小于 S (x) 的值，它们的基值符合条件。 对于大于 S(x) 的值，它们在基础数据中的值不合格。 对于等于 S (x) 的值，它们的基值可能合格也可能不合格，因此我们使用基础数据评估 B &lt; x。 算法 1 描述了这个过程。</p><p>图 2 显示了一个示例。 草图列中的位置 1 和 4 在没有看到基础数据的情况下合格。 基础数据中需要检查位置 5 和 6，这两个中只有位置 6 符合条件。 在示例中，Column Sketch 需要两次访问基础数据，同时检查 8 个值。 这是由压缩代码中的少量比特解释的。 通常，Column Sketch 中的每个代码具有相对相等的值数量，并且 Column Sketch 每当看到映射的谓词值 S (x) 时都需要检查基础数据。 因此，我们预计每 2#bits 值需要访问一次基础数据。</p><p><img src="/col_sketch_algo.png" alt="image-20230209220857541"></p><p>**Byte Alignment. ** Sketch Column 适用于任何code大小。 然而，我们发现在现代硬件上，使用非字节对齐代码会导致大约 30% 的性能损失。 因此，我们特别关注 8 位 Column Sketches 和 16 位 Column Sketches。 </p><h1 id="3-Constructing-Compression-Maps"><a href="#3-Constructing-Compression-Maps" class="headerlink" title="3 Constructing Compression Maps"></a>3 Constructing Compression Maps</h1><p>我们现在展示如何为 Column Sketches 构建 Compression Map。 根据定义，此映射是从 base data 到 sketch column 的函数。 为了复习压缩图，我们在第 3.1 节讨论了它们的目标，在第 3.2 节中保证了它们的实用性，并在第 3.3 和 3.4 节中讨论了如何为数值和分类属性构建它们。</p><h2 id="3-1-Compression-Map-Objectives"><a href="#3-1-Compression-Map-Objectives" class="headerlink" title="3.1 Compression Map Objectives"></a>3.1 Compression Map Objectives</h2><blockquote><p>说明了 compression map 设定的一些原则</p></blockquote><p>压缩图的目标是限制我们需要访问基础数据的次数并有效地支持数据修改。 为此，压缩图：</p><p>(1) 为常见值分配自己唯一的代码。 当检查 B &lt; x 等查询的 endpoints  时，Column Sketch  scan   需要检查代码 S (x) 的基础数据。 如果 x 是一个有自己编码的值（即 S <sup>-1</sup> (S(x)) &#x3D; {x}），那么我们不需要检查基础数据，只需通过 Column Sketch 就可以直接回答查询。 此属性适用于范围谓词和相等谓词。</p><p>为了实现强大的 scan   性能，我们识别频繁值并为它们提供自己的唯一 code。 举一个简单的例子来说明为什么这对稳健的性能至关重要，如果我们有一个值占元组的 10%，并且它有一个非唯一代码，那么基于这个值的分配代码的谓词需要访问基础数据一个重要的 次数。 因为访问高速缓存行中的任何数据项都会将整个高速缓存行带到处理器，所以访问 10% 的元组可能会使性能类似于传统 scan   。 因此，我们确定了频繁值，以便我们可以限制我们为任何谓词接触的基础数据量。</p><p>(2) Assign non-unique codes similar numbers of values。 这样做的原因类似于为什么频繁值需要唯一代码的原因。 我们为每个非唯一代码分配了相对均匀且较小的数据集部分，因此我们只需要少量的基础数据访问即可进行任何 scan   。</p><p>(3) 必要时保持顺序。 某些属性会看到范围谓词，而其他属性则不会。 对于看到范围谓词的属性，压缩映射应该保持顺序，以便可以使用 Column Sketch 评估范围查询。</p><p>(4) Handle unseen values in the domain without re-encoding。 重新编码应该是一种罕见的按需操作。 由于有损的性质，有损压缩意味着允许新值与已经出现的值无法区分。 因此，只要我们巧妙地定义我们的编码，新值就不需要重新编码 Column Sketch。 对于不需要重新编码的有序柱状图，不能有连续的唯一代码。 例如，如果 S 将唯一代码 i 分配给“gale”，将 i + 1 分配给“gate”，则输入字符串（如“game”）没有代码值。 将“gate”的代码更改为非唯一的可以解决这个问题。 对于无序的 Column Sketches，只要存在至少一个非唯一代码，每个看不见的值都有一个可能的值。</p><p>(5) Optional：利用频繁查询的值。 利用频繁查询的值可以提供额外的性能优势； 然而，与频繁的数据值不同，识别频繁的查询值会降低查询性能的稳健性。 我们在本文的主要部分着重于描述如何为任何查询实现高效和稳健的性能，并在附录 F 中包含有关利用频繁查询值的详细信息。</p><h2 id="3-2-Bounding-Base-Data-Accesses"><a href="#3-2-Bounding-Base-Data-Accesses" class="headerlink" title="3.2 Bounding Base Data Accesses"></a>3.2 Bounding Base Data Accesses</h2><p>以下两个定理适用于我们如何限制分配给非唯一代码的值的数量。</p><p>定理一：对于有限定义域 X 中的 x<sub>1</sub> &lt; x<sub>2</sub> &lt;…&lt;x<sub>m</sub> ，值域 Y 中的  y<sub>1</sub> &lt; y<sub>2</sub> &lt;…&lt;y<sub>m</sub> ，要保证函数对应之后是有序的，也就是类似上面的图二中的（保序性）</p><p>这个保序函数的定理意味着结果也适用于非保序函数。</p><p>定理二：要保证每个  y<sub>i</sub> 的 frequency 都大于等于 <code>2/n</code>，（其实就是限制 y 的个数）</p><p>定理和推论证明我们可以创建映射来限制分配给任何非唯一代码的基础数据中的值的数量。 这直接意味着我们可以限制我们需要访问基础数据的次数。 定理 1 和推论 2 的证明分别在附录 A 和 B 中给出，两个证明都给出了显式构造 S 的算法。每个单独给出证明，因为用于创建无序压缩映射的算法给出 每个非唯一代码中值的数量差异较小的映射。</p><h2 id="3-3-Numerical-Compression-Maps"><a href="#3-3-Numerical-Compression-Maps" class="headerlink" title="3.3 Numerical Compression Maps"></a>3.3 Numerical Compression Maps</h2><p><strong>Lossless Numerical Compression.</strong> 对于数字数据类型，frame-of-reference (FOR)、prefix supression 和 null supression 等无损压缩技术通过将值存储为某个偏移量的相对值来工作。 这三种技术都支持压缩形式的操作； 特别是，它们可以在不解压缩的情况下执行相<strong>等谓词、范围谓词和聚合运算符</strong>。 然而，为了有效地支持聚合，这些技术中的每一种都保留了差异； 即，给定基值 a 和 b，它们的编码值 e<sub>a</sub> 和 e<sub>b</sub> 满足 e<sub>a</sub> -  e<sub>b</sub>  &#x3D; a - b。 这限制了它们改变高位熵的能力，并且只有当列中的每个值在这些高位上全为 0 或全为 1 时，这些位才能被截断。</p><p><strong>Constructing Numerical Compression Maps</strong> 与无损技术相比，有损压缩仅专注于最大化草图中位的效用。 在保留顺序的情况下执行此操作的最简单方法是构建一个近似于输入数据的 CDF 的等深度直方图，然后根据每个直方图桶的 endpoints  创建代码。 当在我们的数值域中给定一个值时，映射的输出就是一个值所属的直方图桶。 我们通过从基列中统一采样值、对这些值进行排序，然后根据该排序列表生成每个桶的 endpoints  来创建近似等深度直方图。<br>因为直方图桶是连续的，存储每个桶的 endpoints   i 就足以知道直方图桶覆盖的范围。 图 3a 和 3b 显示了使用两个不同数据集的直方图进行映射的示例。 在这两个图中，我们使用 200,000 个样本来创建 256 个 endpoints  。 均匀分布从 0 到 10,000,000，正态分布数据的均值为 0，方差为 1000。均匀分布的代码在整个过程中均匀分布，正态分布的代码越靠近分布的 endpoints  ，越接近 一起走向中间。 直方图捕获两个函数的分布，并在代码之间均匀分布基础数据中的值。</p><blockquote><p>通过等高直方图来选择这些数字，形成近似直方图</p></blockquote><p><strong>Handling Frequent Values</strong> </p><blockquote><p>Q: 这里有点复杂，没完全看明白，需要下来仔细看下理一下流程，给出原文，下次再仔细看</p><p><img src="/col_sketch_handing_freq1.png" alt="image-20230209234105694"></p><p><img src="/col_sketch_handling_freq2.png" alt="image-20230209234134888"></p></blockquote><p>我们将频繁值定义为出现在超过 z1 个基础数据值中的值。 为了处理这些频繁值，我们首先执行与之前相同的过程并创建一个排序的采样值列表。 如果一个值表示大小为 n 的样本的 z1 个以上，则排序列表 at(n,2n,…, (z-1)n) 中的值之一必须是该值。 因此，对于这些 z 值中的每一个，我们可以搜索它的第一次和最后一次出现以检查它是否代表样本的 z1 以上。 如果是这样，请标记该值在列表中的中间位置，并为该值赋予唯一代码 c ⇒ 中点（四舍五入为最接近的整数），其中 c 是 sketched column 中的代码数。 在 z &gt; c 并且两个值将被赋予相同的唯一代码 c 的情况下，更频繁的值将被赋予该唯一代码。 在本文中，我们使用 z &#x3D; c。 虽然较大的 z 值可以创建更快的平均查询时间，但我们选择 z &#x3D; c 以便使代码唯一不会增加非唯一代码中值的比例。</p><p>在找到值得唯一代码的值并为它们提供关联的代码值后，我们在每个唯一代码之间平均划分排序列表，并相应地分配剩余的代码值。 与单次遍历样本相比，唯一代码的识别在最坏的情况下，非唯一代码的划分是一个恒定时间的操作。<br>为了使更新不能强制重新编码，我们不允许唯一代码占据后续位置。 如果在先前的过程中，值 i 和 i+1 将分别被赋予唯一代码 i 和 i + 1，则只有更频繁的值被赋予唯一代码。 对于要分配给后续代码的值，频率较低的代码最多只能包含 c2 个采样值，因此我们先前针对没有具有太多值的非唯一代码的稳健性结果仍然成立。 此外，我们不允许压缩图中的 first 和 last 代码是唯一的。</p><p><strong>Estimating the Base Data Distribution.</strong> </p><blockquote><p>PS：这里也没太仔细深究其中数据分布的细节，下次需要再仔细看，给出原文</p><p><img src="/col_sketch_distribution.png" alt="image-20230209234236843"></p></blockquote><p>为了使压缩图在每个代码中具有大致相等数量的值，从经验 CDF 创建的采样直方图需要密切遵循基础数据的分布。 Dvoretzky-Kiefer-Wolfowitz 不等式提供了 n 个样本的经验 CDF Fn 向真实 CDF F 收敛的界限，我们可以将真实分布 F 视为未知量，将列视为独立同分布。 F 的样本，或者我们可以将该列视为离散分布，其 CDF 恰好等于基础数据的 CDF。 在这两种情况下，从基础数据中采样 n 次可以得到我们采样数据的经验 CDF Fn 与真实 CDF F1 的距离所需的结果。 我们在第 7 节中证明，对于一个字节的 Column Sketch，任何少于 4 个基础数据的列都比普通 scan   提供 2 x  性能优势。 由于 Column Sketch 映射从不分配单个非唯一代码。 我们的目标是 &#x3D; 256 。 对于图 3 中的 200,000 个样本，出现此数量错误的可能性小于 10 5。 样本数 n 和期望值都是可调的。</p><h2 id="3-4-Categorical-Compression-Maps"><a href="#3-4-Categorical-Compression-Maps" class="headerlink" title="3.4 Categorical Compression Maps"></a>3.4 Categorical Compression Maps</h2><p><strong>Categorical Data and Dictionary Encoding.</strong> 与数字分布不同，分类分布通常具有占据数据集重要部分的值。 此外，某些分类分布不需要顺序。</p><p>传统上，分类分布已经使用（可选的保序）固定宽度字典编码进行编码。 字典编码通过为每个唯一值赋予其自己的数字代码来工作。 一个简单的例子是美国的州。</p><p>虽然这可能被声明为 varchar 列，但只有 50 个不同的值，因此每个状态都可以用 0 到 49 之间的数字表示。由于每个不同的值都需要一个不同的代码，因此存储字典所需的位数 -编码值为 [log2 n]，其中 n 是唯一值的数量。</p><p><strong>Lossy Dictionaries.</strong> 分类分布的压缩图看起来类似于字典编码，除了稀有代码已经相互折叠，使代码值的数量更小。 这种折叠的主要好处是 scan   绘制的列读取更少的内存。 然而，还有一个处理上的好处，因为我们可以选择非单射编码中代码值的数量，以便代码具有固定的字节长度。 例如，如果我们查看具有 1200 个唯一值的数据集，那么字典编码的列每个值需要 11 位。 如果这些代码一个接一个地密集打包，它们将不会从字节边界开始，CPU 将需要解包代码以将它们对齐到字节边界。 如果它们没有密集打包，那么代码将被填充到 16 位，这反过来又会带来更高的 data movement 成本。 使用 Column Sketches，有损编码方案可以选择位数为 8 的倍数，从而节省 data movement 而无需进行代码解包。</p><p>图 4 中显示的示例比较了美国各州的保序字典和保序有损字典。 仅显示唯一代码，在隐含间隙中显示非唯一代码。 尽管这是一个简化的示例，但它显示了我们希望为有损字典保留的各种属性。 最频繁的值，在这种情况下是人口最多的州，被赋予唯一的代码，而较少的值共享代码。 加利福尼亚州的唯一代码为 1，而怀俄明州与威斯康星州、西弗吉尼亚州和其他几个州共享代码 14。 这 7 个唯一代码覆盖了美国近 50% 的人口。另外 50% 的人口分布在 8 个非唯一代码中，每个非唯一代码预计有 6.25% 的数据。 然而，这是由于非唯一代码的数量很少。 例如，如果我们将其更改为美国的城市，其中大约有 19,000 个，并且有 128 个唯一代码和 128 个非唯一代码，那么每个非唯一代码将只有 0.6% 的数据。</p><p><img src="/col_sketch_lossy.png" alt="image-20230209225202248"></p><p><strong>Unordered Categorical Data.</strong> 我们首先讨论为不需要数据排序的分类分布分配代码。 不需要有序的代码值，我们可以自由地将任何值分配给任何代码值。 这种选择的自由使得可能的压缩图空间非常大，但也产生了相当好的直观解决方案。 我们有三个主要的设计决策：</p><ol><li>应该给多少个值赋予唯一码？</li><li>我们给哪些值赋予独特的代码？</li><li>我们如何在非唯一代码之间分配值？</li></ol><p>(1) Assigning Unique Codes.。 分配唯一代码的最简单方法是为最频繁出现的值赋予唯一代码。 这是稳健的，因为它限制了我们访问任何谓词的基础数据的次数。 附录 F 中介绍了分析查询历史以分配唯一代码值的更积极（但可能不太健壮）的方法。</p><p>(2) Number of Unique Codes.。 选择要创建多少个唯一代码是一个可调的设计决策，具体取决于手头应用程序的重新要求。 我们在这里描述了做出此决定的两种方法。 一种方法是为样本中出现频率超过某个频率 z 的每个值赋予一个唯一的代码值，让剩余的代码分布在频率小于指定截止值的所有值中。 此参数 z 具有与有序情况相同的权衡，根据工作负载和应用程序要求对其进行调整是未来工作的一部分。 在本文中，z 设置为 256，原因与有序情况类似。 分配唯一代码的第二种方法是为唯一代码的数量设置一个常量值。 第二种方法对某些值特别有效。 例如，如果恰好一半分配的代码是唯一代码，那么我们可以使用代码值的第一位或最后一位来描述唯一代码和非唯一代码。</p><p>(3) Assigning Values to Non-Unique Codes。 摄取数据的最快方法是使用哈希函数在非唯一代码之间相对均匀地分配值。 如果有 c 个代码和 u 个唯一代码，我们将唯一代码分配给代码 0、1、…. , u-1. 当对传入值进行编码时，我们首先检查包含频繁值的哈希表以查看传入值是否被唯一编码。 如果该值是唯一编码的，则其代码将写入 Sketch Column。 如果不是，则将该值编码为“u +[h(x)%(c-u)]”。</p><blockquote><p>这里是分配 unique code 和 non-unique code 的方式，这样可以将非唯一编码写入到唯一编码之后的位置中去</p></blockquote><p>Analysis of Design Choices. 。 到目前为止，<strong>最重要的性能特征是确保最频繁的值被赋予唯一代码</strong>。 图 5 和图 6 显示了赋予任何非唯一代码的最大数据项数以及所有非唯一代码的平均值。 在这两个图中，我们有 100,000 个元组，给定 10,000 个唯一值，我们看到每个值遵循 Zipfian 分布的频率。 稀有值通过散列分布在非唯一代码中。 在第一个图中，我们将偏斜参数保持为 1 并改变唯一代码的数量。 在第二张图中，我们使用了 128 个唯一代码并更改了数据集的倾斜度。 如图 5 所示，选择适量的唯一代码可确保每个非唯一代码在基础数据中具有合理数量的值。 图 6 显示，对于同时具有高偏斜和低偏斜的数据集，每个非唯一代码中的元组数量只占数据的一小部分。 有序分类数据。 有序分类数据共享无序分类数据和数值数据的属性。 与数字数据一样，我们希望看到查询询问有关域中某些元素范围的问题。 与无序分类数据一样，我们希望看到基于相等比较的查询。 跨代码均匀分布域中的值可实现两者所需的属性。 因此，为识别数值数据中的频繁值而给出的算法也适用于有序的分类数据。</p><h1 id="4-Predicate-Evaluation-Over-Column-Sketches"><a href="#4-Predicate-Evaluation-Over-Column-Sketches" class="headerlink" title="4 Predicate Evaluation Over Column Sketches"></a>4 Predicate Evaluation Over Column Sketches</h1><p>对于 Column Sketch 评估的任何谓词，我们都有可以被视为查询 endpoints  的 code。 例如，第 2 节中作为示例给出的比较 B &lt; x 具有 endpoints   S(x)。 对于具有小于和大于子句的范围谓词，例如 x1 &lt; B &lt; x2，谓词有两个 endpoints  ：S(x1) 和 S(x2)。 虽然从技术上讲，相等谓词没有 endpoints  ，因为它不是范围，但为了符号的一致性，我们可以将 S(x) 视为谓词 B &#x3D; x 的 endpoints  。</p><p>**SIMD Instructions.**。 SIMD 指令提供了一种通过一次对多个数据元素执行一条指令来实现数据级并行性的方法。 这些指令看起来像传统的 CPU 指令，例如加法或乘法，但有两个额外的参数。 第一个是所讨论的 SIMD 寄存器的大小，可以是 64、128、256 或 512 位。 第二个参数是正在操作的数据元素的大小，可以是 8、16、32 或 64。例如，指令 <code>_mm256_add_epi8 (_- _m256i a, __m256i b) </code>需要两个数组，每个数组有 32 个元素 8 位，并通过将输入中的相应位置一次性相加产生一个包含 32 个 8 位元素的数组。 </p><p><strong>Scan API</strong>。 Column Sketch scan 采用 Column Sketch、predicate operation 及其的值。 它可以输出匹配位置的 bit-vector 或匹配 position-list ，默认输出是 bit-vector 。 通常，对于非常低的选择性，应该使用 position-list ，而对于更高的选择性，应该使用 bit-vector 。 这是因为在高选择性下， position-list 格式需要大量的内存移动。</p><p> <strong>Scan Procedure</strong>。 算法 2 描述了针对一字节 sketched column 的基于 SIMD 的 sketched column  scan   。 它使用 Intel 的 AVX 指令集并产生 bit-vector 输出。 出于空间原因，我们省略了几个变量的设置，并使用逻辑描述而不是物理指令来进行更长的操作。</p><p>嵌套循环的内部负责逻辑计算哪些位置匹配，哪些位置可能匹配。 在第一行中，我们在执行我们需要的两个逻辑比较之前加载了我们需要的 16 个代码。 对于小于的情况，我们唯一的 endpoints  是 S (x)，我们在第 10 行使用相等谓词检查这个值。对于匹配这个谓词的每个位置，我们需要转到基础数据。</p><p>在这些比较之后，我们将绝对合格的位置转换为 bit-vector 并立即存储。 对于可能的匹配位置，我们执行条件存储。 为了简洁起见，我们的代码中省略了条件存储，首先检查其结果 bit-vector 是否全为零。 如果不是，它将条件 bit-vector 转换为 position-list 并将结果存储在堆栈上的一个小缓冲区中。 在创建 Column Sketch 时，可能匹配值的结果 bit-vector 通常全为零，因此没有代码包含太多值，因此很少执行将 bit-vector 转换为 position-list 并存储位置的代码 . 作为一个小细节，我们发现将临时结果存储在 stack 中很重要。 将这些临时结果存储在 heap 上会降低 15% 的性能。</p><p>Column Sketch  scan   被分成较小段的嵌套循环，因此该算法可以使用 base data 修补结果 bit-vector ，同时结果 bit-vector 保留在 CPU 缓存的高级中。 如果我们最后检查所有可能匹配的位置，我们会看到大约 5% 的轻微性能下降。</p><p><strong>Unique Endpoints</strong>。 独特的 endpoints  使 scan   的计算效率更高。 如果代码 S(x) 是唯一的，则不需要跟踪位置，也不需要条件存储指令。 此外，该算法只需要进行一次小于比较。 比较之后，它立即写出 bit-vector 。 更一般地说，给定一个唯一代码，对 Column Sketch 的 scan   完全回答了查询而不参考基础数据，因此看起来与正常 scan  完全一样，但是 data movement 较少。</p><p><strong>Equality and Between Predicates</strong>。 相等谓词和谓词之间的处理与算法2类似。对于相等谓词，主要区别在于，根据代码是否唯一，初始比较只需要存储</p><p><img src="/col_sketch_predicate_eval.png" alt="image-20230209231840553"></p><blockquote><p>算法主要利用了 SIMD 指令加速，</p><p>循环所有数据，每次循环处理一次 SIMD 指令能够处理的最多位数</p><p>将所有数据分成两部分，</p><ul><li>第一部分是肯定全部 ok 的，也就是根据 unique code 或者条件满足的</li><li>第二部分就是可能不满足，出现假阳性的时候把可能得位置存到 tmp_result 中</li></ul><p>最后统一检查原始数据是否满足条件</p></blockquote><h1 id="5-Performance-Modeling"><a href="#5-Performance-Modeling" class="headerlink" title="5 Performance Modeling"></a>5 Performance Modeling</h1><p>Column Sketch 的性能主要取决于 memory 中 data movement 的 cost，这里发现内存带宽都是打满的。</p><h1 id="6-System-Integration-and-Memory-Overhead"><a href="#6-System-Integration-and-Memory-Overhead" class="headerlink" title="6 System Integration and Memory Overhead"></a>6 System Integration and Memory Overhead</h1><h2 id="System-Integration"><a href="#System-Integration" class="headerlink" title="System Integration"></a>System Integration</h2><p>Column Sketches 的许多组件已经部分或完全存在于成熟的数据库系统中。 <strong>创建压缩图需要采样和直方图</strong>，几乎每个主要系统都支持它们。 第 4 节中给出的 SIMD 扫描类似于分析数据库中已经存在的优化扫描，基础数据上的区域映射可以过滤出 Column Sketch 的相应位置对齐部分。 在 Column Sketch 中添加数据和更新数据类似于对字典编码的属性进行数据修改。 在第 7 节中，我们展示了 Column Sketch 扫描总是比传统扫描更快。 因此，优化器可以在传统索引和 Column Sketch 之间使用相同的基于选择性的访问路径选择，具有较低的交叉点。 同样，Column Sketches 可以自然地处理任何支持比较的有序数据类型。 这与早期剪枝技术等相关技术形成对比，这些技术需要修改浮点数等各种类型的默认编码，使其具有二进制可比性。 最后，Column Sketches 不改变基础数据布局，因此除了 select 之外的所有其他运算符都可以保持不变。</p><h2 id="Memory-overhead"><a href="#Memory-overhead" class="headerlink" title="Memory overhead"></a>Memory overhead</h2><p>令  b<sub>s</sub>  为  column sketch  中每个元素的位数。 然后我们需要  b<sub>s</sub>   x  n 位的空间用于 sketch column 。 如果我们让  b<sub>b</sub>  是基本数据元素所需的位数，那么每个字典条目都需要  b<sub>b</sub>  + 1 位空间，其中额外的位来自标记该代码的值是否唯一。 完整字典的大小是 ( b<sub>b</sub>  + 1)  x  2<sup>b</sup> 位。 值得注意的是，b 通常很小（我们在本文的所有点都使用 b &#x3D; 8 来创建字节对齐），因此字典通常也很小。 此外，字典的大小与列的大小 n 无关，因此随着 n 的增长，  column sketch  的开销接近  b<sub>s</sub>   x  n 位。 此外，我们注意到 Column Sketch 最适合在允许高效位置访问的基列上使用压缩技术。 当数据在内存中时，大多数分析系统通常都是这种情况，因为数据通常使用固定宽度编码进行压缩。</p><h1 id="7-Expermental-Analysis"><a href="#7-Expermental-Analysis" class="headerlink" title="7 Expermental Analysis"></a>7 Expermental Analysis</h1><p>和下面三种方法对比：</p><ul><li>BitWeaving&#x2F;V</li><li>Column Imprints</li><li>a B-tree index</li></ul><p>实验都是在内存中进行，不包含磁盘IO。</p><p>16MB L3 cache + 64 physical thread + 1T memory，取100次实验的平均值代表性能</p><ul><li>7.1 Uniform Numerical Data</li><li>7.2 Skewed Numerical Data</li><li>7.3 Categorical Data</li><li>7.4 Load Performance</li></ul><p>给几张实验结果图，可以看到 column sketch 相较其他的 index 方法全面碾压。</p><p><img src="/col_sketch_expe.png" alt="image-20230209234901743"></p><p><img src="/col_sketch_expe2.png" alt="image-20230209235022689"></p><h1 id="8-Related-Work"><a href="#8-Related-Work" class="headerlink" title="8 Related Work"></a>8 Related Work</h1><p><strong>Compression and Optimized Scans</strong>。 2000 年代中期，MonetDB 和 C-Store 开始将压缩和执行紧密集成到面向列的数据库中的扫描中。 从那时起，已经完成了将多种类型的压缩集成到扫描中的工作，特别是字典压缩、增量编码、参考帧编码和游程编码。 如今，混合压缩和执行已成为标准，并且在大多数商业 DBMS 中都可以看到。 最近，IBM 创建了频率压缩，利用数据重新排序来提高扫描性能。</p><p>这些技术中的每一种都是无损的，旨在用于基础数据。 因此，这些技术可以通过使用有损而不是无损压缩来实现更高的压缩率，从而减少扫描期间的内存和磁盘带宽。 过去曾假设有损压缩用于谓词评估的潜力，但没有提出解决方案 。</p><p><strong>Early Pruning Extensions</strong>。 Li、Chasseur 和 Patel 研究了用于位分片索引的无损可变长度编码方案，旨在使查询处理中的高阶位信息丰富。 这解决了数据值倾斜的问题，并且还利用了更频繁查询的谓词值。 如果偏斜足够严重，以至于频繁值或频繁查询的值需要少于 8 位，那么生成的位切片索引在查询这些值时会比 Column Sketch 更快。 此外，与传统的位分片索引一样，即使列的代码值小于 8 位，生成的索引也很有用。</p><p>然而，没有考虑有损编码方案。 通过保持编码方案无损，填充的可变长度编码方案具有更大的内存占用和更昂贵的写入时间。 同样，生成的填充可变长度编码方案的生成成本很高，需要花费大量运行时间来生成 24 位或更小代码大小的编码方案。 未来工作的一个有趣方向是将中的技术与 Column Sketches 混合，并在 Column Sketch 的草绘列内使用填充位编织列。</p><p><strong>Lightweight Indices</strong>。 轻量级数据跳过技术（例如 Zone Maps 及其等效技术）提供了一种通过为每列保留简单元数据（例如最小值和最大值）来跳过大数据块的方法。 它们包含在许多最近的系统中 ，而如何最好地组织数据和元数据是一个正在进行的研究领域。 在最近的方法中，Column Imprints 脱颖而出，因为它与 Column Sketches 在本质上相似。 Column Imprints 还使用直方图来更好地评估谓词，但一次对一组值而不是一次对单个值这样做。</p><p>对于具有聚类属性的数据集，数据跳过技术注意到整组值会将谓词评估为真或假，因此在这些场景中提供了令人难以置信的加速。 对于非集群的数据集，轻量级索引无法一次评估一组数据，因此扫描需要单独检查每个值。 相比之下，Column Sketches 能够处理这些查询，因为它已经在逐个元素的基础上工作。 因此，轻量级索引应与 Column Sketches 结合使用，因为两者更新成本低且针对不同的场景。</p><p>**Other Operators over Early Pruning Techniques.**。 Early Pruning Techniques 的使用已推广到谓词评估之外的其他运算符。 这项工作的大部分都可以应用于柱草图。 例如，MAX 运算符可以查看 b 个高阶位并修剪所有小于仅使用这 b 个位看到的最大值的记录，因为它们肯定不是最大值。 这类似于 Column Sketches，其中任何不是 Column Sketch 字节最大值的值显然不是列中的最大值。</p><p>**SIMD Scan Optimizations.**。 最近有一股关于如何最好地将 SIMD 集成到列扫描中的研究。 本文中的解包方法基于 Willhalm 等人的工作。 他们使用 SIMD 通道解包代码，每个 SIMD 通道一个代码。 扫描计算性能的改进是对 Column Sketches 的补充。 Column Sketches 被设计为可扫描的密集数组结构，因此评估谓词的改进同样适用于 Column Sketches。</p><h1 id="9-Conclusion"><a href="#9-Conclusion" class="headerlink" title="9 Conclusion"></a>9 Conclusion</h1><p>在本文中，我们展示了传统索引和轻量级数据跳过技术都无法为对非集群数据具有适度选择性的查询提供性能优势。 为了为这一大类查询提供性能改进，我们引入了一种新的索引技术 Column Sketches，无论数据排序、数据分布和查询选择性如何，它都能提供更好的扫描性能。 与扫描加速器的最先进方法相比，Column Sketches 更容易更新，并且在一系列不同数据分布的扫描中性能更高。 Column Sketches 的可能扩展包括等式和范围谓词以外的运算符的使用，例如聚合、集合包含谓词和近似查询处理。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SIGMOD 2008 | Column-Stores-vs-Row-Stores</title>
    <link href="/2023/02/07/SIGMOD-2008-Column-Stores-vs-Row-Stores/"/>
    <url>/2023/02/07/SIGMOD-2008-Column-Stores-vs-Row-Stores/</url>
    
    <content type="html"><![CDATA[<h1 id="要点总结"><a href="#要点总结" class="headerlink" title="要点总结"></a>要点总结</h1><p>本文读完的一些要记住的点</p><ul><li>尝试通过垂直分区和 <code>Index-only plans</code> 等技术在行存储中模拟列存储的物理布局不会产生良好的性能。 我们将这种缓慢归因于tuple 的高昂重建成本，以及窄的垂直分区表中的每个 tuple 额外的高额开销。 </li><li>分解了列存储能够如此有效地处理面向列的数据的原因<ul><li><code>late materialization</code> 将性能提高了三倍</li><li><code>compress</code> 平均提供了大约两倍，或者一个数量级访问排序数据的查询的幅度</li><li>提出了一种新的连接技术，称为 <code>invisible join</code>，可将性能进一步提高约 50%。</li><li>使用谓词在事实表上建立 hash table 进行维度表的 bitmap 选择，多个不同事实表谓词选择的 bitmap 可以进行 AND，从而减少最终无序读取多个事实表or维度表的数据</li><li>或者在某个维度表的属性连续的时候，改写成 between 谓词</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>列存的系统在AP的场景下性能要比传统的行存好一个数量级以上，一开始认为原因很简单：只不过是列存储读取数据的I&#x2F;O效率更高。这种简单的观点导致了一个错误的假设：即可以通过行存系统可以获取得到列存系统的分析性能（做法是：只是在行存中进行垂直分区或者独立索引每一列就认为转变为列存系统）。</p><p>本文分析了行存和列存系统的性能差异，说明了在查询的执行器上的一些重要差异，还说明了许多列存上的查询技术对性能的影响，包括 向量化查询、压缩、以及一些新的 join 算法。最终得出结论：<strong>虽然行存并非不可能得到列存的某些性能优势，但是仍然需要对存储层和查询执行器都进行对应的改动才能获得列存方法的优势</strong>。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>这些年有些列存的数据库例如 MonetDB，C-Store 出现，这些系统都在某些 AP 数仓场景下能够提供相较于传统行存的一个数量级以上的优势，但是这些实验对比通常仍然是和传统的行存对比，这些行存的系统还是使用传统的物理存储。虽然这种方法可以看到列存系统确实比行存更有优势，但是遗留下了一个重大问题：</p><p><strong>这些性能提升是由于列存系统内部的架构的某些基本原因，还是说这种提升在行存系统中也能通过使用列存的物理存储来实现？</strong></p><p>本文的作者是一个专业的DBA，使用了一些对行存的优化：</p><ul><li>将系统中的表垂直分区为两列（key -&gt; attribute），这样只需要读取必要的列</li><li>只使用 index-only plan，每个列都上索引</li><li>使用物化视图的集合，有一个视图包含 benchmark 中每个查询所需要的列，虽然会消耗巨大的空间，但是这是行存的最佳情况</li></ul><p>通过将这些技术与 SSBM 上开源的 C-Store 的性能进行比较，尽管上面的方法能够模拟数据库中的列存，但是 AP 性能很差，这表明在传统的TP系统中通过模拟列列存储格式并不能获得列存系统的优势。</p><p>这里引出的第二个问题，列存中哪些具体的技术是列存相比行存在数据仓库方面具有显著优势的点？</p><ul><li>Late Materization（延迟物化）：从磁盘中读取的列尽可能晚的连接成行</li><li>Block Iteration：不像 volcano 风格的迭代器每次只传递一个值，每次传递一组值</li><li>基于列的压缩技术：例如 run-length encoding，能够在延迟物化的压缩数据上进行直接操作</li><li>Invisible Join：本文刚提出的</li></ul><p>本文通过逐个删除这些优化来和行存的系统性能进行对比，发现 <strong>压缩可以提供数量级级别的优势，延迟物化可以提供大约3倍的性能提升，其他优化例如 block iteration 和 invisible join 平均提供了 1.5 倍的提升</strong>。本文有三个主要贡献：</p><ul><li>说明了在行存中模拟列存无法得到好的性能提升，并且通常认为在数仓中各种好技术也无法提升（only index-plan, bitmap index等）</li><li>本文提出了一种新的 join 算法，invisible join，得出结论在行存中使用的非规范化性能增强技术在列存中试不必要的</li></ul><blockquote><p>Q：啥是  that denormalization, an important but expensive (in space requirements) and complicated (in deciding in ad- vance what tables to denormalize) 非规范化</p></blockquote><ul><li>通过分解各种技术，说明了多种列存技术对查询性能的提升，并在SSBM上证明了并证明了简单的面向列的实现，如果没有压缩和延迟实现并没有显着优于优化良好的行存储设计。</li></ul><h1 id="2-Background-And-Prior-Work"><a href="#2-Background-And-Prior-Work" class="headerlink" title="2 Background And Prior Work"></a>2 Background And Prior Work</h1><ul><li>MonetDB和MonetDB&#x2F;X100是现代列存和向量化查询的先去，说明了由于CPU和cache的性能，面向列的设计可以在TPC-H等测试中明显优于各种商业和开源数据库。</li><li>C-Store 是面向列更新的DB，很多操作和 MonetDB 类似，也有基于压缩数据直接操作的优化。</li><li>Harizopoulos 自底向上构建简单的 早期物化说明列存好于行存，通过在 Shore 中构建了一个列存存储，提出了一种 ”super tuple“ 的优化，可以避免重复标头信息并将许多元组一起打包成一个块，这可以减少完全垂直分区方案的开销</li></ul><h1 id="3-STAR-Schema-Benchmark（SSB）"><a href="#3-STAR-Schema-Benchmark（SSB）" class="headerlink" title="3 STAR Schema Benchmark（SSB）"></a>3 STAR Schema Benchmark（SSB）</h1><p>本文使用 SSB 进行性能基准测试。SSB 源自 TPC-H，与 TPC-H 不同的是，SSB 使用的是纯 STAR 的 schema（数仓的最佳实践），包含比 TPC-H 更少的查询。可以看到 pattern 如下图：</p><p><img src="/SSB.png" alt="image-20230207115014748"></p><p>和 TPC-H 一样，有一个比例因子可以用于缩放基准大小，本文使用 10 的比例因子，产生一个包含 60,000,000 个元组的 LINEOREDER 表。</p><h1 id="4-ROW-Oriented-Execution"><a href="#4-ROW-Oriented-Execution" class="headerlink" title="4 ROW-Oriented Execution"></a>4 ROW-Oriented Execution</h1><p>下面介绍三种可以在面向行的数据库（化名 System X）实现列存数据库存储的设计方案</p><h2 id="Vertical-Partitioning"><a href="#Vertical-Partitioning" class="headerlink" title="Vertical Partitioning"></a>Vertical Partitioning</h2><p>行存模拟列存最简单的方法就是采用完全垂直分区。在完全垂直分区的系统中，需要某种机制将每一行的各各列连接起来（列存通常采用存储相同的顺序来隐式匹配记录）。需要实现这种方法最简单就是增加一个整数的位置列（比主键更好，因为有时候主键可能多复合的），这种方法将为每个逻辑列创建一个物理表。当从同一关系中获取多个列时，查询将被重写以对位置属性执行 join。在实现中，默认情况下，System X 为此目的选择使用 hash join，这个效率很低下。 出于这个原因，我们尝试在每个表的位置列上添加聚集索引，并强制 System X 使用 index join，但这并没有提高性能，索引访问引起的额外 I&#x2F;O 使它们比 hash join 更慢。</p><h2 id="Index-only-plans"><a href="#Index-only-plans" class="headerlink" title="Index-only plans"></a>Index-only plans</h2><p>垂直分区方法有两个问题。 首先，它要求在每一列中存储位置属性，这会浪费磁盘和I&#x2F;O带宽。 其次，大多数行存储在每个元组上存储一个相对较大的 header，这进一步浪费了空间（列存储通常，将header存储在单独的列中以避免这些开销）。</p><p> 为了改善这些问题，本文考虑的第二种方法使用 index-only plans，其中使用标准的、面向行的设计存储存储基础的 schema，但在每个表的每一列上添加一个额外的非聚集 B+Tree 索引（添加二级索引）。</p><p> index-only plans 需要数据库的特殊支持，但由 System X 实现，通过构建满足每个表上的谓词的 (record-id,value) 对列表来工作，在内存中合并在一张表上的多个谓词，通过对比每个二级索引的 rid 来去除一些不必要的行。 当必填字段没有谓词时，可以生成列中所有 (record-id,value) 对的 list。 看到这样可以不实际访问实际的数据，但是仍然可以存储对应的 rids，也不存储重复的列值，而且由于没有了 header 的开销，所以 overhead 更小了。</p><blockquote><p>Q：这里的 header 指什么？指类似 innodb page 上的 meta 信息吗？每个 tuple 都有的</p></blockquote><p>这个方法在没有谓词的列上性能会比直接扫描 heap file 更差，因为要走二级索引。所以一个优化手段是创建具有复合键的索引（composite index），在无谓词的列上和其他列一起建立二级索引。 通过<strong>将每个维度表的主键存储为该维度表属性的索引的辅助排序属性</strong>，这样可以高效地访问到需要与事实表连接的维度的主键值。</p><h2 id="Materialized-Views"><a href="#Materialized-Views" class="headerlink" title="Materialized Views"></a>Materialized Views</h2><p>实现一些物化视图，这些物化视图给每个 query 都创建一组最佳视图，都只包含该 flight 中查询所需要的列，而不是提前预先做好 join。做这个优化的目的是让 System X 只从磁盘读取想要的数据，避免额外的存储 record-id 或者 position，也避免给每个 tuple 都存储 tuple header。</p><h1 id="5-Column-Oriented-Execution"><a href="#5-Column-Oriented-Execution" class="headerlink" title="5 Column-Oriented Execution"></a>5 Column-Oriented Execution</h1><h2 id="5-1-Compression"><a href="#5-1-Compression" class="headerlink" title="5.1 Compression"></a>5.1 Compression</h2><p>面向列的压缩算法压缩数据并且操作时以这种压缩格式保存数据已经证明可以将查询性能提高一个数量级。存储在列中的数据比行中的数据更好压缩，而且压缩算法在信息熵低（数据局部性高）的数据上表现更好。此外，例如如果数据按照某一列排序，那么该列式超级可压缩的（相同值可以进行 RLE）。</p><p>上面说的只会影响压缩率，但是磁盘空间很便宜。压缩更重要的是可以提高性能，因为如果数据被压缩，那么当数据从磁盘读入内存（或从内存读入 CPU）时，在 I&#x2F;O 上花费的时间就会减少。 因此，一些优化压缩比的“重量级”压缩方案（例如 Lempel-Ziv, Huffman, or arithmetic encoding）可能不如牺牲压缩比以进行解压的“轻量级”压缩方案上性能更好。 事实上，压缩可以提高查询性能，而不仅仅是节省 I&#x2F;O。 如果面向列的查询执行器可以直接对压缩后的数据进行操作，就可以完全避免解压，进一步提高性能。例如对于RLE的数据进行计数的时候可以直接计算，进一步降低CPU消耗。</p><p>行存储中的压缩与列存储中的压缩之间的最大区别是对列进行排序（或二次排序）并且列中连续重复相同值的情况。如果是行存的话，由于周围的值的影响，排序会很难搞。因此，一般来说，如果查询访问的大部分列具有某种程度的顺序，则压缩会对查询性能产生更大的影响。</p><h2 id="5-2-Late-Materialization"><a href="#5-2-Late-Materialization" class="headerlink" title="5.2 Late Materialization"></a>5.2 Late Materialization</h2><p>延迟物化，来自多个列的数据必须组合在一起成一个一行的 tuple，类似的 tuple Materialization 在列存系统中很常见。早期物化也就是将列存数据读到内存中来之后就组合，然后利用一些行存计算的 operator 计算并不能很好的提升计算性能。</p><p>比较新的列存例如X100，C-Store都很晚才进行物化。例如，一个查询在两列上应用谓词，并从所有传递谓词的元组中投射第三个属性。 在使用 延迟物化 的列存储中，谓词分别应用于每个属性的列，并生成通过谓词的值的位置列表（列内的序数偏移量）。 根据谓词的选择性，这个位置列表可以表示为一个简单的数组、一个 bitmap（其中第 i 位中的 1 表示第 i 个值通过了谓词）或一组位置范围。 然后将这些位置表示相交（如果它们是位串，则可以使用按位 AND 运算）以创建单个位置列表。 然后将此列表发送到第三列以提取所需位置的值。</p><p>延迟物化有四个优点：</p><ol><li>select 和 agg 操作符倾向于让某些物化变得不必要</li><li>使用面向列的压缩方法压缩数据，则必须在将值与其他列的值组合之前对其进行解压缩。 这消除了直接对上述压缩数据进行操作的优势。</li><li>直接使用列数据操作时候，缓存性能提升</li><li>block iteration 对于固定长度属性具有更高的性能</li></ol><h2 id="5-3-Block-Iteration"><a href="#5-3-Block-Iteration" class="headerlink" title="5.3 Block Iteration"></a>5.3 Block Iteration</h2><p>为了处理一系列元组，行存储首先遍历每个元组，然后需要通过元组表示接口从这些元组中提取所需的属性。 在许多情况下，例如在 MySQL 中，这会导致一次处理一个元组，其中有 1-2 个函数调用来为每个操作从元组中提取所需的数据（如果它是一个小表达式或谓词计算，与函数调用相比成本较低）。</p><p>最近的工作表明，如果元组块立即可用并在单个运算符调用中操作，则可以在行存储中减少元组处理的一些每元组开销，这在 IBM 中实现了 DB2 [。 与行存储中的逐案实现相反，在所有列存储（我们知道）中，来自同一列的 block of value 在单个函数调用中被发送到运算符。 此外，如果不需要属性提取，如果列是固定宽度的，这些值可以直接作为数组进行迭代。 将数据作为数组进行操作不仅可以最大限度地减少每个元组的开销，而且还可以利用现代 CPU 的并行性潜力，因为可以使用循环流水线技术 。</p><h2 id="5-4-Invisible-Join"><a href="#5-4-Invisible-Join" class="headerlink" title="5.4 Invisible Join"></a>5.4 Invisible Join</h2><p>对数据仓库的查询，特别是对使用STAR模式建模的数据仓库的查询，通常具有以下结构：使用一个（或多个）维度表上的选择谓词来限制事实表中的 tuple 。 然后，对受限事实表进行一些聚合，通常根据其他维度表属性进行分组。 因此，需要为每个选择谓词和每个聚合分组执行事实表和维度表之间的连接。 Star Schema Benchmark 的 Query 3.1 就是一个很好的例子。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span> c.nation, s.nation, d.year,<br>       <span class="hljs-built_in">sum</span>(lo.revenue) <span class="hljs-keyword">as</span> revenue<br><span class="hljs-keyword">FROM</span> customer <span class="hljs-keyword">AS</span> c, lineorder <span class="hljs-keyword">AS</span> lo,<br>     supplier <span class="hljs-keyword">AS</span> s, dwdate <span class="hljs-keyword">AS</span> d<br><span class="hljs-keyword">WHERE</span> lo.custkey <span class="hljs-operator">=</span> c.custkey<br>  <span class="hljs-keyword">AND</span> lo.suppkey <span class="hljs-operator">=</span> s.suppkey<br>  <span class="hljs-keyword">AND</span> lo.orderdate <span class="hljs-operator">=</span> d.datekey<br>  <span class="hljs-keyword">AND</span> c.region <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;ASIA&#x27;</span><br>  <span class="hljs-keyword">AND</span> s.region <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;ASIA&#x27;</span><br>  <span class="hljs-keyword">AND</span> d.year <span class="hljs-operator">&gt;=</span> <span class="hljs-number">1992</span> <span class="hljs-keyword">and</span> d.year <span class="hljs-operator">&lt;=</span> <span class="hljs-number">1997</span><br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> c.nation, s.nation, d.year<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> d.year <span class="hljs-keyword">asc</span>, revenue <span class="hljs-keyword">desc</span>;<br></code></pre></td></tr></table></figure><p>此查询查找 1992 年至 1997 年期间居住在亚洲并购买亚洲供应商提供的产品的客户的总收入，按客户所在国家&#x2F;地区、供应商所在国家&#x2F;地区和供应商所在国家&#x2F;地区的每个唯一组合分组 交易的年份。</p><p>执行这些类型的查询的传统计划是按照谓词选择性顺序进行管道连接。 例如，如果 c.region &#x3D; ‘ASIA’ 是最具选择性的谓词，则首先执行 lineorder 和 customer 表之间的 custkey 连接，过滤 lineorder 表，以便只保留来自居住在亚洲的客户的订单。 在执行此连接时，这些客户的国家被添加到连接的客户订单表中。 这些结果通过管道传输到与供应商表的连接中，其中应用了 thes.region &#x3D; ‘ASIA’ 谓词并提取了 s.nation，然后是与数据表的连接和应用的年份谓词。 然后对这些连接的结果进行分组和聚合，并根据 ORDER BY 子句对结果进行排序。</p><p>传统计划的替代方案是延迟物化 join 技术。 在这种情况下，谓词应用于 c.region 列 (c.region &#x3D; ‘ASIA’)，并在与该谓词匹配的位置提取客户表的客户键。 然后将这些键与事实表中的客户键列连接起来。 这个连接的结果是两组位置，一组用于事实表，一组用于维度表，指示来自各个表的哪些元组对通过连接谓词并被连接。 通常，这两个位置列表中最多有一个是按排序顺序生成的（连接中的外表，通常是事实表）。 然后从 c.nation 列的这个（乱序）位置集合中提取值，连同来自其他事实表列（供应商键、订单日期和收入）的值（使用有序的位置集合） . 然后对供应商和日期表执行类似的连接。</p><p>这些计划中的每一个都有一系列缺点。 在第一种（传统的）情况下，在连接之前构建元组排除了第 5.2 节中描述的所有延迟实现的好处。 在第二种情况下，需要以错位顺序从维度表分组列中提取值，这可能会产生很大的成本。</p><p>作为这些查询计划的替代方案，我们引入了一种我们称为 invisible join 的技术，它可以在面向列的数据库中用于 STAR PATTERN 上的外键&#x2F;主键连接。 它是一个延迟物化连接，但最大限度地减少了需要乱序提取的值，从而减轻了上述两组缺点。 它的工作原理是将连接重写为事实表中外键列的谓词。 这些谓词可以通过使用散列查找（在这种情况下模拟 hash join ）或使用更高级的方法来计算，例如我们称为谓词间重写的技术，将在下面的第 5.4.2 节中讨论。</p><p>通过将 join 重写为事实表列上的选择谓词，它们可以与应用于事实表的其他选择谓词以及先前工作中描述的任何谓词应用算法同时执行可以使用。 例如，可以并行应用每个谓词，并使用快速位图操作将结果合并在一起。 或者，谓词应用程序的结果可以通过管道传输到另一个谓词应用程序中，以减少必须应用第二个谓词的次数。 只有在应用了所有谓词之后，才会从相关维度中提取适当的元组（这也可以并行完成）。 通过在执行此提取之前等到所有谓词都已应用，可以最大限度地减少无序提取的数量。</p><blockquote><p>也就是先把所有的 选择谓词都做完，然后再去提取对应的表列的值</p></blockquote><p>不可见连接扩展了之前关于提高星型模式连接性能的工作，这让人想起半连接通过利用面向列的布局，并重写谓词以避免哈希查找，如 如下面所描述的。</p><h3 id="join-details"><a href="#join-details" class="headerlink" title="join details"></a>join details</h3><p>不可见连接分三个阶段执行连接。 首先，将每个谓词应用于适当的维度表以提取满足谓词的维度表键列表。 这些键用于构建哈希表，该哈希表可用于测试特定键值是否满足谓词（哈希表应该很容易放入内存，因为维度表通常很小并且表仅包含键）。 图 2 显示了针对某些示例数据执行上述查询的第一阶段的示例。</p><p><img src="/store_join_phase1.png" alt="image-20230207144918003"></p><p>在下一阶段，每个哈希表用于提取满足相应谓词的记录在事实表中的位置。 这是通过使用事实表外键列中的每个值探查哈希表，创建外键列中满足谓词的所有位置的列表来完成的。 然后，将来自所有谓词的位置列表相交以生成事实表中满足位置 P 的列表。 图 3 显示了第二阶段的执行示例。请注意，位置列表可以是明确的位置列表，也可以是示例中所示的位图。</p><p><img src="/store_join_phase_2.png" alt="image-20230207145006199"></p><p> 连接的第三阶段使用事实表中满足位置 P 的列表。 对于包含回答查询所需的维度表的外键引用的事实表中的每个列 C（例如，在选择列表、分组依据或聚合子句中引用维度列的位置），外键值来自 C是用P提取出来的，在对应的维表中查找。 请注意，如果维度表键是一个从 1 开始的排序的、连续的标识符列表（这是常见的情况），那么外键实际上表示所需元组在维度表中的位置。 这意味着可以使用此位置列表直接提取所需的维度表列（这只是一个快速数组查找）。</p><p>这种直接的数组提取是（连同维度表通常很小，因此被查找的列通常可以放在 L2 缓存中的事实）为什么这种连接不会遭受上述先前发布的延迟物化的陷阱的原因。 join 方法由于维度表值提取的乱序性质，最终位置列表提取非常昂贵。 此外，需要提取的数值被最小化，因为 P 中的位置数取决于整个查询的选择性，而不是仅取决于到目前为止已执行的查询部分的选择性。</p><p>图 4 显示了第三阶段的执行示例。请注意，对于日期表，键列不是从 1 开始的经过排序的连续标识符列表，因此必须执行完全连接（而不仅仅是一个 位置提取）。 此外，请注意，由于这是一个外键主键连接，并且由于已经应用了所有谓词，因此对于相交位置列表中的每个位置，从事实中保证每个维度表中只有一个结果。 这意味着来自第三阶段的每个维表连接都有相同数量的结果，因此每个连接都可以单独完成，并在稍后的查询计划中将结果组合（缝合在一起）。</p><p><img src="/store_join_phase3.png" alt="image-20230207145128497"></p><h3 id="Between-Predicate-Rewriting"><a href="#Between-Predicate-Rewriting" class="headerlink" title="Between-Predicate Rewriting"></a>Between-Predicate Rewriting</h3><blockquote><p>就是将前面提到的 hash join 改写成 between 谓词判断，需要维度表的某些键的集合是连续的，这样可以直接在事实表上进行裁剪，而不是使用 hash join 去进行查找</p></blockquote><p>正如到目前为止所描述的，这个算法只不过是另一种思考面向列的 semijoin 或 late Materialization hash join 的方法。 尽管连接的散列部分被表示为事实表列上的谓词，但实际上应用谓词的方式与执行（ Late Materialization ） hash join 的方式之间几乎没有区别。 将连接表示为谓词的优势在常见情况（对于星型模式连接）中发挥作用，在这种情况下，在应用谓词之后保留的维表中的键集是连续的。 在这种情况下，可以使用我们称为“between-predicate rewriting”的技术，其中可以将谓词从事实表上的哈希查找谓词重写为外键位于之间的“between”谓词 键范围的两端。 例如，如果在应用谓词后有效的连续键集是键 1000-2000，那么不是将这些键中的每一个插入到哈希表中，而是探查哈希表中的每个外键值 事实表，我们可以简单地检查外键是否在 1000 到 2000 之间。如果是，则元组连接； 否则它不会。 由于显而易见的原因，谓词之间的执行速度更快，因为可以直接评估它们而无需查找任何内容。</p><p>应用此优化的能力取决于这些有效维度表键的集合是连续的。 在许多情况下，此属性不成立。 例如，未排序字段上的范围谓词会导致结果位置不连续。 甚至对于已排序字段上的谓词，按该属性对维度表进行排序的过程也可能会重新排序主键，因此它们不再是有序的、连续的标识符集。 然而，通过使用字典编码来实现密钥重新分配（而不是压缩），可以轻松缓解后一种担忧。 由于键是唯一的，因此字典对列进行编码会导致字典键成为从 0 开始的有序连续列表。只要事实表外键列使用相同的字典表进行编码，哈希表就可以在 -可以执行谓词重写。</p><p>此外，关于优化仅适用于维度表的已排序列的谓词的断言并不完全正确。 事实上，数据仓库中的维度表通常包含粒度越来越细的属性集。 例如，SSBM 中的日期表有一个年列、一个年月列和完整的日期列。 如果表按年排序，第二次按年月排序，第三次按完整日期排序，那么对这三列中任何一列的相等谓词都将产生一组连续的结果（或对已排序的结果进行范围谓词） 柱子）。 又如，供应商表有地区列、国家列和城市列（一个地区有多个国家，一个国家有多个城市）。 同样，从左到右排序将导致对这三列中的任何一列的谓词产生连续的范围输出。 由于在连续查询中汇总数据的 OLAP 实践（按地区告诉我利润，按国家告诉我利润，告诉我按城市告诉我利润），数据仓库查询经常访问这些列。 因此，“谓词间重写”可以比人们最初预期的更频繁地使用，并且（正如我们在下一节中展示的那样）通常会产生显着的性能提升。</p><p>请注意，谓词重写不需要更改查询优化器来检测何时可以使用此优化。 根据维表评估谓词的代码能够检测结果集是否连续。 如果是这样，事实表谓词将在运行时重写。</p><h1 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6 Experiments"></a>6 Experiments</h1><p>SSB 测试</p><h2 id="6-1-Motivation-for-Experimental-Setup"><a href="#6-1-Motivation-for-Experimental-Setup" class="headerlink" title="6.1 Motivation for Experimental Setup"></a>6.1 Motivation for Experimental Setup</h2><h2 id="6-2-Column-Store-Simulation-in-a-Row-Store"><a href="#6-2-Column-Store-Simulation-in-a-Row-Store" class="headerlink" title="6.2 Column-Store Simulation in a Row-Store"></a>6.2 Column-Store Simulation in a Row-Store</h2><p>用行列模拟列存。</p><p>可以看到还是物化视图效果最好，水平分区和全索引效果最差。</p><p><img src="/row_store_exper.png" alt="image-20230207150647009"></p><h2 id="6-3-Column-Store-Performance"><a href="#6-3-Column-Store-Performance" class="headerlink" title="6.3 Column-Store Performance"></a>6.3 Column-Store Performance</h2><p><img src="/store_column_expr1.png" alt="image-20230207151424895"></p><p><a href="http://www.ayqy.net/blog/database-denormalization/">反范式化</a> 是一种不错的优化手段</p><blockquote><p>所谓反范式化，是一种针对遵从设计范式的数据库（关系模式）的性能优化策略：</p><blockquote><p>Denormalization is a strategy used on a previously-normalized database to increase performance.</p></blockquote><p>P.S.注意，<em>反范式化不等于非范式化</em>（<a href="https://en.wikipedia.org/wiki/Unnormalized_form">Unnormalized form</a>），反范式化一定发生在满足范式设计的基础之上。前者相当于先遵守所有规则，再进行局部调整，故意打破一些规则，而后者全然不顾规则</p><p>通过增加冗余数据或对数据进行分组，<em>牺牲一部分写入性能，换取更高的读取性能</em>：</p><blockquote><p>In computing, denormalization is the process of trying to improve the read performance of a database, at the expense of losing some write performance, by adding redundant copies of data or by grouping data.</p></blockquote><p>在设计范式的约束下，数据表中没有冗余信息（某个数据只存放在某张表的某个单元格中），为了得到某个数据可能需要一系列的跨表查询，因而读操作性能不佳，但写操作很快，因为更新数据时只需要修改一处</p><p>反范式化就是要打破这种约束，<em>把某些数据在不同的地方多放几份</em>，以加快数据检索速度：</p><blockquote><p>The opposite of normalization, denormalization is the process of putting one fact in many places.</p></blockquote></blockquote><h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h1><p>在本文中，我们在数据仓库基准 SSBM 上将 C-Store 的性能与商业行存储系统的几种变体进行了比较。 我们表明，尝试通过垂直分区和仅索引计划等技术在行存储中模拟列存储的物理布局不会产生良好的性能。 我们将这种缓慢归因于高元组重建成本，以及窄的垂直分区表中的高每元组开销。 我们分解了列存储能够如此有效地处理面向列的数据的原因，发现 <code>late materialization</code> 将性能提高了三倍，而 <code>compress</code> 平均提供了大约两倍，或者一个数量级访问排序数据的查询的幅度。 我们还提出了一种新的连接技术，称为 <code>invisible join</code>，可将性能进一步提高约 50%。</p><p>这项工作的结论并不是说在行存储中模拟列存储是不可能的。 相反，这种模拟在当今的行存储系统上表现不佳（我们的实验是在 System X 的最新产品版本上进行的）。 一个成功的面向列的模拟将需要一些重要的系统改进，例如虚拟记录 ID、减少元组开销、排序数据的快速合并连接、跨多个元组的运行长度编码，以及一些面向列的查询执行技术，如 直接操作压缩数据、块处理、不可见连接和延迟物化。 其中一些改进已经实施或提议在各种不同的行存储中实施； 然而，构建一个完整的行存储可以在列存储表现良好的工作负载上转换为列存储是一个有趣的研究问题。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Snowflake NSDI&#39;20 | Building An Elastic Query Engine on Disaggregated Storage</title>
    <link href="/2023/02/06/snowflake-NSDI-20-Building-An-Elastic-Query-Engine-on-Disaggregated-Storage/"/>
    <url>/2023/02/06/snowflake-NSDI-20-Building-An-Elastic-Query-Engine-on-Disaggregated-Storage/</url>
    
    <content type="html"><![CDATA[<h1 id="要点概括"><a href="#要点概括" class="headerlink" title="要点概括"></a>要点概括</h1><p>本文链接：<a href="https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/vuppalapati-nsdi22.pdf">Building-An-Elastic-Query-Engine-on-Disaggregated-Storage | NSDI’ 20</a> </p><p>这篇文章是 snowflake 在实际数据分析下对于 snowflake 设计的一篇总结分析，并不是整体架构的设计和介绍。</p><p>主要介绍了<strong>临时存储系统的设计，任务调度，资源弹性和多租户</strong>的一些设计和数据分析上的结果和未来的展望，这个时候 snowflake 已经很成功了，相比四年之前刚提出论文发现了更多实际的问题。</p><blockquote><p>不清楚 snowflake 架构可以先看2016年的第一篇论文 <a href="https://event.cwi.nl/lsde/papers/p215-dageville-snowflake.pdf">The Snowflake Elastic Data Warehouse</a> </p></blockquote><p>下面用一些话总结下本文的要点：</p><ul><li>临时存储系统（Ephemeral Storage System）主要缓存中间计算结果（join）和远程表文件数据，同时为了一致性也作为写入的 write-through cache</li><li>任务调度也是和临时存储系统的特性相耦合的，在临时存储系统上进行 locality-aware 的调度 + work stealing</li><li>计算资源&#x2F;临时存储的弹性是在 VW 之间使用惰性一致性哈希进行的，防止数据 reshuffle 产生大量流量</li><li>多租户下的资源共享主要是计算和存储，云环境下计算隔离已经解决，现在挑战是设计一个共享的临时存储系统，主要挑战来自于在支持细粒度的弹性，而不会牺牲跨租户的隔离属性（很难决策如何 evict 哪些租户的哪些数据）</li></ul><p>这篇文章最好的一点就是从数据分析出发，能够给现在很多云数仓的设计和实现提供很多避坑的参考，但是本文由于篇幅也没有介绍更多的细节了，比较笼统。</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>这篇文章介绍了在 <code>snowflake </code> 的一些使用经验，<code>snowflake</code> 是一个支持SQL的云原生数据仓库，是最先进的数据库之以，snowflake 的设计有三个主要的设计目标：1. 计算和存储的弹性 2. 支持多租户 3. 高性能。</p><p>这篇文章介绍了<code>snowflake</code>的设计与实现，也介绍了这些年云上基础设施（新硬件，细粒度的计费）的一些变化是如何影响<code>snowflake</code>系统的设计和优化的。这篇文章使用系统收集了每14天一个周期的七千万的queries的数据来说明现在存在的问题，以及突出了多个方面的新研究上的挑战，包括<strong>存储系统和高性能查询执行引擎的设计</strong>。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>shared-nothing 的架构一直是传统的查询执行引擎和数据仓库的基础，在这种架构下，持久化的数据被划分到一个个的计算节点上，每个都只负责他自己保存的本地数据。这种shared-nothing的架构让计算能够变得很容易scale-out，能够提供不错的任务隔离性和比较好的数据亲和性，从而在各种 workload 下都能比较好的工作。但是也有以下的一些缺点：</p><ul><li>Hardware-workload mismatch<ul><li>Shared-nothing 的架构尝试在计算节点上的 CPU, memory, storage, bandwidth resources 之间寻求一个完美的平衡来适应不同的 workload。例如，一个高带宽轻量的计算节点可能不适合重计算轻带宽的查询，但是很多人希望执行在同一套配置上各种不同类型的query，而不是给每种查询配置一个新的集群。所以为了达到这个性能目标，通常来说资源都必须被过度配置，这会导致平均资源利用严重不足以及更高的成本。</li></ul></li><li>Lack of Elasticity<ul><li>即使硬件资源可以达到对workload的需求，shared-nothing架构本身的 static-parallelism 和数据分片也会限制 data skew 和随时间变化的 workload。shared-nothing 的弹性不够，例如在计算中通常需要通过增加、删除节点来提供弹性，但是在这种架构下添加删除节点将会导致数据的 reshuffle，会消耗很大的网络带宽，同时也会带来性能的下降，因为进行 data reshuffle 的节点通常也用于计算</li></ul></li></ul><p>传统的数仓处理的query通常都有可预测的模式和速率，例如数据一般都来在OLTP系统中的交易数据。但是这种情况今天被改变了，因为有了大量难以控制的外部数据源（Logs, social media, web applications …），这将导致很多临时的、随时间变化并且难以预测的查询。对于这种查询，shared-nothing的架构将导致成本过高、不灵活、性能差以及效率低下的特点。</p><p>为了突破这个限制，设计并实现了 snowflake，一个弹性、具有事务查询特点的SQL查询引擎。snowflake 的主要目标是为了解决在 shared-nothing 架构中存储和计算的耦合，解决方法就是将二者解耦。snowflake 将数据存放在 Amazon S3 类似的系统中，这些系统能够提高高可用和按需付费的弹性。计算弹性采用了一些提前预热的计算节点池，能够根据需求分配给用户来使用。</p><p>snowflake 的系统设计有两个关键点：</p><ol><li>使用了定制存储系统用来管理和交换在计算节点之间的临时&#x2F;中间数据（tables exchanged during joins），这种临时存储系统很有必要，因为现有的存储有两个大的限制存在<ol><li>现有的存储（S3）不能提供必要的低延迟和高吞吐，会让计算节点再交换中间数据的时候被阻塞住</li><li>S3 等提供了更强的可用性和持久性语义，这是中间数据所不需要的</li></ol></li><li>snowflake 也将这种临时存储系统用作持久化数据的 <code>write-through cache</code>，结合自己实现的查询调度机制，Snowflake 能够减少由计算存储分解引起的额外网络负载，并减轻减少数据局部性的性能开销。</li></ol><p>这篇文章将会讲 snowflake 的系统设计，着重于 临时存储系统的设计，查询调度，支持多租户的弹性和效率。下面是一些重要的发现：</p><ul><li>用户的查询种类有多样性。read-only: 28%, write-only: 13%, read-write: 59%</li><li>查询生成的中间数据量大小相差多个数量级，可能从 GB -&gt; TB，而且查询生成的中间数据量和读取的持久化数据量 or 查询执行时间没有直接的关系</li><li>在DW 中常见的 data skew 和临时访问模式下，即使本地存储容量很小，也能为其提供平均很高的cache hit rate（60%~80%）</li><li>有些客户利用了对弹性资源的需求（20%左右），计算节点的数量可能发生多达两个数量级的变化</li><li>虽然峰值利用率很高，但是平均资源利用率很低，CPU、内存、网络 Tx 和网络 Rx 的平均利用率分别为～51%、～19%、～11%、～32%。</li></ul><p>本文的研究也证实了目前社区中有些很棒的研究方向：</p><ul><li>解耦计算和临时存储<ul><li>计算和持久存储解耦以获得存储和计算资源的弹性，但是目前计算和临时存储仍然是紧耦合的。临时存储和计算资源的比例可能相差几个数量级，导致CPU利用率不足或者临时存储抖动。</li></ul></li><li>深层次存储结构<ul><li>snowflake 的临时存储系统也使用类似存算分离的cache思路，采用双层存储系统（内存主要，SSD&#x2F;HDD作为第二层），生产集群中的存储系统结构越来越深，需要新的机制来有效利用。</li></ul></li><li>Pricing at sub-second timescales<ul><li>snowflake 采用预热的节点池为客户提供服务，通常是在小时为粒度的层次上具有成本效应，但是大多数云厂商已经能够提供更细粒度的定价，这将导致在多租户之间的资源弹性和资源共享之间面临新的挑战。</li></ul></li></ul><p>#2 Design Overview</p><p>snowflake 针对持久化存储和临时存储是不一样的策略。</p><h2 id="2-1-Persistent-and-Intermediate-data"><a href="#2-1-Persistent-and-Intermediate-data" class="headerlink" title="2.1 Persistent and Intermediate data"></a>2.1 Persistent and Intermediate data</h2><p>像大多数 DW 系统一样，snowflake 有三种类型的数据</p><ul><li>持久化的数据：用户在数据库中存储的表的数据，每张表都可能同时被多个查询访问，需要长时间存在并且需要强持久性和高可用性的保证</li><li>中间数据：由查询运算符产生（join），通常由参与该查询的节点使用。中间数据是临时存在的，为了避免访问中间数据被阻塞，对中间数据的低延迟、高吞吐的要求高于对于持久性的要求，因为如果中间数据产生问题，只需要再次计算即可</li><li>元数据：例如 object catalogs，从数据库的表到持久化文件存储的映射，statics, transaction logs 等，大部分数据量小，没有什么系统上有趣的挑战</li></ul><h2 id="2-2-End-to-end-System-Architecture"><a href="#2-2-End-to-end-System-Architecture" class="headerlink" title="2.2 End-to-end System Architecture"></a>2.2 End-to-end System Architecture</h2><p>这是 snowflake 高层次的架构图，分为下面四个部分</p><p><img src="/snowflake_arch.png"></p><ul><li>集中控制的云服务：所有客户端都通过向该 snowflake cloud services 提交查询，该层负责访问控制、查询优化、调度、事务管理，并发控制等。CS 被设计为需要提供多租户的能力，需要有多个复制来实现高可用和高拓展性。</li><li>通过 Virtual Warehouse 抽象来进行弹性计算：每个 VW 本质上是一组 AWS EC2 实例，查询以分布式的方式在上面运行。</li><li>弹性的临时存储：与持久化数据存储的要求不一样，需要低延迟、高吞吐来保证计算节点的最小阻塞。该临时存储系统和 VW 的计算节点处于同一位置，并明确设计为在添加、删除节点时自动拓展，不需要数据 reshuffle 或者重组，每个VW都运行自己独立的分布式临时存储系统。</li><li>弹性的持久化存储：snowflake将数据存储在一个远程的、分解、持久化的S3存储中。S3支持存储不可变文件，为了将表存储在 S3 中，Snowflake 将它们水平分区为大的、不可变的文件，这些文件相当于传统数据库系统中的块。 在每个文件中，每个单独的属性或列的值被组合在一起并压缩，如 PAX中那样。 每个文件都有一个 header，用于存储文件中每一列的偏移量，使snowflake能够使用 S3 的部分读取功能来仅读取执行查询所需的列。</li></ul><h2 id="2-3-End-to-end-query-execution"><a href="#2-3-End-to-end-query-execution" class="headerlink" title="2.3 End-to-end query execution"></a>2.3 End-to-end query execution</h2><p>query 执行从客户查询提交给 CS，然后进行查询解析、优化和调度，生成需要执行的任务，随后在 VW 节点上安排这些任务。CS 将跟踪每个查询的进度，收集性能，并在出现故障的时候重新启动节点进行计算。</p><h1 id="3-Dataset"><a href="#3-Dataset" class="headerlink" title="3 Dataset"></a>3 Dataset</h1><p>数据集在 <a href="https://github.com/resource-disaggregation/snowset">https://github.com/resource-disaggregation/snowset</a> 公布，这里的数据集不看语义信息，只看统计特征。</p><ul><li>Read-only: 28%</li><li>Write-only: 13%</li><li>Read-write: 59%</li></ul><h1 id="4-Ephemeral-Storage-System"><a href="#4-Ephemeral-Storage-System" class="headerlink" title="4  Ephemeral Storage System"></a>4  Ephemeral Storage System</h1><h2 id="4-1-Storage-Architecture-and-Provisioning"><a href="#4-1-Storage-Architecture-and-Provisioning" class="headerlink" title="4.1 Storage Architecture, and Provisioning"></a>4.1 Storage Architecture, and Provisioning</h2><p>临时存储中有两个重要的设计原则：</p><ul><li>同时使用内存和SSD：内存已满时溢出到SSD，因为内存的限制太多，容量可能不够</li><li>允许将中间数据溢出到持久化存储S3中，以防止本地SSD容量耗尽</li></ul><p><strong>未来的发展方向: 将计算和临时存储解耦</strong></p><p>对于性能关键的查询，希望中间数据都在内存orSSD中，而不是溢出到S3，这需要准确的资源配置，有两个原因导致无法实现：</p><ol><li>可用节点的数量有限，每个节点提供了固定数量的CPU、内存、存储</li><li>中间数据的大小无法预测，很难根据先验知识预测，</li></ol><p>所以为了解决第一个问题，希望将计算和临时存储解耦；第二个问题更难阶段，对于此类查询，同时实现高性能和高资源利用率将需要计算和临时存储的解耦，以及临时存储系统的细粒度弹性的有效技术。</p><h2 id="4-2-Persistent-Data-Caching"><a href="#4-2-Persistent-Data-Caching" class="headerlink" title="4.2 Persistent Data Caching"></a>4.2 Persistent Data Caching</h2><p>在临时存储系统中的一个重要发现：中间数据是 <code>short-lived</code>。虽然存储中间数据需要大量的内存和磁盘，但是平均很低，所以临时存储系统的容量需要在中间数据和经常访问的持久化数据之间进行统计复用。有两点比较好：1. DW 查询出现了对于持久化数据的显著倾斜 2. 临时存储系统的性能远高于持久数据存储。</p><p>Snowflake 通过“机会主义（opportunistically）”缓存频繁访问的持久数据文件，实现中间数据和持久数据之间的临时存储系统容量的统计复用，这里的机会主义指的是中间数据存储始终优先于缓存持久数据文件。<strong>持久数据文件采用一致性哈希进行分配到节点中，每个节点采用LRU策略决定持久数据文件的缓存和驱逐</strong>。这种缓存改进了许多查询的时间，而且由于优先缓存中间数据，因此可以在不影响中间数据访问性能的情况下实现查询执行时间的性能改进。</p><p>需要注意的是持久数据的缓存为了保证正确的语义需要仔细设计。首先是 <strong>数据的一致性</strong>，为了确保数据的一致性，通过将临时存储系统当做持久存储系统的 <code>write-through cache</code> 来实现，其次，使用基础的一致性哈希时需要在VW弹性拓展时 reshuffle 数据，通过惰性一致性哈希完全避免这种数据的 reshuffle。</p><p>持久化数据被缓存在临时存储系统中意味着持久数据访问的请求的某些子集可以由临时存储系统提供服务。临时存储系统的 <code>write-through</code> 策略导致写入到临时存储系统的数据和写入持久存储的数据量大概相等。</p><p>尽管临时存储容量远小于持久数据（大约为 0.1%），但是 DW 中的cache 命中率非常高，只读查询接近 80%，读写约为 60%。</p><p><strong>未来发展方向：需要在缓存方面做更细致的工作。</strong></p><p>除了访问位置以外，缓存命中率还取决于 <strong>查询可用的缓存大小相较于查询访问的持久数据量</strong>，反过来说，有效的缓存大小取决于 VW 的大小和并发查询执行生成的中间数据量。</p><p>另外两个技术问题：</p><ol><li>端到端的查询性能取决于持久文件命中率和中间数据的I&#x2F;O吞吐，因此优化临时存储系统如何在两者之间分配容量非常重要。例如优先缓存被许多查询缓存的持久文件，而不是只被一个查询使用的中间数据</li><li>现有的存储机制是为了双层存储系统设计的（内存第一层，HDD&#x2F;SSD作为第二层）在 snowflake 中已经有了三个层次，本地计算内存、临时存储系统、远程持久化存储。云上的存储层次结构变得越来越深。Snowflake 使用传统的两层机制：每个节点实现一个本地 LRU 策略用于从本地内存到本地 SSD 的逐出，以及一个独立的 LRU 策略用于从本地 SSD 到远程持久数据存储的逐出。 然而，为了有效地利用加深的存储层次结构，snowflake需要新的缓存机制来有效地协调跨多个层的缓存。</li></ol><h1 id="5-Query-Task-Scheduling"><a href="#5-Query-Task-Scheduling" class="headerlink" title="5 Query (Task) Scheduling"></a>5 Query (Task) Scheduling</h1><h2 id="Locality-arare-task-scheduling"><a href="#Locality-arare-task-scheduling" class="headerlink" title="Locality-arare task scheduling"></a>Locality-arare task scheduling</h2><p>为了充分利用临时存储系统， snowflake 的调度系统是在持久化的数据文件上  locality-aware 的（这些文件可能在临时存储系统中），具体来说，snowflake 使用 table file name 的一致性哈希来将持久数据文件分配给计算节点。调度系统将计算任务分配给其文件被散列到的节点上。</p><p>这种调度决策下，查询的并行性将和数据文件的一致性哈希结果紧密相连。也就是说假设在有 10 个节点的 VW 中，涉及到 100,000 个文件的查询和涉及到 100 个文件的查询都将分布在这十个节点上。</p><h2 id="work-stealing"><a href="#work-stealing" class="headerlink" title="work stealing"></a>work stealing</h2><p>一致性哈希会导致分区不平衡，所以为了避免这个问题，snowflake 将使用 work-stealing，如果任务的完成时间（执行时间 + 等待时间）早于预期，那么将从另一个节点窃取任务。注意：<strong>窃取任务时，读取数据将从远程数据存储中读取，避免了在已经发生过载的节点上增加负载。</strong></p><h2 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h2><p>调度决策有两种方案，</p><ul><li>将任务尽可能与数据缓存结合到一起<ul><li>优点：最大限度减少了读取持久数据的网络流量</li><li>缺点：导致中间数据交换的网络流量增加</li></ul></li><li>将所有任务都放在一个节点上<ul><li>优点：消除中间数据交换的网络流量</li><li>缺点：增加读取持久数据的网络流量</li></ul></li></ul><p>直接选择两种调度程序可能都不是最好的选择，所以需要在两种调度策略种进行中和，然后将任务调度到各个节点上。</p><h1 id="6-Resource-Elasticity"><a href="#6-Resource-Elasticity" class="headerlink" title="6 Resource Elasticity"></a>6 Resource Elasticity</h1><p>存算分离的架构导致能够独立的拓展计算和存储的弹性。</p><p>计算弹性：通过预热的节点池实现，能够在几十秒的粒度上提供弹性</p><p>存储弹性：存储弹性的实现交给了数据存储（S3……）</p><h2 id="6-1-Lazy-Consistent-Hashing"><a href="#6-1-Lazy-Consistent-Hashing" class="headerlink" title="6.1 Lazy Consistent Hashing"></a>6.1 Lazy Consistent Hashing</h2><p>实现弹性的一个大的问题就是临时存储中的数据管理，临时存储系统将会缓存持久数据文件，每个文件只能缓存在VW中始终散列的节点上，所以还是有点类似 shared-nothing 架构了，任何固定分区的架构都需要在拓展节点时进行大量的数据 reshuffle，此外这些节点还会同时跑计算。</p><p>snowflake 采用了惰性一致性哈希解决了这个问题，整体思路很好理解，利用了缓存数据一定存储在远程存储中这一事实进行实现，完全避免了在节点弹性时增加的对数据的 reshuffle。</p><p>例子如下：开始 VW 中五个节点，File1 File6 在 node1 上，这个时候 Task6 将会在 node1 上执行，之后集群增加一个节点 node6，这个时候本应该将 File6 reshuffle 到 node6 上，但是这里先不进行操作，当下次计算任务安排到 node6 的时候，node6 直接从远程存储中读取，node1 中的 file6 将不再被访问，也不需要进行数据的 reshuffle。</p><p><img src="/snowflake_lazy_hash.png" alt="image-20230205225251951"></p><h2 id="6-2-Elasticity-Characteristics"><a href="#6-2-Elasticity-Characteristics" class="headerlink" title="6.2 Elasticity Characteristics"></a>6.2 Elasticity Characteristics</h2><p>大约 20% 的用户会使用到计算弹性的需求，然后这些弹性会在 VW 的生命周期内会产生多达两个数量级的变化。</p><p><strong>未来发展方向</strong></p><p>目前客户并没有利用超过80% VW的弹性。即使确实有对 VW 进行弹性的用户，也有进一步优化的机会。例如有些客户的 query 间隔要比 snowflake 当前 VW 计算节点弹性的时间更长，snowflake 认为产生这个问题的主要原因在于用户对于需求的错误估计（例如设置了太少的节点，查询的频率又太高），导致二者不匹配。</p><blockquote><p>个人感觉有点像计算资源申请上的某种 “抖动”</p></blockquote><p>两个未来的工作：</p><ol><li><p>希望实现查询内粒度的弹性。具体来说，即使在单个查询的生命周期内，资源消耗也会有很大差异。 这在具有许多内部阶段的长时间运行的查询中尤为普遍。 因此，除了以查询到达间隔的粒度自动缩放 VW 之外，理想情况下，即使在执行查询期间，snowflake也希望支持某种级别的任务级弹性。</p></li><li><p>尝试类似 <code>serverless-less</code> 的平台，类似 AWS Lambda，Azure Functions 以及 Google Cloud Functions 这些自动拓展、高弹性以及细粒度的计费功能。目前 snowflake 很难过渡的原因是：<strong>安全性和性能都缺乏对于隔离的支持</strong>，所以 snowflake 可能自己来定制一种类似 <code>serverless-less</code> 的平台，但是需要解决高效远程临时存储访问的问题。</p></li></ol><h1 id="7-Multi-tenancy"><a href="#7-Multi-tenancy" class="headerlink" title="7 Multi-tenancy"></a>7 Multi-tenancy</h1><p>snowflake 当前通过 VW 的抽象来支持多租户。每个 VW 都在一组独立的节点上运行，都具有自己的临时存储系统，这让 snowflake 能够为其客户提供性能隔离。</p><p>snowflake 的 VW 架构导致了一个问题：性能隔离和利用率上的 trade off。</p><p>下面的前四个图可以看到不错的CPU利用率，但是其他的利用率，例如 memory, network 等利用率比较低。观察发现高达 30% 的VW在 CPU 上的利用率的标准差和均值一样大……，这导致利用率不足。</p><p><img src="/snowflake_multi_tenancy.png" alt="image-20230205231404488"></p><p>最近云厂商的发展让 pre-warmed node pool 变得成本效益很差，过去 infra 通过小时计费，具有成本效益，但是现在计价越来越精确，从snowflake 的角度来说更精细的管理意味着成本的降低利润增多，客户也需要这种计价方式来提供更低的成本。</p><p>以前在按小时计费的模型中，只要至少有一个客户 VW 在一小时内使用了一个特定节点， 可以在整个期间向该客户收费。 但是，对于按秒计费，无法向任何特定客户收取预热节点上未使用的周期费用。 这种成本效率低下的情况为转向<strong>基于共享的模型</strong>提供了强有力的理由，其中<strong>计算和临时存储资源在客户之间共享</strong>：在这样的模型中，可以通过<strong>跨共享资源集统计复用</strong>客户需求来提供弹性，避免 需要维护大量预热节点。</p><h2 id="7-1-Resource-Sharing"><a href="#7-1-Resource-Sharing" class="headerlink" title="7.1 Resource Sharing"></a>7.1 Resource Sharing</h2><p>VW 资源使用随时间变化的变化表明snowflake的一些客户工作负载本质上是突发性的。 因此，转向共享架构将使 Snowflake 能够通过细粒度统计多路复用实现更好的资源利用。</p><p>目前的主要挑战是提供接近当前架构的隔离性。</p><p>从客户的角度来看，他们感兴趣的关键指标是查询性能，即端到端查询完成时间。 虽然纯粹的共享架构可能会提供良好的平均情况性能，但在尾部保持良好的性能比较难搞。 需要在 VW 中隔离的两个关键资源是计算和临时存储。 Snowflake 可以利用数据中心环境中计算隔离方面的一些工作。 此外，Snowflake 中的集中式任务调度程序和统一执行运行时使问题比在通用集群中隔离计算更容易。 在这里，转而关注隔离 memory 和 storage 隔离的问题。</p><p>主要的目标是设计一个 <strong>共享的临时存储系统</strong>，（同时使用内存和 SSD），该系统支持细粒度的弹性，而不会牺牲跨租户的隔离属性。实现这个系统有两个主要挑战：</p><ol><li>首先，由于临时存储系统多路复用缓存的持久数据和中间数据，因此这两个部分数据需要共同共享，同时确保跨租户隔离。 虽然 Snowflake 可以利用现有技术来共享缓存，但snowflake需要一种额外了解中间数据共存的机制。 不幸的是，预测缓存条目的有效生命周期很困难。 在确保硬隔离的同时从租户中驱逐空闲缓存条目并将它们提供给其他租户是不可能的，因为snowflake无法预测租户下一次访问缓存条目的时间。 既要定义更合理的隔离保证，又要设计可以提供此类保证的生命周期感知缓存共享机制。</li><li>第二个挑战是在没有跨租户干扰的情况下实现弹性：扩展共享的临时存储系统容量以满足特定客户的需求不应影响共享该系统的其他租户。 例如，如果简单的使用 Snowflake 当前的临时存储系统，隔离属性将会被马上违反。 由于 Snowflake 中的所有缓存条目都一致地散列到相同的全局地址空间中，因此扩展临时存储系统容量最终会触发所有租户的惰性一致性散列机制。 这可能会导致多个租户看到缓存未命中率增加，从而导致性能下降。 解决这一挑战需要临时存储系统为每个单独的租户提供私有地址空间，并且在资源扩展时，仅为那些已分配额外资源的租户重组数据。</li></ol><p><strong>Memory Disaggregation</strong></p><p>VW 的内存平均利用率很低，这点很难顶，因为 DRAM 很贵，虽然共享资源会提高 CPU 和 memory 的利用率，但是不太可能在两个维度上实现最佳利用率，此外 CPU 和 memory 的特点非常不同，需要独立拓展这些资源，准确配置资源是困难的； 由于过度配置内存的成本很高，snowflake需要有效的机制来在多个租户之间共享分解内存，同时提供隔离保证。</p><h1 id="8-Related-Work"><a href="#8-Related-Work" class="headerlink" title="8 Related Work"></a>8 Related Work</h1><ul><li>SQL-as-a-Service systems<ul><li>还有其他几个系统在云中提供 SQL 功能作为服务。 其中包括 Amazon Redshift 、Aurora 、Athena 、Google BigQuery 和 Microsoft Azure Synapse Analytics 。 虽然有一些论文描述了其中一些系统的设计和操作经验，但是这些系统并没有从数据角度分析工作负载和系统特征。</li><li>Redshift 将持久数据的主要副本存储在计算 VM 集群中（S3 仅用于备份）； 因此，它可能无法实现 Snowflake 通过将计算与持久存储解耦而获得的好处。 Aurora和 BigQuery（基于 Dremel的架构）类似于 Snowflake 将计算和持久存储解耦。 然而，Aurora 依赖于定制设计的持久存储服务，该服务能够分离数据库日志处理，而不是传统的 blob 存储。</li></ul></li><li>存算分离的系统<ul><li>Facebook 的key-value 的存储下的 workload 在 flash 存储上进行了分离。 snowflake 发现了这种思路，并在数据仓库工作负载的背景下进一步扩展了它。 Pocket 和 Locus 是为无服务器分析应用程序设计的临时存储系统。 如果我们要在 Snowflake 中分解计算和临时存储，这样的系统将是很好的选择。 然而，这些系统在查询的生命周期内<strong>不提供细粒度的资源弹性</strong>。 因此，他们要么必须先验地了解中间数据大小（用于在提交查询时配置资源），要么如果事先无法获得此类知识，则会遭受性能下降的困扰。 如 4.1 中所述，预测中间数据大小非常困难。 最好扩展这些系统以提供细粒度的弹性和交叉查询隔离。 对远程闪存存储进行高性能访问的技术  也将是有效实现计算和临时存储系统解耦的重要组成部分。</li></ul></li><li>多租户资源共享<ul><li>ESX 服务器开创了虚拟机环境中多租户内存共享的技术，包括 ballooning 和 idle-memory taxation。 Memshare 考虑在单机上下文中 DRAM 缓存中的缓存容量的多租户共享，以最大化命中率的方式在应用程序之间共享未保留的容量。 FairRide 同样考虑分布式中的多租户缓存共享，同时考虑租户之间的数据共享。 类似于这些系统中使用的缓存资源共享和隔离机制对于使 Snowflake 采用资源共享架构非常重要。 如前所述，扩展这些机制以使其了解中间数据和持久数据的不同特征和要求会很有趣。</li></ul></li></ul><h1 id="9-Conclusion"><a href="#9-Conclusion" class="headerlink" title="9 Conclusion"></a>9 Conclusion</h1><p>我们展示了运行 Snowflake 的操作经验，Snowflake 是一种具有最先进的 SQL 支持的数据仓库系统。 我们在本文中涵盖的关键设计和实现方面涉及 Snowflake 如何实现计算和存储弹性，以及多租户环境中的高性能。 随着 Snowflake 已经成长为每天对数 PB 数据执行数百万次查询的数千名客户提供服务，我们认为自己至少取得了部分成功。 然而，使用在 14 天内执行约 7000 万次查询期间从我们系统的各个组件收集的数据，我们的研究强调了我们当前设计和实现的一些缺点，并强调了可能感兴趣的新研究挑战更广泛的系统和网络社区。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022总结</title>
    <link href="/2022/12/26/2022%E6%80%BB%E7%BB%93/"/>
    <url>/2022/12/26/2022%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>竟然又到了年底了，2022又过去了，今年是非常充实忙碌的一年，总体来说就是秋招+毕业两件大事，想想去年的这个时候是2021年底，刚刚翻看之前的总结又感叹着时间飞逝，自己又老了一岁…今年的年度总结多了一个复盘环节：流水账，复盘，good stuff and bad stuff。</p><h1 id="流水账"><a href="#流水账" class="headerlink" title="流水账"></a>流水账</h1><p>今年年初回家很早，元旦的时候应该就在家里，在家呆了两个月，这两个月里面还是主要在给 databend 实习写代码。回忆起来，在几家公司的实习经历里面，自己最喜欢也是呆的最舒服的就是在 databend 了，还记得当时每天早上起来坐地铁去奥森的办公室，最开始去的时候办公室才刚刚弄好，然后基本上也没什么人，大部分时间也就是和虎哥、wangyin哥以及yijun，xp哥和f叔有时候会来。还记得有一张非常青涩的合影，在这里实习也非常开心。大部分时间每天就去楼下吃字节的食堂，偶尔也会去吃吃其他的饺子或者是面啥的。不论有啥问题都可以随时去找虎哥和其他的几个大佬一起请教和讨论，写代码的效率也很高。在这个经历中，最应该感谢的人就是虎哥和sundy哥了，虎哥把我拉过去让我能够在 databend 实习，非常喜欢虎哥待人的方式，没有一点架子，对人也非常热情非常开放，如果以后还有机会还想跟着虎哥做；sundy哥在我的整个实习过程中帮助了我非常多，sundy是我遇到的最好的导师或者叫mentor了，技术非常强的同时也非常会带人，也非常愿意把好的活交给实习生来做，后来也一直请教了他很多问题。</p><p>希望 databend 发展越来越好！</p><p>另外就是在这个寒假终于把驾照考完了，拖了好久的驾照终于拿到手了！</p><img src="photo.png" alt="databend合影" style="zoom:50%;" /><p>然后到了三月，这个时候大概停了一个月的实习，开始写简历刷题开始找实习，当时也面了好几家公司，不过很早就拿到了当时非常想去的阿里云的 polar-x 的offer，也就很早就结束了找暑期实习的时间。当时一共是经历了一个月，从开始准备到最终面试结束拿到offer，后来四月和五月基本继续在 databend 实习，一直到五月底的时候结束实习。</p><p>记得在五月份的时候北京疫情突然比较严重，学校开始劝返，我就在这个时候刚好向老师申请了回家，就去杭州实习了，在五月底的时候到达了杭州，实习就从六月一直到八月。刚开始去杭州还担心没有认识的人会比较难受，后来发现是多余的担心，当时去了之后发现一些本科同学和朋友也在杭州，也经常在周末一起聚。在杭州的时候自己也是第一次自己一个人租房自己生活，也是一段不错的经历，不过还是更希望能够更快和女朋友在一起。</p><p>在 polarx 的时候当时是做 innodb 相关的一些工作，开始两周读了些书写了些简单的代码，后来分配了一个还不错的活，做完了之后发现大家对实习生的期待非常少，其实预期并不期待实习生能够做什么太多的事情，这也让我其实有点难受…大家还是期待实习生打打杂，稍微做点小活就行了，组里整体的氛围其实比较卷，实习阶段基本每天都是九点之后才能走，也没有太多自己个人的时间了。</p><p>六月底在忙完了毕设的中期答辩之后，七月初我开始了正式的秋招准备和面试，这是一场持久的战斗……</p><p>当时一边实习一边准备秋招确实是自己今年效率最高的时候，白天实习写代码，实习每天晚上回去写简历，复习知识点和刷题，每天基本都是十二点之后才上床，周末去公司继续准备简历和面试。然后还要在这个阶段投递了不少公司，后来证明是一个非常正确的决定，所有的公司我基本都是最早的一批投递的，今年的秋招形势是很严峻的，只不过我们基本都是后知后觉了。秋招的主要面试也是在七月和八月，自己在几十场面试中也像打怪升级一样慢慢变得从容起来，整体来说秋招是比较顺利的，面试绝大部分都能顺利通过，也让我这段时间的心情也一直比较高涨。</p><p>八月底回家呆了几天就回学校了，九月份的时候自己其实基本该拿到的offer都拿到了，也就在九月中旬开始全身心投入进去做毕设，毕设一共从九月中旬到十二月中旬，一共差不多三个月的时间，做毕设的每天都是担心自己能不能毕业🎓..万幸最终还是顺利毕业了</p><p>今年最后的秋招开奖环节真是一个比一个能拖，大部分公司今年都开的比往年更晚，导致在十月底十一月的时候很难完全集中精力，每天想的都是：“怎么还不开奖” 这种事情，所幸大部分offer都开的比较满意 ^ ^。</p><p>到了十二月，这个时候每天关注的都是答辩的事情，在快要答辩的时候，突然宣布放开了，在放开之后的几天之内，整个学校大部分都阳了一遍，大家见面打招呼都变成了问候是不是阳性，而这在一周之前都是不敢想的。在答辩结束之后在冷风里和舍友以及同学出去拍了些照片当做离校前的留恋，然后就离开了寒冷的北京回家了。</p><h1 id="重点复盘"><a href="#重点复盘" class="headerlink" title="重点复盘"></a>重点复盘</h1><h2 id="秋招"><a href="#秋招" class="headerlink" title="秋招"></a>秋招</h2><p>秋招中自己做的好的地方在于提前规划和行动很早，这里就不再仔细聊，自己之前写了<a href="https://zhuanlan.zhihu.com/p/593252104">一篇文章</a>总结。</p><h2 id="毕设"><a href="#毕设" class="headerlink" title="毕设"></a>毕设</h2><p>毕设当中，自己做的不好的地方在于没有提前规划，大部分工作都堆到了最后才完成，导致最后自己的时间非常紧张。</p><h1 id="经验和教训"><a href="#经验和教训" class="headerlink" title="经验和教训"></a>经验和教训</h1><h2 id="good-stuff"><a href="#good-stuff" class="headerlink" title="good stuff"></a>good stuff</h2><ul><li>做事情都比较有规划，做事能够有提前量</li><li>做事效率比较高</li></ul><h2 id="bad-stuff"><a href="#bad-stuff" class="headerlink" title="bad stuff"></a>bad stuff</h2><ul><li>做决定不太果断，不太能够很快做出决定</li><li>不太自律，经常熬夜导致第二天效率低下</li><li>太早亮出底牌</li><li>长期的事情后期容易被打乱，导致不能坚持</li></ul><h1 id="New-Year"><a href="#New-Year" class="headerlink" title="New Year"></a>New Year</h1><p>新的一年，新的开始，要继续努力，在新年之前再定下新年的flag，对新生活充满期待！ ^ ^</p>]]></content>
    
    
    <categories>
      
      <category>年度总结</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>23届小硕秋招分享-数据库/存储方向</title>
    <link href="/2022/09/11/23%E5%B1%8A%E5%B0%8F%E7%A1%95%E7%A7%8B%E6%8B%9B%E5%88%86%E4%BA%AB-%E6%95%B0%E6%8D%AE%E5%BA%93:%E5%AD%98%E5%82%A8%E6%96%B9%E5%90%91/"/>
    <url>/2022/09/11/23%E5%B1%8A%E5%B0%8F%E7%A1%95%E7%A7%8B%E6%8B%9B%E5%88%86%E4%BA%AB-%E6%95%B0%E6%8D%AE%E5%BA%93:%E5%AD%98%E5%82%A8%E6%96%B9%E5%90%91/</url>
    
    <content type="html"><![CDATA[<p>秋招终于尘埃落定，整个过程经历了很久，在这里记录分享一下下自己的秋招过程。</p><p>在23届秋招过程中，幸运地收获了十几家公司的offer，大厂以及创业公司都涉猎了部分</p><ul><li>阿里云，数据库内核</li><li>字节，数据库内核</li><li>百度，数据库内核</li><li>快手，基础架构</li><li>美团，大数据基础研发</li><li>京东，京东云存储</li><li>小红书，数据库内核</li><li>smartx，分布式存储</li><li>akuna，C++开发</li><li>metabit，data infra</li><li>Starrocks，数据库内核</li><li>SelectDB，数据库内核</li><li>DolphinDB，数据库内核</li><li>HashData，数据库内核</li><li>MatrixOrigin，数据库内核</li><li>tplink，研发</li><li>..…</li></ul><p>下面分享一下自己在整个求职过程中的记录，自己的求职准备以及一些踩坑分享，本人技术水平和文章措辞水平都有限，整理出来作为对自己求职的一个阶段性总结。</p><h1 id="基本情况"><a href="#基本情况" class="headerlink" title="基本情况"></a>基本情况</h1><p>先简单介绍一下自己的情况，北航计算机小硕，无paper无竞赛，lc 800 题左右（困难题还是很困难TAT），写过一些公开课，有过一些数据库实习，主要写<code>C++</code>和<code>Rust</code>。</p><p>上大学开始才拥有一台笔记本，本科阶段基本摸鱼度过，折腾过一些脚本语言和基本的Web开发，研究生阶段开始自学入门数据库&#x2F;存储相关的领域，觉得很有意思。</p><p>几次实习经历中浅浅涉猎了 kv，数仓（AP）以及mysql的开发和学习，也给了我在数据库领域更大的视角和更丰富的工程实践。</p><p>这里也推荐大家如果有条件要尽可能多的去实习，去工业界能够更快地了解业界的前沿方向和整体行业市场情况，非常有利于的就业（如果有学术理想，那是另外一个道路了）。</p><h1 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h1><p>自己的秋招开始的很早，因为从暑期实习就大概看出来今年的就业市场不太景气（但是也没预料到后来没想到有这么不景气… 。整个秋招面试大概有80+场左右（面到后面人已经麻了，刚开始还比较紧张，后面面多了之后…自己就完全放松了…有时候面试之前还在打游戏-_-||）罪过罪过……没有不尊重公司的意思…</p><p>时间线大概如下：</p><ul><li><p>五月底开始刷题找手感</p></li><li><p>六月中下旬开始写简历和基础复习，关注各大公司招聘和各种招聘帖</p></li><li><p>七月初开始投递简历，面试主要集中在七八九月</p></li></ul><p>这里就记录下部分公司的面试体验，好多公司后来都有点忘记了</p><p><strong>阿里云数据库</strong>：阿里云暑期实习，是电话面试，所以只面了暑期，暑期一共两轮面试都比较基础，一面基本就是基础概念的考察，二面挺有意思，从一个很简单的需求出发进行一个日志系统的设计，这里基本感觉就是靠积累和知识的广度尽可能多吹一吹了…，后来实习转正了</p><p><strong>字节数据库</strong>：字节一共有五轮技术面试，前两轮技术面试都问的比较基础，基本就是数据库的各种基础概念的考察和做题，三面开始我才觉得面试开始聊一些其他的东西，聊一些更big picture的设计，面试官开始从系统设计的角度和我讨论某个需求方案的设计，讨论各种方案的trade off，聊TP，AP系统的界限设计等，总体也是非常不错的面试体验。</p><p><strong>快手服务架构</strong>：一开始内推快手数据库结果内推人说没hc了……就投了一个C++开发，被捞去服务架构那边，面试聊的也比较基础，一面问了很多细节的生产环境的数据，但是自己确实对当时组里的线上实际情况了解很少（也了解不到，只能推断，印象比较深刻的是二面问了高性能服务器的设计，</p><p><strong>美团大数据研发</strong>：暑期拿到offer没去，延续暑期实习的组投递的，组里的leader非常好，不论是暑期还是秋招面试，都像聊天一样聊，给我聊行业内大数据的发展和现状，感觉像是来学习而不是面试，体验非常好，如果自己最开始做大数据方向，也许就去了</p><p><strong>京东云存储</strong>：京东面试三轮，一开始给我推到了java岗位……我简历上都没出现 java 这个词，后来被推去京东云那边做对象存储的组聊了三轮拿了offer</p><p><strong>小红书数据库</strong>：小红书这两年发展很快，前两轮面试也都是基本的写题和简历，聊聊实习项目基本都差不多，没问太多额外的东西，但是第三轮面试面试官似乎不是做db的，所以只能聊聊宏观上的一些架构取舍，没能聊太多细节，另外，他们今年开的包确实都很大</p><p><strong>smartx</strong>：一直很有好感的一家创业公司，从去年日常实习就投递拿到分布式存储offer了，印象十分深刻，还依稀记得当时一面面了一小时四十五分钟，面得口干舌燥，面试官的问题也一直刨根问底，一直问到非常非常底层的实现，当时就发现自己的基础并没有很好的串起来，所以后来自己一直加强基础的学习。。今年秋招面试就非常有趣了，一面随便聊聊，二面就是面试官前辈（还是去年的面试官）给我分享了不少smartx在做的事情，整个过程基本就是他在分享行业以及公司的信息。</p><p><strong>akuna</strong>：因为有朋友在那里实习，所以内推了下，三轮纯英文技术面试，每轮基本上就是写一两道题，题目并不是很难，工程性比较强，比较考验英文交流和写代码的习惯</p><p><strong>StarRocks</strong>：一家做的很棒的数仓创业公司，面的查询优化组，一共有五轮面试，每轮面试的体验都非常好，都能明显感觉到对面的面试官非常懂数据库，不论是从细节层面抑或是宏观层面聊db内核的各个方面，都能够聊的非常愉快，从执行引擎到存储、到分布式数据库的架构的宏观理解、或者是newsql的一些设计，聊的非常舒服的几次面试。</p><p><strong>SelectDB</strong>：面的存储组，三轮面试，感觉面试官对我的经历也很感兴趣，所以和面试官非常能聊得来，也是我最早收到的offer之一，目前的发展也非常迅猛</p><p><strong>百度</strong>：找前辈内推了百度的数据库，一面聊的很好，但是二面开始让我有些不太舒服，面试官似乎不太在乎我做的东西和简历上的内容，几乎不问我的简历，最后还硬问我了一些linux下命令的各种参数代表的含义，……不太懂硬问linux命令参数的意义。。也可能是面试官希望我能够直接去参与到系统的线上运维工作中去😄</p><p><strong>DolphinDB</strong>：很早因为胡神了解到这家公司，暑期实习和秋招都进行投递了，面试流程和面试体验也属于一流的公司，推荐</p><p><strong>unknown</strong>：面了一家杭州的数据库公司（不提名字）…前两轮技术面都挺正常体验也很好，但是最后去线下去和老板当面聊的时候，本人被当场pua了，上升到人生层面的那种…这也是我秋招过程中最糟糕的一次面试经历😓</p><h1 id="求职思路分享"><a href="#求职思路分享" class="headerlink" title="求职思路分享"></a>求职思路分享</h1><p>求职的部分主要分为几个方面，都是出于我自己有限的经验，我觉得对于像我这种资质普通，大学之前都没接触计算机的同学应该是比较好的思路，如果是大神请略过…（很羡慕很多大学之前就接触过计算机、竞赛的同学，那是非常好的经历，可以减少很多入门时的坑）</p><p>先分享一下在我的视角里面面试官或者公司希望招到的应届生的画像（反侦察</p><p>一个<strong>基础扎实</strong>（计算机基础、数据结构算法），有<strong>相关领域的实习</strong>，或者有等价的比较<strong>有价值的比赛或者（开源）项目</strong>，对<strong>业内工业界有一些了解或参与</strong>（加分项）的应届生，是大多数公司都愿意招收培养的应届生。</p><p>当然除此之外还有一些不可忽视的软技能，就比如说如何在面试的时候如何和面试官进行高效有效的沟通，如何更快更高效的展示自己的价值等</p><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><ul><li>计算机基础</li></ul><p>对于应届生来说，最重要的就是把基础打牢了，不要在学习阶段太迷恋于应用层的各种中间件或者框架，因为工作之后有的是时间去学习和实践，在学校期间最为重要的就是把几门基础课上好，特别是 <strong>操作系统、数据库、计算机网络、编译原理</strong>，最好能上一些名校的公开课做做对应的 lab（可以看看这个<a href="https://csdiy.wiki/">自学网站</a>），理解才会更深一些，不过现在在 infra 方面同学基本已经把这个方面的课程刷爆了，在之前做过 lab 就是优势的时代感觉一去不复返，已经快成标配了</p><ul><li>刷题</li></ul><p>刷题要趁早，可以在 leetcode 开个会员督促自己学习，刷一刷经典的题单，多打打周赛，尽量不要让做题成为自己的短板。</p><ul><li>参与开源项目</li></ul><p>参与有价值的开源项目是妥妥的加分项，一般来说开源项目的质量都比闭源的更高，而且带来的影响力也比闭源项目要好得多，能够直接向其他人展示你的代码能力，也能认识更多志同道合的朋友一起学习</p><ul><li>参加比赛</li></ul><p>有时间可以参加各种业内的比赛，可以关注开源之夏、阿里天池比赛、miniob、talent plan等，如果能拿到名次那将是和面试官吹牛的最好方式</p><h2 id="找实习"><a href="#找实习" class="headerlink" title="找实习"></a>找实习</h2><p>如果有机会一定要多去实习，实习阶段的试错成本是最低的，能够让你更快确定以后自己想要的职业和发展的方向。</p><p>找实习的前期一定要做好准备，从简历准备、基础复习、刷题找手感几个方面分别做好，实习基本就没啥问题。</p><p>准备简历时，首先书写一定要突出重点，将能够体现自己价值、水平的部分写在前面，必要时可以用加粗等字体表明，不相关的经历（例如学生会，各种杂活）可以不写，主要是写了也没啥用，……面试官也都懂，尽量用量化的指标去度量自己的成果；其次写简历切忌多和杂，对于写到简历上的东西一定都要非常了解，对于每段经历和都要做好充足的准备，对于面试官来说，他不需要你样样都懂，但是需要你写上去的东西都非常了解，举个例子，例如写上：“了解LSMTree存储引擎”，那么最好将LSM本身的内容、工业界的实践、和BTree的比较、存储引擎的抽象、以及学术界LSMTree 的一些研究进展都可以做一些相关了解，如果被问到那将会是非常大的亮点，这样在面试的时候就能够完全不慌，当然准备是需要时间的，所以建议自己多做一些整理，经过找实习和秋招一两年的时间也将会积累起不少的笔记。</p><p>基础的复习时，建议从两个方面入手，首先是过一些经典的书籍和课程的notes、PPT等，其次是多搜集各种面经上的问题。在经典的课程下建立起一个大纲，然后刷面经的过程中将不会的问题归类和整理，问题虽然看起来多，但是在看过足够多的面经之后你将会发现，反反复复就是那么些问题，然后从大纲出发建立起整体的脉络。</p><p>刷题找手感，刷题是一个永远避不开的话题。特别是在校招当中，做题尤其重要，对于一个校招生，本身就没有什么工程经验和优势，如果连基础的算法题都写不出来，那么面试通过的可能性非常低^ ^（做数据库感觉一般遇到的算法题都不是很难）。</p><h2 id="秋招"><a href="#秋招" class="headerlink" title="秋招"></a>秋招</h2><p>秋招的准备整体和实习差不多，但是一定要注意提前准备和时间线的安排。</p><ul><li><p>尽早关注一些秋招的公司开启的帖子和博客，一般每年秋招开始前牛客和github都会有，可以自己关注一下，同时对于自己特别感兴趣的公司列一些list多关注，记住，投递都是越早越好，赶早不赶晚。</p></li><li><p>多找内推。尽可能找认识的师兄师姐内推，不然面试排期都会排的很晚</p></li><li><p>mock面试。正式面试之前可以多找水平相近或者水平更高的同学相互mock，做一些讲简历经历和问题挖掘。</p></li></ul><h2 id="面试心得"><a href="#面试心得" class="headerlink" title="面试心得"></a>面试心得</h2><p>谈谈自己在面试的时候刻意注意的一些点。</p><ul><li>STAR法则相关。首先对每一段经历，首先要站在全局的角度去理解讲述整个系统的架构设计，解决的核心问题等；其次对于每一项task，要利用STAR原则去讲述每段经历，必要时候可以写下来自己多排练，同时要做一定的拓展，也就是相似的问题多去搜索和思考，思考解决的方法和设计是否有更好的方案；更重要的是对其中涉及到的基础技术要做深入的了解，而不要只是一些简单的使用。</li><li>不要不懂装懂。每个人都有不懂的地方，不懂的地方可以直接说明，切忌切忌不懂装懂，但是对于不会的问题可以将其引入到自己了解到的类似的问题、类似的原理，说明自己的理解即可。</li><li>不紧不慢，逻辑清晰。要记住面试其实是一个沟通的过程，是人和人的沟通，并不是闭卷考试，要努力做到说话不紧不慢，回答每个问题都要逻辑清晰的去思考，想好再说，最好形成自己的方法论，这样才能遇到什么问题都能顺利接下。</li><li>保持平视的心态。^ ^</li></ul><h1 id="数据库学习"><a href="#数据库学习" class="headerlink" title="数据库学习"></a>数据库学习</h1><p>聊一聊自己认为在入门数据库时候的资料和学习的路线，</p><ul><li><p>CMU15445</p><ul><li>明星之课，最好的数据库入门教程，除了做里面的lab之外，一定好好上课和看书，这门课的意义绝不仅仅是 lab，而更重要的是对于初学者更快的更完整的入门构建起整个数据库的框架结构</li></ul></li><li><p>CMU15721</p><ul><li>andy的数据库进阶课，有非常多的经典论文，非常推荐值得阅读，如果之后要做数据库方向，一定要开始读论文，这里面的论文都非常经典，读不完全集论文可以先读课程重点推荐的论文，相信我，你不会后悔开启这个旅程的</li></ul></li><li><p>MIT6.824&#x2F;tinykv</p><ul><li>这两年分布式系统非常火，数据库也和分布式系统结合非常紧密，有余力可以学习下</li></ul></li><li><p>开源项目</p><ul><li>leveldb：还自己自己最早读完的代码就是 leveldb，入门数据库推荐，代码量很小，静下心来很快就能读完，（不过常读常新，有不少细节还是回顾才意识到的</li><li>pg：pg的代码读的不多，但是组织结构确实比之后接触到的 mysql 的代码组织上更容易阅读一些，TP经典推荐</li><li>duckdb：如果想看看 AP 数据库的设计，可以尝试开始看看 duckdb，代码结构非常清晰</li><li>就不推荐太多了，优秀的项目实在太多，这里只推荐最经典的一些…</li></ul></li><li><p>其他</p><ul><li>其实如果能扎实学习下上面的东西，同时<strong>把计算机基础打牢</strong>，找到一份数据库的实习&#x2F;工作就肯定没问题了，但是我相信想要做数据库方向的同学肯定经历更加充沛也非常卷，我也认识很多其他的同学做了非常多的课和项目</li></ul></li></ul><p>相信每一份耕耘都会有一份收获~</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
    <categories>
      
      <category>记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Join算法笔记整理</title>
    <link href="/2022/04/29/Join%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/"/>
    <url>/2022/04/29/Join%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="Join-算法"><a href="#Join-算法" class="headerlink" title="Join 算法"></a>Join 算法</h1><p>一个好的数据库的设计应该是<strong>尽量减少信息的重复和冗余</strong>，通常会将不同的数据放在不同的表中，对数据进行拆解分别存储。为了获取完整的分析数据，我们就需要从多表中取数据，将多个表连接成一个表，方便我们进行分析。</p><p>Join 一般分为 Inner Join 和 Outer Join，下面用两张图表示他们的区别</p><img src="image-20220429145459031.png" alt="image-20220429145459031" style="zoom:50%;" /><p>Inner Join 表示的是两个表的交集，</p><img src="image-20220429145526956.png" alt="image-20220429145526956" style="zoom:50%;" /><p>上图是 Left Outer Join 表示的是两个表，左边表数据行全部保留，右边表保留符合连接条件的行。</p><h2 id="Join-算子的输出"><a href="#Join-算子的输出" class="headerlink" title="Join 算子的输出"></a>Join 算子的输出</h2><p>对于表 <code>R</code> 和 <code>S</code>  ，通过  Join 算子计算之后，Join 算子只会输出在 join attributes 上符合条件的行并且将两个表中的 tuple 连接起来输出。</p><p>实际系统中 Join 算子输出的的 tuple 的内容会不同，这主要取决于 DBMS 的执行器、存储模型和这个 query 本身。下面有几种 join 的通常的输出</p><ul><li>直接输出数据：这种方法将外部表和内部表中的属性值复制到元组中，转换到一个中间结果表中。<ul><li>优势：在于未来查询计划中的操作符永远不需要查询基表来获取更多数据</li><li>缺点：需要更多的内存来物化整个元组</li></ul></li><li>Record ID：在这种方法中，DBMS只复制连接键和记录id匹配的元组。这种方法对于列存储非常理想，因为DBMS不会复制不是查询所需要的列存储的数据。这就是 <code>late materialization</code></li></ul><h2 id="Cost-估算"><a href="#Cost-估算" class="headerlink" title="Cost 估算"></a>Cost 估算</h2><p>这是用于计算不同 join 算法将只会计算磁盘的 I&#x2F;O。这包括从磁盘读取数据的 I&#x2F;O 以及写出中间结果的 I&#x2F;O。注意只有 I&#x2F;O 被考虑到了，因为输出的结果的代价都是一样的。</p><blockquote><p>Join 的本质就是暴⼒匹配，所有tuple的⽐较时间和最终输出的时间都相同，所以⽐较的就是不同算法之间将⻚⾯Page或者说磁盘块加载进⼊内存中IO的次数，IO的次数越少，join算法的效率越⾼</p></blockquote><p>这里对下面的不同 Join 算法的估算做一个设定：</p><ul><li>外部表 R 中有 M 个 page，m 个 tuple</li><li>内部表 S 中有 N 个 page，n 个 tuple</li></ul><h1 id="单机-Join-算法"><a href="#单机-Join-算法" class="headerlink" title="单机 Join 算法"></a>单机 Join 算法</h1><p>这里只是介绍了一些基础的 Join 算法，还有更多针对 OLAP 系统的 Join 算法的改进这里并未总结到一起。</p><h2 id="Nested-Loop-Join"><a href="#Nested-Loop-Join" class="headerlink" title="Nested Loop Join"></a>Nested Loop Join</h2><p>简而言之，这种类型的连接算法由两个嵌套的for循环组成，它们遍历在两个表中的元组，并比较它们。如果元组匹配<strong>联接谓词</strong>，则输出它们。</p><p>位于外层for循环中的表称为外层表，而位于内部for循环中的表称为内部表。</p><p>DBMS总是希望使用<strong>“较小的”表作为外部表</strong>。可以用小来表示元组数或页面数。DBMS还希望尽可能多地缓冲外部表到内存中，同时外部表还可以尝试利用索引在内部表中查找匹配。</p><h3 id="Simple-Nested-Loop-Join"><a href="#Simple-Nested-Loop-Join" class="headerlink" title="Simple Nested Loop Join"></a>Simple Nested Loop Join</h3><p>对于外部表的每个tuple，和内部表中的每个tuple进行匹配。这是最差的情况，因为 DBMS 必须对于外部表中的每个 tuple 都进行一次全量的 scan，</p><p>Cost：<code>M + (m * N)</code></p><blockquote><p>每次加载⼀个外部⻚⾯，每个外部⻚⾯的tuple都加载⼀个内部表的⻚⾯，就需要m*N次加载内部⻚⾯，然后外部表加载M次</p></blockquote><h3 id="Blocked-Nested-Loop-Join"><a href="#Blocked-Nested-Loop-Join" class="headerlink" title="Blocked Nested Loop Join"></a>Blocked Nested Loop Join</h3><p>对于外部表中的每个 block，从内部表中取得每一个 block来进行两个 block 之间的遍历比较。这种算法可以有更少的磁盘 I&#x2F;O，因为 DBMS 对于外部表的每个 block 扫描一次内部表而不是每个 tuple 都要扫描一次内部表</p><p>Cost：<code>M + (M * N)</code></p><blockquote><p>每次加载⼀个外部⻚⾯，每个外部⻚⾯的tuple都加载⼀个内部表的⻚⾯，就需要m*N次加载内部⻚⾯，然后外部表加载M次</p></blockquote><p>当然，如果 DBMS 有 <code>B</code> 块 Buffer 来给 join 计算，那么每次就可以 scan <code>B-2</code> 个外部表的 block 进入到内存中来，将会使用一个 Buffer 来作为临时的输出的 block</p><p>Cost: <code>M + (M / (B - 2)) * N</code></p><blockquote><p>外部表每次加载B-2个⻚⾯，内部表每次加载⼀个，可以⼀次⽐较B-2个外部⻚⾯和⼀个内部⻚⾯，然后⼀共就是内部表的每个⻚⾯加载进⼊都要和<br>B-2个⽐较，⼀共就需要!</p></blockquote><h3 id="Index-Nested-Loop-Join"><a href="#Index-Nested-Loop-Join" class="headerlink" title="Index Nested Loop Join"></a>Index Nested Loop Join</h3><p>前面两种算法会比较差，这是因为DBMS需要对内部表中的每个数据来进行一次全局的 scan。但是如果说两个 join 表的 key 上有索引，那么就可以加速这个查找过程了。DBMS 能够使用存在的索引或者说临时构建一个索引来给 join 使用。</p><p>外部表必须是没有索引的那一个，内部表必须要有索引。假设每个  index 查找的消耗是一个固定的值 <code>C</code></p><p>Cost：<code>M + (m * C)</code></p><blockquote><p>有索引之后，outer表不带索引，inner表带上索引，对于outer表⾥的每个元素，直接根据索引找到对应的⻚⾯加载进⼊内存即可，所以是M+m*C</p></blockquote><h2 id="Sort-Merge-Join"><a href="#Sort-Merge-Join" class="headerlink" title="Sort-Merge Join"></a>Sort-Merge Join</h2><p>简而言之，sort-merge join 就是让两个 table 在他们 join key 上来做一个排序。DBMS 将会使用一些外排序的归并算法来实现，将会移动两个 table 上的指针来计算匹配的 tuple</p><p>这种算法很有用：当一个或者两个表都已经在 join 属性上排好序了或者说输出需要在 join key 上排好序。</p><p>算法的过程：</p><img src="image-20220429152912102.png" alt="image-20220429152912102" style="zoom:50%;" /><p>最坏的情况就是 join 属性在所有的 tuple 上都有相同的值，但是这是不太可能发生的。在这种情况下，Cost 将会变为 <code>M * N</code>。对于大多数情况来说，这个merge cost 将会大概是 <code>M + N。</code></p><p>假设 DBMS 有 B 块 Buffer 来用来做 Join：</p><ul><li>Sort Cost for Table R: <code>2 * M</code> + log<sub>B-1</sub>(M&#x2F;B)</li><li>Sort Cost for Table S: <code>2 * N</code> + log<sub>B-1</sub>(N&#x2F;B)</li><li>Merge Cost: <code>M+N</code></li></ul><blockquote><p>外排序算法：</p><ul><li>2-way external merge sort</li></ul><img src="image-20220429163605582.png" alt="image-20220429163605582" style="zoom:50%;" /><ul><li>k-way external merge sort</li></ul><p>外排序的一些优化：</p><ul><li>Double Buffering Optimization：预取下一次计算的 page ，这将减少 IO 的等待时间，这将使用多线程，因为需要当前正在计算的时候取出下一个 page 的数据</li><li>Using B+ Trees：如果已经在某些排序键上有了B+树索引，那么就可以加速排序，分为 Clustered B+ Tree 和 Uncluster B+ Tree 来分别不同的实现</li></ul></blockquote><h2 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h2><p>简而言之， Hash join 就是用 hash table 将 tuple 根据 join 属性划分为很多个小的 chunk。这种算法将减少 DBMS 对于每个 tuple 所需要比较的次数。hash join 只能被用在计算 join key 上的等值连接中。</p><h3 id="Basic-Hash-Join"><a href="#Basic-Hash-Join" class="headerlink" title="Basic Hash Join"></a>Basic Hash Join</h3><ul><li>Phase 1, Build：首先将扫描外表根据 hash function h<sub>1</sub> 来对 join 属性进行扫描。在 hash表中的key是 join 属性，value 可以是整个 tuple 也可以是 tuple id</li><li>Phase2, Probe: 扫描内部表，同样使用 hash function h<sub>1</sub> 来进行计算来在哈希表中找到匹配的 tuple。因为 hash表中可能有冲突，所以 DBMS 必须检查一下两个 tuple 是否是真的匹配。</li></ul><img src="image-20220429155153602.png" alt="image-20220429155153602" style="zoom:50%;" /><p>如果 DBMS 知道外表的大小，那么可以使用一个 static hash table，如果不知道外表的大小，那么就可以使用一个 dynamic hash table。</p><p>当然也可以在构建外部表的时候同时构建一个 bloom filter 来加速这个过程，可以快速判断某个值是否存在</p><h3 id="Grace-Hash-Join-x2F-Hybrid-Hash-Join"><a href="#Grace-Hash-Join-x2F-Hybrid-Hash-Join" class="headerlink" title="Grace Hash Join &#x2F; Hybrid Hash Join"></a>Grace Hash Join &#x2F; Hybrid Hash Join</h3><p>当内存放不下 table 的时候，DBMS 将会随机将 table 换出和换入，这将让性能非常差。</p><p>Grace Hash Join 是 Basic Hash Join 的拓展，可以将内部 Hash 的结果划分为多个 partition 来写出到磁盘上。</p><ul><li>Phase1, Build: 首先，同时扫描内表和外表，使用 相同的 hash function 来在 join 属性上进行划分， hash table的 bucket 将会被写出到磁盘上。如果一个 bucket 在内存中放不下了，那么DBMS 将会使用 <code>recursive partitioning</code> ，使用一个不同的 hash function 来对 bucket 做进一步的划分，这个过程将会持续进行，知道 bucket 能够让到内存中</li><li>Phase2, Probe: 对于每个 bucket，从 bucket 中会恢复出内表和外表的数据来做一个 nested loop join，因为所有的 page 都能够放置到内存中，所以 join 操作将会非常快</li></ul><img src="image-20220429154945524.png" alt="image-20220429154945524" style="zoom:50%;" /><p>Cost:</p><ul><li>Partition Phase Cost: <code>2 * (M + N)</code></li><li>Probe Phase Cost: <code>M + N</code></li><li>Total Cost: <code>3 * (M + N)</code></li></ul><h1 id="OLAP-的分布式-Join"><a href="#OLAP-的分布式-Join" class="headerlink" title="OLAP 的分布式 Join"></a>OLAP 的分布式 Join</h1><p>这里主要介绍一些在分布式 OLAP 中用到的 Join 算法。</p><p>对于AP的workload，大部分时间都花在了 Join 和从磁盘读取上， 分布式Join的效率取决于目标表的分区方案。</p><ul><li>一种方法是将整个表放在单个节点上，然后执行 Join。 但是，DBMS 失去了分布式 DBMS 的并行性，这违背了拥有分布式 DBMS 的目的， 这么做还需要通过网络进行非常重的数据传输。</li></ul><p>要连接表 R 和 S，DBMS 需要在同一节点上拿到合适的 tuple， 需要将合适的 tuple 传输到合适的节点上，一旦这个过程完成，就会通过上面的单机 Join 算法 来进行计算，所以 我们应该始终发送计算 Join 所需的最少数量的 tuple 到某个节点上。</p><p>下面有四种常见的分布式 Join 的算法：</p><h2 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h2><p>有一张<strong>小表被复制到了每个节点</strong>，但是其他的表在节点间被分区了。每个节点在本地计算 join 的结果然后将结果发送到聚合结果的节点上。</p><img src="image-20220429161457053.png" alt="image-20220429161457053" style="zoom:50%;" /><h2 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h2><p>每个表都被 join 属性分区了，刚好每个节点在本地执行 join 然后将结果发送到聚合结果的节点上</p><img src="image-20220429161618742.png" alt="image-20220429161618742" style="zoom:50%;" /><h2 id="场景三"><a href="#场景三" class="headerlink" title="场景三"></a>场景三</h2><p>表被不同的属性进行了分区。如果其中某个表更小一点，那么可以将小表 broadcast 到所有的节点上进行计算</p><img src="image-20220429161843754.png" alt="image-20220429161843754" style="zoom:50%;" /><h2 id="场景四"><a href="#场景四" class="headerlink" title="场景四"></a>场景四</h2><p>两个表都不是在 join key 上进行分区，DBMS 需要在节点之间 shuffling 数据来让每个节点上都有对应 join key 的分区的数据进行 join</p><img src="image-20220429162614451.png" alt="image-20220429162614451" style="zoom:50%;" /><img src="image-20220429162625551.png" alt="image-20220429162625551" style="zoom:50%;" /><img src="image-20220429162642489.png" alt="image-20220429162642489" style="zoom:50%;" /><h2 id="Semi-Join"><a href="#Semi-Join" class="headerlink" title="Semi-Join"></a>Semi-Join</h2><p>半连接是一种连接运算符，其结果仅包含左表中的列。 分布式 DBMS 使用半连接来最小化 Join 期间发送的数据量。</p><p>它类似于自然连接，只是右表上不用于计算连接的属性受到限制（例如下图， 只发送<code>R.id</code> 到第二个节点上）。</p><img src="image-20220429163016850.png" alt="image-20220429163016850" style="zoom:50%;" /><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://15445.courses.cs.cmu.edu/fall2020/">https://15445.courses.cs.cmu.edu/fall2020/</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>database</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2022-第四期-新学期的烦恼(2.14-3.22)</title>
    <link href="/2022/03/23/2022-%E7%AC%AC%E5%9B%9B%E6%9C%9F-%E6%96%B0%E5%AD%A6%E6%9C%9F%E7%9A%84%E7%83%A6%E6%81%BC-2-14-3-22/"/>
    <url>/2022/03/23/2022-%E7%AC%AC%E5%9B%9B%E6%9C%9F-%E6%96%B0%E5%AD%A6%E6%9C%9F%E7%9A%84%E7%83%A6%E6%81%BC-2-14-3-22/</url>
    
    <content type="html"><![CDATA[<p>这次鸽了好久了周报和总结，其原因有很多，主要是新学期来了自己又迎来了很多事情，所以这段时间做了很多事情吧，然后发现人生每个阶段的焦虑是一个接着一个…永远不可能做完的 TAT，这里总结反思一下这段时间</p><span id="more"></span><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><p>自己这段时间在 databend 继续工作了半个月到大概月底，然后因为自己个人的事情可能没办法继续像之前那么全职去实习了，所以在到学校之后就和虎哥说明了自己的情况，然后工作上的事情最近就比较少了，感谢老板们对我这样一个实习生的理解</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><ul><li>把 15445 的 lab3 和 lab4 做了，差不多告别 15445 了，可以继续读 15721 的论文了，希望能在毕业前把 15721 的论文读完</li><li>6.s081 的 pgtbl 终于结束了，今年 6.s081 做完就好了</li><li>6.830 开了个头，这个可以慢慢来，只是再熟悉一下，希望做下其中的优化器部分来对优化器的实际实现有些了解</li></ul><h1 id="个人"><a href="#个人" class="headerlink" title="个人"></a>个人</h1><p>最近开始了各个公司开始了暑期实习的投递，自己也投了一些感兴趣的公司，到目前为止比较顺利，暑期实习的面试都比较顺利，很少有比较艰难的情况；虽然如此，但是也反映出来自己的一些问题，对于某些问题理解不够深刻，例如 leveldb 的 restart point 设置的长度为分别是 16 和 2？raft membership change 中 consensus joint 的本质原因是啥？mysql undo log 是否要做 redo？有些细节问题自己之前也没思考过，只能说自己对于整个系统与细节有大概的了解，但是针对某些本质的原因还是要继续思考</p><h1 id="学校"><a href="#学校" class="headerlink" title="学校"></a>学校</h1><p>这段时间只是把毕业设计开了个头，但是自己感觉自己之后这段时间的主要目标应该还是先把毕业设计狠狠的往后推进一下，不然自己一直感觉如鲠在喉没有办法专心学习和做其他的事情了，集中精力冲吧！</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>加油吧！人生是一段长跑，不要短时间用力过度也不要停下来，冲！</p>]]></content>
    
    
    <categories>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2022-03-新年的两周(1.31~2.13)</title>
    <link href="/2022/02/17/2022-03-%E6%9D%82/"/>
    <url>/2022/02/17/2022-03-%E6%9D%82/</url>
    
    <content type="html"><![CDATA[<p>2022 的第三个周报就 delay了…, 究其原因还是自己太懒了，梳理一下到今天为止的做的一些工作和学习，过年期间从初一开始就纯玩了好几天，导致 github 的小绿点都没了 TAT，还是想要小绿点，需要每天都坚持学习和努力工作！</p><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><p>翻看了一下这段时间的PR，主要还是参与 datavalues 和 functions 的重构中，这几个 PR 都是这段时间完成的PR：<br>重写几个函数：bin&#x2F;oct&#x2F;hex&#x2F;unhex&#x2F;repeat&#x2F;substring&#x2F;substr</p><ul><li><a href="https://github.com/datafuselabs/databend">datafuselabs&#x2F;databend </a><a href="https://github.com/datafuselabs/databend/pull/4088">string function: bin&#x2F;oct</a> </li><li><a href="https://github.com/datafuselabs/databend">datafuselabs&#x2F;databend </a><a href="https://github.com/datafuselabs/databend/pull/4095">StringFunction: hex&#x2F;unhex</a> </li><li><a href="https://github.com/datafuselabs/databend">datafuselabs&#x2F;databend </a><a href="https://github.com/datafuselabs/databend/pull/4102">String Func: repeat&#x2F;substring&#x2F;substr&#x2F;mid</a></li></ul><p>参考ck的if实现，重构了一下 if 的实现，效率能够比之前提高50%</p><ul><li><a href="https://github.com/datafuselabs/databend">datafuselabs&#x2F;databend </a><a href="https://github.com/datafuselabs/databend/pull/4160">refactor if to get better performence</a></li></ul><p>修复了一个小bug：</p><ul><li><a href="https://github.com/datafuselabs/databend">datafuselabs&#x2F;databend </a><a href="https://github.com/datafuselabs/databend/pull/4122">fix makefile: make run with build-release</a></li></ul><p>总的来说这段时间还是主要以参与重构为主，也对如何写出性能更好的代码有了一点点感觉了，这个需要形成自然的本能，才能在之后写代码的时候写出性能不错的代码</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><p>发现自己的数据结构和算法能力比较弱，也就是写题能力不是很好，需要加强！</p><p>从上次记录到今天，leetcode 新刷了 67 道题目，这样的效率还不错，但是对于想要努力提升达到的水平其实还有一定距离，需要继续加油！</p><p>需要开始复习计算机基础知识了，很多基础知识其实又开始有点忘记了，需要不断地巩固复习，在职业初期把基础打好！</p><h1 id="个人"><a href="#个人" class="headerlink" title="个人"></a>个人</h1><p>科三考过了！第二次考的压力比第一次大太多了，但是科四由于一些奇怪的问题竟然要拖到23号才能考，所以到学校的时间又被拖延了不少，所以24号才能回学校去</p><h1 id="next"><a href="#next" class="headerlink" title="next"></a>next</h1><p>下个周期要开始更努力的学习和工作了！</p>]]></content>
    
    
    <categories>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2022-02 焦虑</title>
    <link href="/2022/01/30/2022-02-%E7%84%A6%E8%99%91%E7%9A%84%E4%B8%A4%E5%91%A8/"/>
    <url>/2022/01/30/2022-02-%E7%84%A6%E8%99%91%E7%9A%84%E4%B8%A4%E5%91%A8/</url>
    
    <content type="html"><![CDATA[<p>2022 的第二个周报，明年就过年了，主题是焦虑…</p><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><p>这周做的工作如下面几个 PR</p><ul><li><p><a href="https://github.com/datafuselabs/databend/pull/3912">impl <code>getData</code> trait for boolean&#x2F;string column &amp; improve column</a> 这个 closed 掉的 PR 是开始对重构 datavalues 的一些尝试工作，本来以后就给 bool 和 string 类型加上两个 trait 那么简单，结果后来因为代码结构，依赖关系，生命周期等一些问题发现这条路行不通了…正好 cxs 开源的 <a href="https://github.com/skyzh/type-exercise-in-rust">https://github.com/skyzh/type-exercise-in-rust</a> 写得确实很漂亮的代码把 type 解耦得非常好，然后 sundy 哥重改思路很快就写出来一版（太强了，啥时候我也能又快又好的写出正确、高效还解耦清晰的代码 😭）</p></li><li><p><a href="https://github.com/datafuselabs/databend/pull/3988">datavalues2: conditional function: if</a> 重写条件变量表达式 if <code>if(a, b, c)</code> ，确实新版的类型系统的加持下能够对 nullable 和 non-nullable 的 column 都能够写出比较漂亮的代码</p><ul><li><a href="https://github.com/datafuselabs/databend/pull/3992">use aggregate_type to merge type</a>   improve if</li></ul></li><li><p><a href="https://github.com/datafuselabs/databend/pull/3998">datavalues2: Logic function</a> 重写 sql logic function（and,or,xor,not），采用宏静态分发的代码实在写的很舒服</p></li></ul><p>其他：</p><ul><li><a href="https://github.com/datafuselabs/databend/pull/3965">impl Debug trait for Column</a> 给 column 加上 debug 信息</li><li><a href="https://github.com/datafuselabs/databend/pull/4008">fix if with null &amp; fix convert to nullable column bug</a></li></ul><p>这周工作上的内容还比较充实，但是感觉 cxs, sundy 哥太强了，还有很多需要学习的地方，设计出一个好的数据库中的内存数据表示方式确实是一个很难的事情，同时还要注意兼顾SIMD的向量化计算等优化。</p><p>加油！</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><ul><li><p>这周很焦虑，继续在坚持做算法题，感觉自己不能停止思考，算法题确实是一个活跃思维的很好的方式，虽然都是小点，希望能够继续坚持下去</p></li><li><p>但是自己并没有继续做6.s081，上周的 flag 倒了一个，TAT，下周要捡起来</p></li></ul><h1 id="个人"><a href="#个人" class="headerlink" title="个人"></a>个人</h1><p>科三挂了，得重新来过，很难受，因为又得耽误好几天时间</p><p>滑雪因为各种原因没去成…</p><p>都快23岁了啊…，得更努力了，要向大佬们看齐，一步一个脚印冲</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>焦虑，感觉自己想要学习的东西太多了…希望下周周期心态能够变得更好一点，fighting！</p>]]></content>
    
    
    <categories>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2022-01 双周报尝试&amp;回家</title>
    <link href="/2022/01/16/2022-01-%E5%8F%8C%E5%91%A8%E6%8A%A5%E5%B0%9D%E8%AF%95/"/>
    <url>/2022/01/16/2022-01-%E5%8F%8C%E5%91%A8%E6%8A%A5%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p>开始公开记录下自己的工作和学习过程，决定开始采用双周周报的形式。这个想法来自于团队里的一个大佬 <a href="https://xuanwo.io/">Xuanwo</a> 的想法，在看到他利用开源的方式来记录下自己工作学习历程之后，自己也决定学习模仿一下，代替在 Notion 里的月度记录，这样做有几个好处，</p><ol><li>双周的迭代能够更加紧凑，让自己知道短期内做了什么，对工作、学习计划能够有及时的调整</li><li>把自己的工作学习成果写出来能够监督自己，相当于立下一个 flag</li></ol><p>结合自己的情况，把自己的情况分为下面几个部分，先从几个月初定下的目标来看</p><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><p>看一下 GitHub，这个双周提了下面几个 PR</p><p>databend</p><ul><li>feature <a href="https://github.com/datafuselabs/databend/pull/3805">support <code>[NOT] IN (a, b, c, ...)</code> InList SQL</a> 给 databend 完成了一个 feature，这个是完成了一个对于 <code>[NOT] IN (...)</code> 语法的支持。<ul><li>写这个 PR 的时候发现 parser 部分已经改动太多，自己对这部分熟悉了一次，印象比较深的就是在 parser 解析生成 expression 的时候，winter 哥对于解决 stack overflow 的问题，由于 Rust 的 async 不能够递归，所以需要采用 visitor 模式，采用栈来模拟递归操作，解决 parser 在生成 expression 的时候 stackoverflow 的问题（但是依赖的 sql-parser 仍然有可能出现栈溢出的问题…）</li></ul></li><li><a href="https://github.com/datafuselabs/databend/pull/3761">Fuse metadata cache</a> 给 fuse 开始加上了 metadata 的缓存，也是缓存在内存中<ul><li>这个PR合进去之后不久就被 dantengsky 哥把 fuse 的 cache 层重构了，代码写的真的很漂亮，特别是把 cache 和 reader 的解耦，自己对于复杂结构的解耦能力需要学习努力，多写代码多思考才能学到</li></ul></li><li><a href="https://github.com/datafuselabs/databend/pull/3779">remove useless series_debug &amp; make mod series not pub</a>  删除了一些无用的文件以及一些简单的 improvement</li><li>开始参与到 datavalues 的重构，这部分是一个大活，设计好一个好的类型系统对于之后的各种函数的功能拓展是非常有利的，之前的设计确实在拓展性上很难搞</li></ul><p>openraft：</p><ul><li><a href="https://github.com/datafuselabs/openraft/pull/57">fix some guide doc typo</a> 草率地学习了下 openraft 的代码，修了几个 typo<ul><li>自己还没有参与过工业界级别的 raft 的算法的代码编写，之后希望能够有机会参与到合适的 feature 开发中去</li></ul></li></ul><p>总的来说，工作完成了一些，但是自己在 1.11 回家之后这几天就几乎很少能够专注的工作了，由于是刚刚回到家，所以一些事情比较多，想要专注的学习工作还是得在学校或者office找一个安静的环境专注的学习工作</p><h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><p>这周看了下上周的学习任务，基本上算是完成了，因为自己前一周比较充实，惭愧，这周因为刚回家基本没怎么学习</p><ul><li>完成了 6.s081 的前两个 lab，util  和 syscall 两个实验，并且写了实验报告 <a href="https://tanweime.com/2022/01/03/6-s081-lab-util-%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">6.s081-lab-util-实验报告</a> 和 <a href="https://tanweime.com/2022/01/09/6-s081-lab-syscall-%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">6.s081-lab-syscall-实验报告</a><ul><li>自己算是再次对 xv6 的代码再次开始熟悉了一下，希望再接再厉，把后面的 lab 都坚持做完，自己对于操作的系统的认识确实需要好好提升一下</li></ul></li><li>写了点算法题，新建了一个list，从 14 天前到现在写了 56 道算法题，一天平均下来 4 道，看起来还不错，每天都坚持了写了一点题目，希望能够继续坚持下来，还是写的高频题，虽然不少题目都很熟悉了，但是还是不能够马上写出来，需要继续努力坚持下来</li><li>自己写了点 tinykv 的内容，然后队友把我鸽了，哎，导致后面自己也没跟着做了，等有时间了自己再跟着做吧…</li></ul><h1 id="学校"><a href="#学校" class="headerlink" title="学校"></a>学校</h1><p>元旦的时候和实验室的小伙伴们一起出去团建了下，印象最深还是开了卡丁车，卡丁车是真滴好玩！下次还会再去！和大学同学见了见面，连续吃了几天的火锅涮肉 …</p><p>后面可能的开始抽时间弄毕设的东西了，毕竟都留到最后暑假的那几个月来做是不现实的，所以可能从二月开始就可以开始做计划了。</p><h1 id="个人"><a href="#个人" class="headerlink" title="个人"></a>个人</h1><p>今年回来的好早，1.11 号就回来了，和老师请了两次假才回来，毕竟一年没回家了，确实比较想家了。这次提前回家被证明是正确的，因为学校旁边在 1.15 号出了一个阳性，现在宿舍舍友在阳性之后两天内全部都回家了…</p><p>回家之后确实吃的非常好！比在学校吃的好多了 QAQ，到处去吃饭，每天都好多菜。</p><p>然后回家之后开始学车，学科目三，练了三天就报名了，感觉上路跑比科目二的倒车入库简单太多了，希望能一把过吧！</p><p>但是一个整体的感觉是回家之后自己比较难找到一个安静的环境专注的学习工作，但是还是要监督好自己</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>太菜了，自己和前辈们的差距还是很大，感觉自己在各个方面都需要努力学习工作和总结，需要多思考，多想问题。</p><p>写出这样一个总结让自己的思路很快就清晰了起来，感觉很不错！希望能够坚持下来！需要定下下个 iteration 的计划了，明天起来再定，对了，下个 iteration 希望吧 blog 的评论功能给加上（虽然没啥人看）…，看 GitHub issue 作为评论的那个就很不错！</p>]]></content>
    
    
    <categories>
      
      <category>周报</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2021总结</title>
    <link href="/2021/12/29/2021%E6%80%BB%E7%BB%93/"/>
    <url>/2021/12/29/2021%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>转眼间又到了年底，总感觉在成年之后每年都过得很快。这是写总结的第二年，我一直认为总结复盘是一种战略，能够在比较长的时间内看清楚自己做的好的地方和不好的地方，同时也能看到自己一点一滴的进步，能够看到自己完成了哪些目标而又错过了哪些事情，从而能够更好的指导自己未来努力的方向、修正自己的缺点。</p><p>先来回忆回忆我这一年都干了啥，就当是记流水账了。</p><h1 id="流水账"><a href="#流水账" class="headerlink" title="流水账"></a>流水账</h1><p>年初的时候还是研一，还在准备期末考试，考完结束回家；二月份在家做了点题，三月的时候做了下 15-445，然后开始刷了点题开始找实习，找实习的过程也比较幸运比较顺利，面过的几个公司都拿到offer了；但是当时找的时候也不太确定自己之后到底对哪个方面感兴趣，但是总觉得自己要做一点有技术含量的东西，刚好有两个岗位都是分布式存储，不过一个是分布式块存储，一个是分布式kv，最终在请教了磊神的建议后去了字节基础架构做 kv 的组。这段准备实习面试的过程中对基础知识又复习了相当于一轮吧，在后面的实习里也越来越认识到基础知识牢固的重要性。</p><p>在字节大概呆了五个多月快小半年了吧，现在想起来觉得这个实习的选择是一个比较正确的决定，对于我一个没有实习&#x2F;工作经验的人来说，在字节基础架构的实习是一个很好的经历，先说说好的地方吧。首先是因为是大厂，所以能够熟悉大厂的工作流程，大概了解了在大厂的一个比较正规的工作&#x2F;开发流程应该是怎么样的，让我明白了多人协作开发的正规流程（之前自己写代码几乎都没有 review，做代码改动的时候也从来不会写设计文档，自己的工程水平需要提升的点太多了），在大厂的另外一个好处是让我知道了基础架构组在公司内的处于什么样的位置，也就是了解到基础架构和业务之间的沟通协作是怎么样的；其次是组内的情况，组里是做分布式 kv 的，这是一个比较成熟的领域，但是我们小组人不多，有几个大佬都非常友好，让我了解到工业界里分布式 kv 是什么样的；另外，更重要的是认识到了一些大佬，从大佬身上学习到应该如何去学习去工作，应该多读论文，多写代码，多做总结；最后是关于实际的工程能力，写分布式系统里的 C++ 代码确实是一个比较难搞的事情，因为测试不好做，每次写完因为分布式的功能不好写单测，所以总是需要跑起来一个实际的系统然后通过日志和一些metrics来观测，总觉得不太靠谱，在这个过程中也发现自己的工程能力需要努力提升，实际去改系统内的代码的时候需要先考虑清楚，设计清楚再写代码。但是在字节也有一些不好的地方，那就是这里确实会比较累一点，每天的工作时间太长了，下班都在九点之后，在这段时间里，我周内的时间几乎是没有自己的空闲时间留出来学习的，只有晚上九点多之后有时候回到实验室再继续看看书啥的，但是整体来说自己的时间变少太多了。</p><p>九月底从字节离职了，之后九十月份的时候接触到 <code>Rust</code>，开始写这门语言的时候就觉得特别吊，没有GC，不用手动管理内存而且多线程安全…跑题了，不是说吹<code>Rust</code>，而是接触到  <code>Rust</code> 的我开始对这门语言产生了浓烈了的兴趣。然后在 <code>Rust</code>社区发现一个很酷的项目<code>databend</code>，这也是我最早参与的开源项目，正好有一个比较有意思的 <code>issue</code>，要做一个内存监控模块，这对刚接触 <code>Rust</code> 的我来说还是有点难度，花了几天时间做好成功合进去了。这个时候虎哥看到这个PR之后看到我是学生，就问我有没有实习的想法，在这实习可以远程工作，就算在学校能干，而且大佬非常多，这种好机会送到我眼前，我抓住机会就直接来 <code>datafuselabs</code> 实习了。截止到今天我刚好在在项目里写了 1w 行 <code>Rust</code> 了，在这里实习的另外一个非常好的感受就是同事们前辈们都特别友好，对我这样的新手也非常有耐心。</p><p>在十一月十二月的时候，学校要准备毕业论文开题了，这段时间其实自己是比较难受的，因为自己的毕业设计是和老师的一个项目相关，但是项目本身却因为各种原因要推迟了，所以这段时间也让我比较疲惫，但是最后开题的时候还好，没有被老师们怼得很惨，老师们都还是比较好，毕设的事情等到明年具体做的时候再确定怎么弄吧。</p><p>今年的整体时间感觉就是在实习中度过的，两个实习之前间隔了大概一个月，休息的这一个月里，总算是有了自己的时间来给自己继续看书，看 paper，现在更羡慕那种完全自由的学习时间了。</p><p>妹子今年出国了，妹子在的时间把2021分成了两半，前半年妹子一直在北京，一起度过一段很棒的时间，当时也是一起在实习，然后一起做饭一起生活；之后七月之后妹子就去了美帝，我们成了异国，这一天其实早就想到了，不过是终于到来了，不过这也更好，在二十多岁的年纪各自先好好为以后奋斗奋斗。</p><h1 id="Good-stuff-and-bad-stuff"><a href="#Good-stuff-and-bad-stuff" class="headerlink" title="Good stuff and bad stuff"></a>Good stuff and bad stuff</h1><p>找找自己身上做的好的地方和不好的地方吧，做的好的地方：</p><ul><li>比较积极主动，能够主动发起做一些事情</li><li>专注于做一件事情的效率比较高</li><li>短时间内做事情的时候比较有条理，能够把任务拆分并且一个个小任务完成</li></ul><p>做的不好的地方：</p><ul><li><p>忽略掉了日常的学习</p></li><li><p>一个时间只能专注于一个事情，做一个事情的时候就容易忽略掉其他事情</p></li><li><p>很容易定下不能完成的计划，然后有一些挫败感</p></li><li><p>日渐长胖，太久没运动，这样不好</p></li></ul><h1 id="future"><a href="#future" class="headerlink" title="future"></a>future</h1><p>在博客里不适合放太具体的计划，这里就定下2022 年里需要做的一些大的方向吧，之后再自己的计划本里再具体下自己的具体执行计划吧！</p><p>学校：</p><ul><li>毕设好好做</li></ul><p>学习&#x2F;工作：</p><ul><li>看经典 DB 和 system 论文，整理记录到 GitHub 仓库去</li><li>认真学习两个课程 OS + DB（6.s081 + 6.830）</li><li>好好写算法题</li></ul><p>其他：</p><ul><li>坚持运动打卡，争取回到之前的状态</li><li>每双周写总结</li></ul>]]></content>
    
    
    <categories>
      
      <category>年度总结</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[论文笔记]Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores2</title>
    <link href="/2021/11/17/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-Delta-Lake-High-Performance-ACID-Table-Storage-over-Cloud-Object-Stores2/"/>
    <url>/2021/11/17/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-Delta-Lake-High-Performance-ACID-Table-Storage-over-Cloud-Object-Stores2/</url>
    
    <content type="html"><![CDATA[<h1 id="Delta-lake"><a href="#Delta-lake" class="headerlink" title="Delta lake"></a>Delta lake</h1><blockquote><p>博客总算开张了，感觉每天其实花不了多少时间就能整理一下，以后也可以把之前一些Notion里面的笔记整理出来发成博客，希望能坚持下去</p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Amazon S3 等云对象存储是地球上最大、最具成本效益的存储系统之一，使其成为存储大型数据仓库和数据湖的有吸引力的目标。不幸的是，它们作为键值存储的实现使得难以实现 ACID 事务和高性能：metadata 操作（例如 <code>LIST</code> ）成本高昂，并且一致性保证有限。在本文中，我们介绍了 Delta Lake，它是最初在 Databricks 开发的云对象存储之上的开源 ACID 表存储层。 Delta Lake 使用压缩为 Apache Parquet 格式的事务日志来为大型表格数据集提供 ACID 属性、time travel 和显着更快的 metadata 操作（例如，能够快速搜索数十亿个表分区以查找与查询相关的分区）。它还利用这种设计来提供高级功能，例如自动数据布局优化、更新插入、缓存和审计日志。 Delta Lake 表可以从 Apache Spark、Hive、Presto、Redshift 和其他系统访问。 Delta Lake 部署在数千个每天处理 EB 级数据的 Databricks 客户中，其中最大的实例管理 EB 级数据集和数十亿个对象。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Amazon S3 [4] 和 Azure Blob Storage [17] 等云对象存储已成为地球上规模最大、使用最广泛的存储系统之一，为数百万客户保存 EB 级数据 [46]。除了云服务的传统优势，如即用即付计费、规模经济和专家管理[15]，云对象存储特别有吸引力，因为它们允许用户分别扩展计算和存储资源：对于例如，用户可以存储 PB 的数据，但只能运行一个集群来对其执行几个小时的查询。因此，许多组织现在使用云对象存储来管理数据仓库和数据湖中的大型结构化数据集。</p><p>主要的开源“大数据”系统，包括 Apache Spark、Hive 和 Presto [45, 52, 42]，支持使用 Apache Parquet 和 ORC [13, 12] 等文件格式读取和写入云对象存储。包括 AWS Athena、Google BigQuery 和 Redshift Spectrum [1, 29, 39] 在内的商业服务也可以直接查询这些系统和这些开放文件格式。</p><p>不幸的是，尽管许多系统支持对云对象存储的读写，但在这些系统上实现高性能和可变的表存储具有挑战性，因此很难在它们上实现数据仓库功能。与 HDFS [5] 等分布式文件系统或 DBMS 中的自定义存储引擎不同，大多数云对象存储只是键值存储，没有跨键一致性保证。它们的性能特征也与分布式文件系统有很大不同，需要特别注意。</p><p>在云对象存储中存储关系数据集的最常见方法是使用列式文件格式，例如 Parquet 和 ORC，其中每个表都存储为一组对象（Parquet 或 ORC “文件”），可能按某些字段（例如，每个日期的一组单独对象）聚集成“分区”[45]。只要目标文件适中，这种方法就可以为扫描工作负载提供可接受的性能。但是，它为更复杂的工作负载带来了正确性和性能方面的挑战。首先，因为多对象更新不是原子的，查询之间没有隔离：例如，如果一个查询需要更新表中的多个对象（例如，删除所有表的 Parquet 文件中关于一个用户的记录），读者当查询单独更新每个对象时，将看到部分更新。回滚写入也很困难：如果更新查询崩溃，则表处于损坏状态。其次，对于拥有数百万个对象的大表， metadata 操作成本很高。例如，Parquet 文件包含具有最小&#x2F;最大统计信息的 footer ，可用于在选择性查询中跳过读取它们。在 HDFS 上读取这样的 footer 可能需要几毫秒，但云对象存储的延迟要高得多，以至于这些数据跳过检查可能比实际查询需要更长的时间。</p><p>根据我们与云客户合作的经验，这些一致性和性能问题给企业数据团队带来了重大挑战。大多数企业数据集都在不断更新，因此它们需要原子写入的解决方案；大多数关于用户的数据集需要全表更新以实施隐私政策，例如 GDPR 合规性 [27]；甚至纯粹的内部数据集也可能需要更新以修复不正确的数据、合并迟到的记录等。有趣的是，在 Databricks 云服务的最初几年（2014-2016），我们收到的支持升级中约有一半是由于数据由于云存储策略而导致的损坏、一致性或性能问题（例如，撤销崩溃的更新作业的影响，或提高读取数万个对象的查询的性能）。</p><p>为了应对这些挑战，我们设计了 Delta Lake，这是一个基于云对象存储的 ACID 表存储层，我们于 2017 年开始向客户提供并于 2019 年开源 [26]。 Delta Lake 的核心思想很简单：我们以 ACID 方式维护有关哪些对象属于 Delta 表一部分的信息，使用本身存储在云对象存储中的预写日志。对象本身是用 Parquet 编码的，这使得从已经可以处理 Parquet 的引擎编写连接器变得容易。这种设计允许客户端以可序列化的方式一次更新多个对象，用另一个对象替换对象的一个子集，等等，同时仍然从对象本身实现高并行读写性能（类似于原始 Parquet）。日志还包含 metadata ，例如每个数据文件的最小&#x2F;最大统计信息，与“对象存储中的文件”方法相比， metadata 搜索速度快了一个数量级。至关重要的是，我们设计了 Delta Lake，以便所有 metadata 都在底层对象存储中，并且使用针对对象存储的乐观并发 protocol 来实现事务（一些细节因云提供商而异）。这意味着不需要运行服务器来维护 Delta 表的状态；用户只需在运行查询时启动服务器，即可享受计算和存储分开扩展的好处。</p><p>基于这种事务性设计，我们还能够在 Delta Lake 中添加传统云数据湖中不具备的多项其他功能，以解决常见的客户痛点，包括：</p><ul><li>time travel 让用户可以查询时间点快照或回滚对其数据的错误更新。</li><li>UPSERT、DELETE 和 MERGE 操作，它们有效地重写相关对象以实现对存档数据和合规性工作流的更新（例如，对于 GDPR [27]）。</li><li>高效的流I&#x2F;O，通过让流式作业以低延迟将小对象写入表中，然后稍后将它们以事务方式合并为更大的对象以提高性能。 还支持对添加到表中的新数据进行快速“拖尾”读取，因此作业可以将 Delta 表视为消息总线。</li><li>缓存：因为Delta 表中的对象及其日志是不可变的，集群节点可以安全地将它们缓存在本地存储上。我们在 Databricks 云服务中利用它来为 Delta 表实现透明的 SSD 缓存。</li><li>数据布局优化：我们的云服务包括一项功能，可自动优化表中对象的大小和数据记录的聚类（例如，以 Z-order 存储记录以实现多个维度的局部性），而不会影响正在运行的查询。</li><li>架构演变，允许 Delta 继续读取旧的 Parquet 文件，而无需在表的架构更改时重写它们。</li><li>基于事务日志的审计日志。</li></ul><p>这些功能共同提高了在云对象存储中处理数据的可管理性和性能，并启用了一个“lakehouse”范式，该范式结合了数据仓库和数据湖的关键功能：标准 DBMS 管理功能可直接用于低成本对象存储。事实上，我们发现许多 Databricks 客户可以使用 Delta Lake 简化他们的整体数据架构，通过用 Delta 表替换以前单独的数据湖、数据仓库和流存储系统，为所有这些用例提供适当的功能。图 1 显示了一个极端示例，其中包含对象存储、消息队列和用于不同商业智能团队（每个团队运行自己的计算资源）的两个数据仓库的 data pipeline 被替换为对象存储上的 Delta 表，使用 Delta 的流 I&#x2F;O 和性能特性来运行 ETL 和 BI。新管道仅使用低成本的对象存储并创建更少的数据副本，从而降低了存储成本和维护开销。</p><p>Delta Lake 现在被 Databricks 的大多数大客户使用，每天处理 EB 的数据（大约是我们总工作量的一半）。它还得到了谷歌云、阿里巴巴、腾讯、Fivetran、Informatica、Qlik、Talend 等产品的支持 [50, 26, 33]。在 Databricks 客户中，Delta Lake 的 user case 非常多样化，从传统的 ETL 和数据仓库工作负载到生物信息学、实时网络安全分析（每天数百 TB 的流事件数据）、GDPR 合规性和机器学习的数据管理（管理数百万张图像作为 Delta 表中的记录而不是 S3 对象以获得 ACID 和改进的性能）。我们在第 5 节中详细介绍了这些用例。</p><p><img src="/lakehouse1.png" alt="lakehouse1"></p><p>有趣的是，Delta Lake 将 Databricks 的云存储支持问题的比例从一半减少到几乎没有。 它还提高了大多数客户的工作负载性能，在使用其数据布局优化和快速访问统计数据来查询非常高维数据集（例如，网络安全和生物信息学用例）的极端情况下，加速高达 100 倍。 开源 Delta Lake 项目 [26] 包括到 Apache Spark（批处理或流媒体）、Hive、Presto、AWS Athena、Redshift 和 Snowflake 的连接器，并且可以在多个云对象存储或 HDFS 上运行。 在本文的其余部分，我们将介绍 Delta Lake 的动机和设计，以及激发我们设计动机的客户用例和性能实验。</p><h1 id="2-Motivation-Characteristics-and-challenges-of-object-stores"><a href="#2-Motivation-Characteristics-and-challenges-of-object-stores" class="headerlink" title="2. Motivation: Characteristics and challenges of object stores"></a>2. Motivation: Characteristics and challenges of object stores</h1><p>在本节中，我们将描述云对象存储的 API 和性能特征，以解释为什么这些系统上的高效表存储具有挑战性，并概述管理这些系统上的表格数据集的现有方法。</p><h2 id="2-1-Object-Store-APIs"><a href="#2-1-Object-Store-APIs" class="headerlink" title="2.1 Object Store APIs"></a>2.1 Object Store APIs</h2><p>云对象存储，例如 Amazon S3 [4] 和 Azure Blob Storage [17]、Google Cloud Storage [30] 和 OpenStack Swift [38]，提供了一个简单但易于扩展的键值存储接口。这些系统允许用户创建每个存储多个对象的存储桶，每个存储桶都是大小不超过几 TB 的二进制 blob（例如，在 S3 上，对象大小的限制为 5 TB [4]）。每个对象都由一个字符串键标识。在文件系统路径（例如，warehouse&#x2F;table1&#x2F;part1.parquet）之后对键进行建模是很常见的，但与文件系统不同，云对象存储不提供对象或“目录”的廉价重命名。云对象存储还提供 metadata  API，例如 S3 的 LIST 操作 [41]，它通常可以在给定开始键的情况下，按照键的字典顺序列出存储桶中的可用对象。如果使用文件系统样式的路径，通过在代表该目录前缀的键（例如，warehouse&#x2F;table1&#x2F;）处启动 LIST 请求，这使得有效地列出“目录”中的对象成为可能。不幸的是，这些 metadata  API 通常很昂贵：例如，S3 的 LIST 每次调用最多只能返回 1000 个键，并且每次调用需要几十到几百毫秒，因此使用顺序执行。</p><p>读取对象时，云对象存储通常支持字节范围请求，因此仅读取大对象内的范围（例如，字节 10,000 到 20,000）是有效的。这使得利用将经常访问的值聚集在一起的存储格式成为可能。</p><p>更新对象通常需要一次重写整个对象。这些更新可以是原子的，这样读者要么看到新的对象版本，要么看到旧的。一些系统还支持附加到对象 [48]。</p><p>一些云供应商还在 blob 存储上实现了分布式文件系统接口，例如 Azure 的 ADLS Gen2 [18]，它基于与 Hadoop 的 HDFS 相似的语义（例如，目录和原子重命名）。尽管如此，Delta Lake 解决的许多问题，例如小文件 [36] 和跨多个目录的原子更新，即使在使用分布式文件系统时仍然存在——事实上，多个用户通过 HDFS 运行 Delta Lake。</p><h2 id="2-2-Consistency-Properties"><a href="#2-2-Consistency-Properties" class="headerlink" title="2.2 Consistency Properties"></a>2.2 Consistency Properties</h2><p>最流行的云对象存储为每个键提供最终一致性，但没有跨键的一致性保证，这在管理由多个对象组成的数据集时带来了挑战，如简介中所述。特别是，当一个客户端上传一个新对象后，其他客户端不一定能立即看到 LIST 中的对象或进行读取操作。同样，其他客户端可能不会立即看到对现有对象的更新。此外，根据对象存储，即使客户端进行写入也可能不会立即看到新对象。</p><p>确切的一致性模型因云提供商而异，并且可能相当复杂。作为一个具体的例子，Amazon S3 为写入新对象的客户端提供了写后读一致性，这意味着 S3 的 GET 等读取操作将在 PUT 之后返回对象内容。但是，有一个例外：如果写入对象的客户端在其 PUT 之前向（不存在的）键发出 GET，则后续 GET 可能不会在一段时间内读取该对象，这很可能是因为 S3 使用了negative caching。此外，S3 的 LIST 操作始终是最终一致的，这意味着 PUT 之后的 LIST 可能不会返回新对象 [40]。其他云对象存储提供更强的保证 [31]，但仍然缺乏跨多个键的原子操作。</p><blockquote><p>注意到，现在 S3 已经提供了更强的一致性，见于 <a href="https://aws.amazon.com/cn/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/">https://aws.amazon.com/cn/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/</a></p></blockquote><h2 id="2-3-Performance-Characteristic"><a href="#2-3-Performance-Characteristic" class="headerlink" title="2.3 Performance Characteristic"></a>2.3 Performance Characteristic</h2><p>根据我们的经验，使用对象存储实现高吞吐量需要在<strong>大型顺序 I&#x2F;O 和并行性之间</strong>进行仔细平衡。对于读取，可用的最细粒度的操作是读取顺序字节范围，如前所述。每次读取操作通常会产生至少 5-10 ms 的基本延迟，然后可以大约 50-100 MB&#x2F;s 的速度读取数据，因此一次操作至少需要读取数百 KB 才能实现至少一半的顺序峰值吞吐量。读取和数兆字节以接近峰值吞吐量。此外，在典型的 VM 配置中，应用程序需要并行运行多个读取以最大化吞吐量。例如，AWS 上最常用于分析的 VM 类型至少具有 10 Gbps 的网络带宽，因此它们需要运行 8-10 个并行读取以充分利用此带宽。</p><p>LIST 操作也需要显着的并行性来快速列出大型对象集。例如，S3 的 LIST 操作每个请求最多只能返回 1000 个对象，并且需要几十到几百毫秒，因此客户端需要并行发出数百个 LIST 来列出大桶或“目录”。在我们针对云中 Apache Spark 的优化运行时中，我们有时会在 Spark 集群中的工作节点上并行化 LIST 操作以及驱动程序节点中的线程，以使它们运行得更快。在 Delta Lake 中，有关可用对象的 metadata （包括它们的名称和数据统计信息）存储在 Delta 日志中，但我们也在集群上并行化从该日志中读取。</p><p>写操作通常必须替换整个对象（或附加到它），如第 2.1 节所述。这意味着如果一个表需要接收点更新，那么其中的对象应该保持小，这与支持大读取不一致。或者，可以使用日志结构的存储格式。</p><p>对表存储的影响。对象存储的性能特征导致分析工作负载的三个注意事项：</p><ol><li>将频繁访问的数据按顺序排列，这通常会导致选择列格式。</li><li>使object变大，但不要太大。大对象会增加更新数据的成本（例如，删除有关一个用户的所有数据），因为它们必须完全重写。</li><li>避免 LIST 操作，并尽可能让这些操作请求字典键范围。</li></ol><h2 id="2-4-Existing-Approaches-for-Table-Storage"><a href="#2-4-Existing-Approaches-for-Table-Storage" class="headerlink" title="2.4 Existing Approaches for Table Storage"></a>2.4 Existing Approaches for Table Storage</h2><p>基于对象存储的特性，目前使用三种主要的方法来管理它们上的表格数据集。我们简要概述了这些方法及其挑战。</p><ol><li>文件目录。开源big data stack 以及许多云服务支持的最常见方法是<strong>将表存储为对象的集合，通常采用列式格式，例如 Parquet</strong>。作为一种改进，<strong>可以根据一个或多个属性将记录“划分”到目录中</strong>。例如，对于带有日期字段的表，我们可能会为每个日期创建一个单独的对象目录，例如 mytable&#x2F;date&#x3D;2020-01-01&#x2F;obj1 和 mytable&#x2F;date&#x3D;2020-01-01&#x2F;obj2 用于数据从 1 月 1 日开始，然后是 1 月 2 日的 mytable&#x2F;date&#x3D;2020-01-02&#x2F;obj1，依此类推，并根据此字段将传入数据拆分为多个对象。这样的分区降低了 LIST 操作和只访问少数分区的查询的读取成本。</li></ol><p>  这种方法很有吸引力，因为表“只是一堆对象”，可以从许多工具访问，而无需运行任何额外的数据存储或系统。它起源于 HDFS 上的 Apache Hive [45]，与 Parquet、Hive 和其他文件系统上的大数据软件相匹配。<br>  这种方法的挑战。如简介中所述，“仅一堆文件”方法在云对象存储上存在<strong>性能和一致性问题</strong>。客户遇到的最常见挑战是：</p><ol><li><p>没有跨多个对象的原子性：任何需要写入或更新多个对象的事务都有部分写入对其他客户端可见的风险。此外，如果此类事务失败，数据将处于损坏状态。</p></li><li><p>最终一致性：即使 transaction 成功，客户端也可能会看到一些更新的对象，但看不到其他对象。</p></li><li><p>性能差： <code>LIST</code> 以查找与查询相关的对象的开销很大，即使它们通过一个键分区到目录中。此外，访问存储在 Parquet 或 ORC 文件中的每个对象的统计数据非常昂贵，因为它需要对每个功能进行额外的高延迟读取。</p></li><li><p>无管理功能：对象存储不实施标准实用程序，例如数据仓库中熟悉的表版本控制或审计日志。  </p></li><li><p>自定义存储引擎。为云构建的“封闭世界”存储引擎，例如 Snowflake 数据仓库 [23]，可以通过在<strong>单独的、强一致的服务</strong>中管理 metadata 本身来绕过云对象存储的许多一致性挑战，该服务拥有“source of truth”关于哪些 objects 构成了一张 table。在这些引擎中，云对象存储可以被视为dumb block device，并且可以使用标准技术在云对象上实现高效的 metadata 存储、搜索、更新等。然而，这种方法需要运行高度可用的服务来管理 metadata ，这可能很昂贵，在使用外部计算引擎查询数据时会增加开销，并且会将用户锁定到一个提供者。</p></li></ol><p>  这种方法的挑战。尽管全新的“封闭世界”设计有很多好处，但我们在使用这种方法时遇到的一些具体挑战是：</p><ul><li>所有I&#x2F;O 操作都需要联系 metadata 服务，这会增加其资源成本并降低性能和可用性。例如，在 Spark 中访问 Snowflake 数据集时，从 Snowflake 的 Spark 连接器读取的数据通过 Snowflake 的服务流式传输，与直接从云对象存储读取相比，性能降低。</li><li>与重用现有开放格式（如 Parquet）的方法相比，连接到现有计算引擎需要更多的工程工作来实现。根据我们的经验，数据团队希望对他们的数据使用广泛的计算引擎（例如 Spark、TensorFlow、PyTorch 等），因此使连接器易于实现非常重要。</li><li>专有 metadata 服务将用户与特定的服务提供商联系起来，而基于直接访问云存储中的对象的方法使用户能够始终使用不同的技术访问他们的数据。<br>Apache Hive ACID [32] 通过使用 Hive Metastore（一种事务性 RDBMS，如 MySQL）在 HDFS 或对象存储上实现了类似的方法，以跟踪保存以 ORC 格式存储的表的更新的多个文件。但是，这种方法受到 Metastore 性能的限制，根据我们的经验，这可能成为具有数百万个对象的表的瓶颈。</li></ul><ol start="3"><li>Object Stores 中的 metadata 。 DeltaLake 的方法是直接在云对象存储中存储事务日志和 metadata ，并在对象存储操作上使用一组 protocol 来实现可序列化。 然后，表中的数据以 Parquet 格式存储，只要有最小的连接器可用于发现要读取的对象集，就可以轻松地从任何已经支持 Parquet 的软件访问。 尽管我们认为 Delta Lake 是第一个使用这种设计的系统（从 2016 年开始），但现在还有另外两个软件包也支持它——Apache Hudi [8] 和 Apache Iceberg [10]。 Delta Lake 提供了许多这些系统不支持的独特功能，例如 Z-order 集群、缓存和后台优化。 我们在第 8 节中更详细地讨论了这些系统之间的异同。</li></ol><h1 id="3-Delta-Lack-Storage-format-and-access-protocols"><a href="#3-Delta-Lack-Storage-format-and-access-protocols" class="headerlink" title="3. Delta Lack Storage format and access protocols"></a>3. Delta Lack Storage format and access protocols</h1><p>Delta Lake 表是云对象存储或文件系统上的一个目录，其中包含包含表内容和事务操作日志（偶尔有 checkpoint ）的数据对象。 客户端使用我们针对云对象存储特性量身定制的乐观并发控制 protocol 来更新这些数据结构。 在本节中，我们将描述 Delta Lake 的存储格式和这些访问 protocol 。 我们还描述了 Delta Lake 的事务隔离级别，包括表内的可序列化和快照隔离。</p><h2 id="3-1-Storage-Format"><a href="#3-1-Storage-Format" class="headerlink" title="3.1 Storage Format"></a>3.1 Storage Format</h2><p>图 2 显示了 Delta 表的存储格式。 每个表都存储在文件系统目录（此处为 mytable）中，或作为对象存储中以相同“目录”键前缀开头的对象。</p><img src="lakehouse2.png" alt="image-20211117231929209" style="zoom:67%;" /><h3 id="3-1-1-Data-Objects"><a href="#3-1-1-Data-Objects" class="headerlink" title="3.1.1 Data Objects"></a>3.1.1 Data Objects</h3><p>表内容存储在 Apache Parquet 对象中，可能使用 Hive 的分区命名约定组织到目录中。<br>例如，在图 2 中，表按日期列分区，因此每个日期的数据对象位于不同的目录中。我们选择 Parquet 作为我们的底层数据格式，因为它面向列，提供多样化的压缩更新，支持半结构化数据的嵌套数据类型，并且已经在许多引擎中实现了高性能。建立在现有的开放文件格式上还确保 Delta Lake 可以继续利用新发布的 Parquet 库更新，并简化与其他引擎的连接器的开发（第 4.8 节）。其他开源格式，如 ORC [12]，可能也有类似的工作方式，但 Parquet 在 Spark 中拥有最成熟的支持。<br>Delta 中的每个数据对象都有一个唯一的名称，通常由作者通过生成 GUID 来选择。但是，哪些对象属于表的每个版本是由事务日志决定的。</p><h3 id="3-1-2-Log"><a href="#3-1-2-Log" class="headerlink" title="3.1.2 Log"></a>3.1.2 Log</h3><p>日志存储在表内的 _delta_log 子目录中。 它包含一系列 JSON 对象，这些对象具有递增的、用零填充的数字 ID 来存储日志记录，以及特定日志对象的临时 checkpoint ，这些对象以 Parquet 格式汇总到该点的日志。 正如我们在第 3.2 节中讨论的，一些简单的访问 protocol （取决于每个对象存储中可用的原子操作）用于创建新的日志条目或 checkpoint ，并让客户端就事务的顺序达成一致。</p><p>每个日志记录对象（例如 000003.json）都包含一组操作，这些操作应用于表的先前版本以生成下一个版本。 可用的操作是：</p><ul><li><p>Change Metadata ,metaData 操作更改表的当前 metadata 。表的第一个版本必须包含 metadata 操作。后续的 metadata 操作会完全覆盖表的当前 metadata 。 metadata 是一个数据结构，包含模式、分区列名称（即我们示例中的日期）、数据文件的存储格式（通常是 Parquet，但这提供了可扩展性）和其他配置选项，例如将表标记为仅附加。</p></li><li><p>Add or Remove Files。添加和删除操作分别用于通过添加或删除单个数据对象来修改表中的数据。因此，客户端可以搜索日志以查找所有尚未删除的添加对象，以确定构成该表的对象集。</p><ul><li>数据对象的 add record 还可以包括数据统计信息，例如总记录数和每列的最小值&#x2F;最大值和空值。当表中已存在的路径遇到添加操作时，最新版本的统计信息将替换任何先前版本的统计信息。这可用于在新版本的 Delta Lake 中“升级”具有更多类型统计信息的旧表。</li><li>Delete record 包括一个时间戳，指示删除发生的时间。在用户指定的保留时间阈值之后，数据对象的物理删除可能会延迟发生。此延迟允许并发读取器继续针对过时的数据快照执行。删除操作应该保留在日志和任何日志 checkpoint 中作为 tombstone ，直到底层数据对象被删除。</li><li>添加或删除操作上的 dataChange 标志可以设置为 false 以指示此操作与同一日志记录对象中的其他操作组合时，仅重新排列现有数据或添加统计信息。例如，跟踪事务日志的流式查询可以使用此标志跳过不会影响其结果的操作，例如更改早期数据文件中的排序顺序。</li></ul></li><li><p>Protocol Evolution,  protocol 操作用于增加读取或写入给定表所需的 Delta  protocol 版本。 我们使用此操作向格式添加新功能，同时指示哪些客户端仍然兼容。</p></li><li><p>Add Provenance Information. 每个日志记录对象还可以在 commitInfo 操作中包含出处信息，例如，记录哪个用户执行了操作。</p></li><li><p>Update Application Transaction IDs. DeltaLake 还为应用程序提供了一种将它们自己的数据包含在日志记录中的方法，这对于实现端到端的事务应用程序非常有用。例如，写入 Delta 表的流处理系统需要知道哪些写入之前已经提交，以实现“ exactly-once ”语义：如果流式作业崩溃，它需要知道哪些写入之前已提交将其放入表中，以便它可以 replay 从其输入流中正确偏移量开始的后续写入。为了支持这个用例，Delta Lake 允许应用程序在其日志记录对象中使用 appId 和 version 字段编写自定义 txn 操作，这些字段可以跟踪特定于应用程序的信息，例如我们示例中输入流中的相应偏移量。通过将此信息与相应的 Delta 添加和删除操作放在同一日志记录中，并以原子方式插入日志，应用程序可以确保 Delta Lake 以原子方式添加新数据并存储其版本字段。每个应用程序可以简单地随机生成其 appId 以接收唯一 ID。我们在 Delta Lake 连接器中使用此工具进行 Spark 结构化流 [14]。</p></li></ul><h3 id="3-1-3-Log-CheckPoints"><a href="#3-1-3-Log-CheckPoints" class="headerlink" title="3.1.3 Log CheckPoints"></a>3.1.3 Log CheckPoints</h3><p>为了性能，需要定期将日志压缩成 checkpoint 。 checkpoint 以 Parquet 格式将所有非冗余操作存储在表的日志中，直到某个日志记录 ID。某些操作集是多余的，可以删除。这些包括：</p><ul><li>为同一数据对象添加操作，然后是删除操作。由于数据对象不再是表的一部分，因此可以删除添加。根据表的数据保留配置，删除操作应保留为 tombstone 。具体来说，客户端在删除操作中使用时间戳来决定何时从存储中删除对象。</li><li>同一对象的多次添加可以被最后一个替换，因为新添加的只能添加统计信息。</li><li>来自同一个appId 的多个 txn 操作可以替换为最新的操作，其中包含其最新版本字段。</li><li>changeMetadata 和 protocol 操作也可以合并以仅保留最新的 metadata 。</li></ul><p>因此， checkpoint 过程的最终结果是一个 Parquet 文件，其中包含为仍在表中的每个对象添加记录，删除已删除但需要保留到保留期到期的对象的记录，以及一个少数的其他记录，例如 txn、 protocol 和 changeMetadata。这个面向列的文件是查询有关表的 metadata 以及根据数据统计查找哪些对象可能包含与选择性查询相关的数据的理想格式。根据我们的经验，使用 Delta Lake  checkpoint 查找要为查询读取的对象集几乎总是比使用 LIST 操作和读取对象存储上的 Parquet 文件 footer 更快。</p><p>任何客户端都可以尝试创建一个 checkpoint ，直到给定的日志记录 ID，如果成功，应该将它写为相应 ID 的 .parquet 文件。例如，000003.parquet 将代表直到并包括 000003.json 的记录的 checkpoint 。默认情况下，我们的客户端每 10 个事务写入一次 checkpoint 。</p><p>最后，访问 Delta Lake 表的客户端需要高效地找到最后一个 checkpoint （以及日志的尾部），而无需列出 _delta_log 目录中的所有对象。 如果 _delta_log&#x2F;_last_checkpoint 文件比该文件中的当前 ID 新，则 checkpoint 编写者会将其新 checkpoint  ID 写入该文件中。 请注意，由于云对象存储的最终一致性问题，_last_checkpoint 文件过时是可以的，因为客户端仍将在此文件中的 ID 之后搜索新的 checkpoint 。</p><h2 id="3-2-Access-Protocols"><a href="#3-2-Access-Protocols" class="headerlink" title="3.2 Access Protocols"></a>3.2 Access Protocols</h2><p>Delta Lake 的访问 protocol 旨在让客户端仅使用对象存储上的操作来实现可序列化的事务，尽管对象存储有最终的一致性保证。 使这成为可能的关键选择是日志记录对象，例如 000003.json，是客户端读取特定版本表所需的 “root” 数据结构。 给定这个对象的内容，客户端然后可以从对象存储中查询其他对象，如果由于最终的一致性延迟它们尚不可见，可能会等待，并读取表数据。 对于执行写入的事务，客户端需要一种方法来确保只有一个写入者可以创建下一条日志记录（例如，000003.json），然后可以使用它来实现乐观并发控制。</p><h3 id="3-2-1-Reading-from-Tables"><a href="#3-2-1-Reading-from-Tables" class="headerlink" title="3.2.1 Reading from Tables"></a>3.2.1 Reading from Tables</h3><p>我们首先描述如何对 Delta 表运行只读事务。 这些事务将安全地读取表的某些版本。 只读事务有五个步骤：</p><ol><li>读取表日志目录中的 _last_checkpoint 对象（如果存在）以获取最近的 checkpoint  ID。</li><li>使用 LIST 操作，其开始键是最后一个 checkpoint  ID（如果存在），否则为 0，在表的日志目录中查找任何更新的 .json 和 .parquet 文件。这提供了一个列表文件，可用于从最近的 checkpoint 开始重建表的状态。 （请注意，由于云对象存储的最终一致性，此 LIST 操作可能会返回一组不连续的对象，例如有 000004.json 和 000006.json 但没有 000005.json。尽管如此，客户端可以使用返回的最大 ID 作为要读取的目标表版本，并等待丢失的对象变为可见。）</li><li>使用上一步中确定的 checkpoint （如果存在）和后续日志记录来重建表的状态——即，具有添加记录但没有相应删除记录的数据对象集及其关联的数据统计信息。我们的格式旨在使此任务可以并行运行：例如，在我们的 Spark 连接器中，我们使用 Spark 作业读取 checkpoint  Parquet 文件和日志对象。</li><li>使用统计信息识别与读取查询相关的数据对象文件集。</li><li>查询对象存储以读取相关数据对象，可能跨集群并行。请注意，由于云对象存储的最终一致性，某些工作节点可能无法查询查询计划器在日志中找到的对象；这些可以在很短的时间后简单地重试。</li></ol><p>我们注意到该 protocol 旨在容忍每一步的最终一致性。 例如，如果客户端读取 _last_checkpoint 文件的旧版本，它仍然可以在后续 LIST 操作中发现更新的日志文件并重建表的最近快照。 _last_checkpoint 文件仅通过提供最近的 checkpoint  ID 来帮助降低 LIST 操作的成本。</p><p>同样，客户端可以容忍在列出最近的记录（例如，日志记录 ID 中的间隙）或读取日志中引用的数据对象时的不一致，这些对象在对象存储中可能尚不可见。</p><h3 id="3-2-2-Write-Transactions"><a href="#3-2-2-Write-Transactions" class="headerlink" title="3.2.2 Write Transactions"></a>3.2.2 Write Transactions</h3><p>写入数据的事务通常最多分五个步骤进行，具体取决于事务中的操作：</p><ol><li>使用读取 protocol 的第 1-2 步（即，从最后一个 checkpoint  ID 向前看）确定最近的日志记录 ID，例如 r。然后事务将读取表版本 r 的数据（如果需要）并尝试写入日志记录 r + 1。</li><li>如果需要，使用与读取 protocol 相同的步骤读取表版本 r 的数据（即，组合先前的 checkpoint 和任何进一步的日志记录，然后读取其中引用的数据对象）。</li><li>将事务旨在添加到表中的任何新数据对象写入正确数据目录中的新文件中，使用 GUID 生成对象名称。这一步可以并行发生。最后，这些对象已准备好在新的日志记录中引用。</li><li>如果没有其他客户端写入此对象，则尝试将事务的日志记录写入 r + 1 .json 日志对象。这一步需要是原子的，我们很快就会讨论如何在各种对象存储中实现这一点。如果步骤失败，可以重试事务；根据查询的语义，客户端还可以重用它在第 3 步中写入的新数据对象，并尝试将它们添加到新日志记录中的表中。</li><li>可选地，为日志记录 r + 1 写入一个新的 .parquet  checkpoint 。（实际上，我们的实现默认每 10 条记录执行一次。）然后，在写入完成后，更新 _last_checkpoint 文件以指向 checkpoint  r + 1.<br>请注意，第五步，即写入 checkpoint ，然后更新 _last_checkpoint 对象，只会影响性能，并且在此步骤中任何地方的客户端故障都不会破坏数据。例如，如果客户端写入 checkpoint 失败，或写入 checkpoint  Parquet 对象但未更新 _last_checkpoint，则其他客户端仍然可以使用较早的 checkpoint 读取该表。如果第 4 步成功，事务将自动提交。</li></ol><p>**Adding Log Records Atomically.**。在写入 protocol 中很明显，第 4 步，即创建 r + 1 .json 日志记录对象，需要是原子的：只有一个客户端应该成功创建具有该名称的对象。不幸的是，并不是所有的大型存储系统都有一个原子的 put-if-absent 操作，但是我们能够以不同的方式为不同的存储系统实现这个步骤：</p><ul><li>Google Cloud Storage 和Azure Blob Store 支持原子put-if-absent 操作，因此我们使用它们。</li><li>在分布式文件系统（如 HDFS）上，我们使用原子重命名将临时文件重命名为目标名称（例如 000004.json），如果它已经存在则失败。 Azure Data Lake Storage [18] 还提供了一个带有原子重命名的文件系统 API，所以我们在那里使用相同的方法</li><li>Amazon S3 没有原子“如果不存在则放置”或重命名操作。在 Databricks 服务部署中，我们使用单独的轻量级协调服务来确保只有一个客户端可以为每个日志 ID 添加一条记录。该服务仅用于日志写入（不是读取，也不是数据操作），因此其负载较低。在我们用于 Apache Spark 的开源 Delta Lake 连接器中，我们确保通过同一个 Spark 驱动程序（SparkContext 对象）的写入使用内存状态获得不同的日志记录 ID，这意味着用户仍然可以对 Delta 表进行并发操作在单个 Spark 集群中。我们还提供了一个 API 来插入自定义 LogStore 类，如果用户想要运行一个单独的、高度一致的存储，该类可以使用其他协调机制。</li></ul><h2 id="3-3-Available-Isolation-Levels"><a href="#3-3-Available-Isolation-Levels" class="headerlink" title="3.3 Available Isolation Levels"></a>3.3 Available Isolation Levels</h2><p>鉴于 Delta Lake 的并发控制 protocol ，<strong>所有执行写入的事务都是可序列化的，从而导致按日志记录 ID 递增顺序的串行调度</strong>。这遵循写入事务的提交 protocol ，其中只有一个事务可以使用每个记录 ID 写入记录。读取事务可以实现快照隔离或可序列化。我们在 3.2.1 节中描述的读取 protocol 只读取表的快照，因此利用该 protocol 的客户端将实现快照隔离，但希望运行可序列化读取（可能在其他可序列化事务之间）的客户端可以执行读取-write 事务执行虚拟写入以实现此目的。在实践中，Delta Lake 连接器实现还将他们为内存中的每个表访问过的最新日志记录 ID 缓存，因此即使客户端使用快照隔离进行读取，客户端也会“读取自己的写入”，并在读取时读取表版本的单调序列进行多次读取。</p><p>重要的是，Delta Lake 目前<strong>仅支持一张表内的事务</strong>。将来也可以扩展对象存储日志设计以管理同一日志中的多个表。</p><h2 id="3-4-Transaction-Rates"><a href="#3-4-Transaction-Rates" class="headerlink" title="3.4 Transaction Rates"></a>3.4 Transaction Rates</h2><p>Delta Lake 的写入事务率受到写入新日志记录的 put-if-absent 操作延迟的限制，如第 3.2.2 节所述。与任何乐观并发控制 protocol 一样，写入事务的高速率将导致提交失败。在实践中，写入对象存储的延迟可能是几十到几百毫秒，将写入事务的速率限制为每秒几个事务。然而，我们发现这个速率几乎适用于所有当前的 Delta Lake 应用程序：即使是将流数据摄取到云存储中的应用程序通常也有一些高度并行的作业（例如 Spark Streaming 作业）进行写入，这些作业可以将许多新数据对象批处理在一起一笔 transaction 。如果将来需要更高的速率，我们相信协调对日志的访问的自定义 LogStore，类似于我们的 S3 提交服务，可以提供明显更快的提交时间（例如，通过在低延迟 DBMS 并将其异步写入对象存储）。当然，快照隔离级别的读取事务不会产生争用，因为它们只读取对象存储中的对象，因此可以同时运行任意数量的事务。</p><h1 id="4-Higher-Level-features-in-Delta"><a href="#4-Higher-Level-features-in-Delta" class="headerlink" title="4. Higher-Level features in Delta"></a>4. Higher-Level features in Delta</h1><p>Delta Lake 的事务设计支持广泛的高级数据管理功能，类似于传统分析 DBMS 中的许多设施。 在本节中，我们将讨论一些最广泛使用的功能以及激发它们的客户用例或痛点。</p><h2 id="4-1-Time-Travel-and-Rollbacks"><a href="#4-1-Time-Travel-and-Rollbacks" class="headerlink" title="4.1 Time Travel and Rollbacks"></a>4.1 Time Travel and Rollbacks</h2><p>Data engineering pipelines 经常出错，尤其是在从外部系统摄取“脏”数据时，但在传统的数据湖设计中，很难撤消将对象添加到表中的更新。此外，一些工作负载，例如机器学习训练，需要忠实地再现旧版本的数据（例如，在相同数据上比较新旧训练算法）。这两个问题都给 Delta Lake 之前的 Databricks 用户带来了重大挑战，要求他们设计复杂的补救措施来解决 data pipeline 错误或复制数据集。</p><p>由于 Delta Lake 的数据对象和日志是不可变的，Delta Lake 可以直接查询数据的过去快照，就像在典型的 MVCC 实现中一样。客户端只需要根据旧的日志记录 ID 读取表状态。为了方便time travel ，Delta Lake 允许用户配置每个表的数据保留间隔，并支持 SQL AS OF 时间戳和 VERSION AS OF commit_id 语法来读取过去的快照。客户端还可以通过 Delta Lake 的 API 发现他们刚刚在操作中读取或写入的提交 ID。例如，我们在 MLflow 开源项目 [51] 中使用这个 API 来自动记录在 ML 训练工作负载期间读取的表版本。</p><p>用户发现time travel 对于修复 data pipeline 中的错误特别有用。例如，为了有效地撤消覆盖某些用户数据的更新，分析人员可以使用表的 SQL MERGE 语句针对其以前的版本，如下所示：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">MERGE</span> <span class="hljs-keyword">INTO</span> mytable target<br><span class="hljs-keyword">USING</span> mytable <span class="hljs-type">TIMESTAMP</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> <span class="hljs-operator">&lt;</span>old_date<span class="hljs-operator">&gt;</span> source <span class="hljs-keyword">ON</span> source.userId <span class="hljs-operator">=</span> target.userId<br><span class="hljs-keyword">WHEN</span> MATCHED <span class="hljs-keyword">THEN</span> <span class="hljs-keyword">UPDATE</span> <span class="hljs-keyword">SET</span> <span class="hljs-operator">*</span><br></code></pre></td></tr></table></figure><p>我们还开发了一个 CLONE 命令，该命令创建一个表的写时复制新版本，从其现有快照之一开始。</p><h2 id="4-2-Efficient-UPSERT-DELETE-and-MERGE"><a href="#4-2-Efficient-UPSERT-DELETE-and-MERGE" class="headerlink" title="4.2 Efficient UPSERT, DELETE and MERGE"></a>4.2 Efficient UPSERT, DELETE and MERGE</h2><p>企业中的许多分析数据集需要随着时间的推移进行修改。例如，为了遵守 GDPR [27] 等数据隐私法规，企业需要能够按需删除特定用户的数据。即使内部数据集与个人无关，由于上游数据收集中的错误或迟到的数据，旧记录可能需要更新。最后，计算聚合数据集（例如，业务分析师查询的表格摘要）的应用程序将需要随着时间的推移对其进行更新。</p><p>在传统的数据湖存储格式中，例如 S3 上的 Parquet 文件目录，很难在不停止并发读取器的情况下执行这些更新。即便如此，更新作业也必须小心执行，因为作业期间的失败将使表处于部分更新状态。使用 Delta Lake，所有这些操作都可以事务性地执行，通过在 Delta 日志中添加和删除新记录来替换任何更新的对象。 Delta Lake 支持标准的 SQL UPSERT、DELETE 和 MERGE 语法。</p><h2 id="4-3-Streaming-Ingest-and-Consumption"><a href="#4-3-Streaming-Ingest-and-Consumption" class="headerlink" title="4.3 Streaming Ingest and Consumption"></a>4.3 Streaming Ingest and Consumption</h2><p>许多数据团队希望将流式管道部署到 ETL 或实时聚合数据，但传统的云数据湖很难用于此目的。 因此，这些团队部署了单独的流消息总线，例如 Apache Kafka [11] 或 Kinesis [2]，这通常会复制数据并增加管理复杂性。<br>我们设计了 Delta Lake，这样表的日志可以帮助数据生产者和消费者将其视为消息队列，从而在许多场景中不需要单独的消息总线。 这种支持来自三个主要功能：</p><ul><li><strong>Write Compaction</strong>。组织为对象集合的简单数据湖可以轻松插入数据（只需写入新对象），但会在写入延迟和查询性能之间产生令人不快的折衷。如果写入者希望通过写入小对象快速将新记录添加到表中，由于较小的顺序读取和更多的 metadata 操作，读取器最终会变慢。相比之下，Delta Lake 允许用户运行后台进程，以事务方式压缩小数据对象，而不会影响读者。在第 3.1.2 节中描述的压缩文件的日志记录上将 dataChange 标志设置为 false 还允许流消费者在他们已经读取小对象的情况下完全忽略这些压缩操作。因此，流应用程序可以通过写入小对象来快速地相互传输数据，同时对旧数据的查询保持快速。</li><li><strong>Exactly-OnceStreamingWrites</strong>。 Writer 可以使用 3.1.2 节中描述的日志记录中的 txnaction 类型来跟踪他们将哪些数据写入 Delta Lake 表并实现“ exactly-once ”写入。通常，旨在更新外部存储中数据的流处理系统需要某种机制来使其写入具有幂等性，以避免在发生故障后重复写入。这可以通过确保每条记录在覆盖的情况下具有唯一键来完成，或者更一般地说，通过在每次写入时自动更新“上次写入的”记录，然后可以仅用于写入较新的更改。 Delta Lake 通过允许应用程序更新每个事务的 (appId, version) 对来促进后一种模式。我们在结构化流 [14] 连接器中使用此功能来支持任何类型的流计算（追加、聚合、更新插入等）的一次性写入。</li><li>**Efficient Log Tailing.**。使用 Delta Lake 表作为消息队列所需的最后一个工具是消费者有效查找新写入的机制。幸运的是，日志的存储格式是一系列具有字典序递增 ID 的 .json 对象，使这变得容易：消费者可以简单地从它看到的最后一个日志记录 ID 开始运行对象存储 LIST 操作，以发现新的.日志记录中的 dataChange 标志允许流消费者跳过仅压缩或重新排列现有数据的日志记录，而只读取新的数据对象。通过记住它完成处理的最后一个记录 ID，流应用程序也很容易在 Delta Lake 表中的同一日志记录处停止和重新启动。</li></ul><p>结合这三个特性，我们发现许多用户可以完全避免运行单独的消息总线系统，而使用低成本的云对象存储和 Delta 来实现延迟为秒级的流管道。</p><h2 id="4-4-Data-Layout-Optimization"><a href="#4-4-Data-Layout-Optimization" class="headerlink" title="4.4 Data Layout Optimization"></a>4.4 Data Layout Optimization</h2><p>数据布局对分析系统中的查询性能有很大影响，特别是因为许多分析查询是高度选择性的。由于 Delta Lake 可以事务性地更新表示表的数据结构，因此可以在不影响并发操作的情况下支持多种布局优化。例如，后台进程可以压缩数据对象，更改这些对象内的记录顺序，甚至更新辅助数据结构，例如数据统计和索引，而不会影响其他客户端。我们利用这个属性来实现一些数据布局优化功能：</p><p><strong>优化命令</strong>。用户可以在表上手动运行 OPTIMIZE 命令，在不影响正在进行的事务的情况下压缩小对象，并计算任何丢失的统计信息。默认情况下，此操作旨在使每个数据对象的大小为 1 GB，我们发现该值适合许多工作负载，但用户可以自定义此值。</p><p>**Z-Ordering by Multiple Attributes.**。许多数据集会根据多个属性接收高度选择性的查询。例如，我们使用的一个网络安全数据集将网络上发送的数据的信息存储在作为（sourceIp、destIp、time）元组中，每个维度都有高度选择性的查询。一个简单的目录分区方案，如 Apache Hive [45]，可以帮助在写入数据后通过几个属性对数据进行分区，但是当使用多个属性时，分区数量变得非常大。 Delta Lake 支持按照给定的属性集以 Z-order  [35] 重新组织表中的记录，以实现多个维度的高局部性。 Z 阶曲线是一种易于计算的空间填充曲线，可在所有指定维度上创建局部性。对于在实践中结合这些维度的查询工作负载，它可以显着提高性能，正如我们在第 6 节中展示的那样。用户可以在表上设置 Z-order 规范，然后运行 OPTIMIZE 来移动所需的数据子集（例如，只是最新的记录）沿着选定的属性到 Z 排序的对象中。用户也可以稍后更改顺序。</p><p>Z-ordering 与数据统计一起工作，让查询读取更少的数据。特别是，Z-ordering 将倾向于使每个数据对象在每个所选属性中包含一个小范围的可能值，以便在运行选择性查询时可以跳过更多的数据对象。</p><p><strong>AUTO OPTIMIZE</strong>.。在 Databricks 的云服务上，用户可以在表上设置 AUTO OPTIMIZE 属性，让服务自动压缩新写入的数据对象。</p><p>更一般地说，Delta Lake 的设计还允许在更新表时维护索引或计算成本高的统计信息。我们正在探索该领域的几个新功能。</p><h2 id="4-5-Caching"><a href="#4-5-Caching" class="headerlink" title="4.5 Caching"></a>4.5 Caching</h2><p>许多云用户为临时查询工作负载运行相对较长的集群，可能会根据他们的工作负载自动扩展和缩减集群。 在这些集群中，有机会通过在本地设备上缓存对象存储数据来加速对频繁访问的数据的查询。 例如，AWS i3 实例为每个内核提供 237 GB 的 NVMe SSD 存储，成本比相应的 m5（通用）实例高出约 50%。</p><p>在 Databricks，我们构建了一个功能来透明地在集群上缓存 Delta Lake 数据，通过缓存数据和日志对象来加速对这些表的数据和 metadata 查询。 缓存是安全的，因为 Delta Lake 表中的数据、日志和 checkpoint 对象是不可变的。 正如我们在第 6 节中展示的，从缓存中读取可以显着提高查询性能。</p><h2 id="4-6-Audit-Logging"><a href="#4-6-Audit-Logging" class="headerlink" title="4.6 Audit Logging"></a>4.6 Audit Logging</h2><p>Delta Lake 的事务日志也可以用于基于 commitInfo 记录的审计日志。 在 Databricks 上，我们为 Spark 集群提供了锁定执行模式，用户自定义函数不能直接访问云存储（或调用 Apache Spark 中的私有 API），这使我们能够确保只有运行时引擎才能写入 commitInfo 记录 ，并确保不可变的审计日志。 用户可以使用 DESCRIBE HISTORY 命令查看 Delta Lake 表的历史记录，如图 3 所示。 Delta Lake 的开源版本中也提供了提交信息日志记录。 审计日志是一种数据安全最佳实践，由于法规的原因，它对许多企业来说越来越具有强制性。</p><h2 id="4-7-Schema-Evolution-and-Enforcement"><a href="#4-7-Schema-Evolution-and-Enforcement" class="headerlink" title="4.7 Schema Evolution and Enforcement"></a>4.7 Schema Evolution and Enforcement</h2><p>长期维护的数据集通常需要模式更新，但将这些数据集存储为“只是一堆对象”意味着旧对象（例如，旧 Parquet 文件）可能具有“错误”模式。 Delta Lake 可以事务性地执行架构更改，并在需要时随架构更改一起更新底层对象（例如，删除用户不再希望保留的列）。 在事务日志中保留架构更新的历史记录还可以允许使用较旧的 Parquet 对象，而无需针对某些架构更改（例如，添加列）重写它们。 同样重要的是，Delta 客户端确保新写入的数据遵循表的架构。 这些简单的检查发现了许多用户错误，这些错误将数据附加到错误的模式中，当单个作业在使用 Delta Lake 之前只是将 Parquet 文件写入同一目录时，这些错误很难追踪。</p><h2 id="4-8-Connectors-to-Query-and-ETL-Engines"><a href="#4-8-Connectors-to-Query-and-ETL-Engines" class="headerlink" title="4.8 Connectors to Query and ETL Engines"></a>4.8 Connectors to Query and ETL Engines</h2><p>Delta Lake 使用 Apache Spark 的数据源 API [16] 为 Spark SQL 和结构化流提供了成熟的连接器。此外，它目前提供与其他几个系统的只读集成：Apache Hive、Presto、AWS Athena、AWS Redshift 和 Snowflake，使这些系统的用户能够使用熟悉的工具查询 Delta 表并将它们与这些系统中的数据连接起来。最后，包括 Fivetran、Informatica、Qlik 和 Talend 在内的 ETL 和变更数据捕获 (CDC) 工具可以写入 Delta Lake [33, 26]。</p><p>一些查询引擎集成使用一种特殊机制，最初用于 Hive 中的符号链接，称为符号链接清单文件。符号链接清单文件是对象存储或文件系统中的文本文件，其中包含应该在目录中可见的路径列表。各种兼容 Hive 的系统可以在读取目录时查找此类清单文件，通常命名为 _symlink_format_manifest，然后将清单文件中指定的路径视为目录的内容。在 Delta Lake 的上下文中，清单文件允许我们通过简单地创建一个列出这些对象的清单文件，将构成表格的 Parquet 数据对象的静态快照公开给支持这种输入格式的读者。该文件可以为每个目录自动写入，这意味着从非分区 Delta 表读取的系统会看到该表的完全一致的只读快照，而从分区表读取的系统会看到每个分区的一致快照目录。要为表生成清单文件，用户需要运行一个简单的 SQL 命令。然后，他们可以将数据作为外部表加载到 Presto、Athena、Redshift 或 Snowflake 中。<br>在其他情况下，例如 Apache Hive，开源社区使用可用的插件 API 设计了 Delta Lake 连接器。</p><h1 id="5-Delta-Lake-Use-Cases"><a href="#5-Delta-Lake-Use-Cases" class="headerlink" title="5. Delta Lake Use Cases"></a>5. Delta Lake Use Cases</h1><p>Delta Lake 目前在数以千计的 Databricks 客户以及开源社区中的其他组织中积极使用，每天处理 EB 级数据 [26]。 这些用例跨越各种数据源和应用程序。 Delta Lake 中存储的数据类型包括来自企业 OLTP 系统的变更数据捕获 (CDC) 日志、应用程序日志、时间序列数据、图表、用于报告的聚合表以及用于机器学习 (ML) 的图像或特征数据。 在这些数据上运行的应用程序包括 SQL 工作负载（最常见的应用程序类型）、商业智能、流媒体、数据科学、机器学习和图形分析。 Delta Lake 非常适合大多数使用 Parquet 或 ORC 等结构化存储格式的数据湖应用程序，以及许多传统的数据仓库工作负载。</p><p>在这些用例中，我们发现客户经常使用 Delta Lake 来简化他们的企业数据架构，方法是直接针对云对象存储运行更多工作负载并创建一个具有数据湖和 transaction 功能的“ lake house ”系统。例如，考虑一个典型的 data pipeline ，它从多个来源加载记录——例如，来自 OLTP 数据库的 CDC 日志和来自设施的传感器数据——然后将其传递给 ETL 步骤，使派生表可用于数据仓库和数据科学工作负载（如图 1）。传统的实现需要结合消息队列（如 Apache Kafka [11]），用于需要实时计算的任何结果，数据湖用于长期存储，以及数据仓库（如 Redshift [3]），用于需要实时计算的用户需要通过利用索引和快速节点附加存储设备（例如，SSD）来进行快速分析查询。这需要数据的多个副本并不断地将作业摄取到每个系统中。借助 Delta Lake，其中一些存储系统可以根据工作负载替换为对象存储表，利用 ACID 事务、流 I&#x2F;O 和 SSD 缓存等功能重新获得每个专用系统中的一些性能优化。虽然 Delta Lake 显然不能取代我们列出的系统中的所有功能，但我们发现在很多情况下它至少可以取代其中的一部分。 Delta 的连接器（第 4.8 节）还允许从许多现有引擎查询它。<br>在本节的其余部分，我们详细介绍了几个常见的用例。</p><h2 id="5-1-Data-Engineering-and-ETL"><a href="#5-1-Data-Engineering-and-ETL" class="headerlink" title="5.1 Data Engineering and ETL"></a>5.1 Data Engineering and ETL</h2><p>许多组织正在将 ETL&#x2F;ELT 和数据仓库工作负载迁移到云中以简化其管理，而其他组织正在使用来自其他来源的更大数据流来增强传统企业数据源（例如 OLTP 系统中的销售点事件） （例如，网络访问或库存跟踪系统）用于下游数据和机器学习应用程序。这些应用程序都需要一个可靠且易于维护的数据工程&#x2F;ETL 过程来为它们提供数据。当组织将他们的工作负载部署到云时，我们发现他们中的许多人更喜欢使用云对象存储作为着陆区（数据湖）以最大限度地降低存储成本，然后计算派生数据集并将其加载到更优化的数据仓库中系统（也许带有节点附加存储）。 Delta Lake 的 ACID 事务、UPSERT&#x2F;MERGE 支持和time travel 功能允许这些组织重用现有的 SQL 查询来直接在对象存储上执行他们的 ETL 过程，并利用熟悉的维护功能，如回滚、time travel 和审计日志。此外，使用单个存储系统 (Delta Lake) 而不是单独的数据湖和仓库，通过消除对单独摄取过程的需要，减少了使新数据可查询的延迟。最后，Delta Lake 对 SQL 和编程 API（通过 Apache Spark）的支持使得使用各种工具编写数据工程管道变得容易。<br>这种数据工程用例在我们遇到的几乎所有数据和机器学习工作负载中都很常见，涵盖金融服务、医疗保健和媒体等行业。在许多情况下，一旦他们的基本 ETL 管道完成，组织也会将他们的部分数据公开给新的工作负载，这些工作负载可以简单地运行在单独的集群上，使用 Delta Lake 访问相同的对象存储（例如，使用 PySpark 的数据科学工作负载）。其他组织使用 Spark 的结构化流（流 SQL）[14] 等工具将管道的一部分转换为流查询。这些其他工作负载可以轻松地在新的云虚拟机上运行并访问相同的表。</p><h2 id="5-2-Data-Warehousing-and-BI"><a href="#5-2-Data-Warehousing-and-BI" class="headerlink" title="5.2 Data Warehousing and BI"></a>5.2 Data Warehousing and BI</h2><p>传统数据仓库系统将 ETL&#x2F;ELT 功能与高效工具相结合，以查询生成的表，以启用交互式查询工作负载，例如商业智能 (BI)。支持这些工作负载的关键技术特性通常是高效的存储格式（例如列格式）、数据访问优化（如集群和索引）、快速存储硬件以及适当优化的查询引擎 [43]。 Delta Lake 可以直接为云对象存储中的表支持所有这些功能，通过组合列格式、数据布局优化、最大-最小统计和 SSD 缓存，所有这些功能都可以通过其事务设计可靠地实现。因此，我们发现大多数 Delta Lake 用户还直接通过 SQL 或通过 Tableau 等 BI 软件针对他们的 Lakehouse 数据集运行临时查询和 BI 工作负载。这个用例非常普遍，以至于 Databricks 为 BI 工作负载开发了一个新的矢量化执行引擎 [21]，并对其 Spark 运行时进行了优化。与 ETL 工作负载的情况一样，直接在 Delta Lake 上运行 BI 的优势之一是更容易为分析师提供新数据进行工作，因为数据不需要加载到单独的系统中。</p><h2 id="5-3-Compliance-and-Reproducibility"><a href="#5-3-Compliance-and-Reproducibility" class="headerlink" title="5.3 Compliance and Reproducibility"></a>5.3 Compliance and Reproducibility</h2><p>传统的数据湖存储格式主要是为不可变数据而设计的，但是欧盟的 GDPR [27] 等新的数据隐私法规以及行业最佳实践要求组织有一种有效的方法来删除或更正有关个人用户的数据。 我们已经看到多个行业的组织将现有的云数据集转换为 Delta Lake，以使用其高效的 UPSERT、MERGE 和 DELETE 功能。 用户还利用审计日志功能（第 4.6 节）进行数据治理。<br>Delta Lake 的time travel 支持对于可重复的数据科学和机器学习也很有用。 我们将 Delta Lake 与 MLflow [51]（Databricks 开发的开源模型管理平台）集成，以自动记录使用哪个版本的数据集来训练 ML 模型并让开发人员重新加载它。</p><h2 id="5-4-Specialized-Use-Cases"><a href="#5-4-Specialized-Use-Cases" class="headerlink" title="5.4 Specialized Use Cases"></a>5.4 Specialized Use Cases</h2><h3 id="5-4-1-Computer-System-Event-Data"><a href="#5-4-1-Computer-System-Event-Data" class="headerlink" title="5.4.1 Computer System Event Data"></a>5.4.1 Computer System Event Data</h3><p>我们见过的最大的单一用例之一是将 Delta Lake 部署为一家大型技术公司的安全信息和事件管理 (SIEM) 平台。该组织将整个公司范围内的各种计算机系统事件（例如网络上的 TCP 和 UDP 流、身份验证请求、SSH 登录等）记录到一组集中的 Delta Lake 表中，这些表可以跨越 PB 级。然后针对这些表运行多个编程 ETL、SQL、图形分析和机器学习作业，以搜索指示入侵的已知模式（例如，来自用户的可疑登录事件，或导出大量数据的一组服务器）。其中许多是流式作业，以最大限度地减少检测问题的时间。此外，超过 100 名分析师直接查询源和派生的 Delta Lake 表，以调查可疑警报或设计新的自动化监控作业。<br>这个信息安全用例很有趣，因为它很容易自动收集大量数据（在此部署中每天数百 TB），因为数据必须保留很长时间，以便对新发现的入侵进行取证分析（有时几个月后），并且因为数据需要沿多个维度进行查询。例如，如果分析人员发现某个特定服务器曾经被入侵，她可能希望通过源 IP 地址（以查看攻击者从那里访问的其他服务器）、目标 IP 地址（以查看如何攻击者登录到原始服务器）、时间和任意数量的其他维度（例如，该攻击者获得的员工访问令牌）。为这些多 PB 数据集维护重量级索引结构将非常昂贵，因此该组织使用 Delta Lake 的 ZORDER BY 功能来重新排列 Parquet 对象内的记录，以提供跨多个维度的聚类。因为沿着这些维度的取证查询是高度选择性的（例如，从数百万中寻找一个 IP 地址），Z-ordering 与 Delta Lake 最小&#x2F;最大基于统计的跳过很好地结合在一起，以显着减少每个查询具有的对象数量阅读。尽管数百名开发人员在 data pipeline 上进行协作，但 Delta Lake 的 AUTO OPTIMIZE 功能、time travel 和 ACID 事务也在保持这些数据集的正确性和快速访问方面发挥了重要作用。</p><h3 id="5-4-2-Bioinformatics"><a href="#5-4-2-Bioinformatics" class="headerlink" title="5.4.2 Bioinformatics"></a>5.4.2 Bioinformatics</h3><p>生物信息学是我们看到 Delta Lake 广泛用于管理机器生成数据的另一个领域。许多数据源，包括 DNA 测序、RNA 测序、电子病历和医疗设备的时间序列，使生物医学公司能够收集有关患者和疾病的详细信息。这些数据源通常与公共数据集相结合，例如英国生物银行 [44]，其中包含 500,000 个人的测序信息和医疗记录。<br>尽管传统的生物信息学工具使用了自定义数据格式，例如 SAM、BAM 和 VCF [34, 24]，但许多组织现在正在将这些数据存储在数据湖格式中，例如 Parquet。大数据基因组学项目 [37] 开创了这种方法。 Delta Lake 通过支持快速多维查询（通过 Z 排序）、ACID 事务以及高效的 UPSERT 和 MERGE，进一步增强了生物信息学工作负载。在某些情况下，这些功能比以前的 Parquet 实现提高了 100 倍以上。 2019 年，Databricks 和 Regeneron 发布了 Glow [28]，这是一个使用 Delta 进行存储的基因组数据开源工具包。</p><h3 id="5-4-3-Media-Datasets-for-Machine-Learning"><a href="#5-4-3-Media-Datasets-for-Machine-Learning" class="headerlink" title="5.4.3 Media Datasets for Machine Learning"></a>5.4.3 Media Datasets for Machine Learning</h3><p>我们看到的一个更令人惊讶的应用程序是使用 Delta Lake 来管理多媒体数据集，例如上传到需要用于机器学习的网站的一组图像。 尽管图像和其他媒体文件已经以高效的二进制格式编码，但将这些数据集作为云对象存储中数百万个对象的集合进行管理是具有挑战性的，因为每个对象的大小只有几千字节。 对象存储 LIST 操作可能需要几分钟才能运行，而且很难并行读取足够多的对象来提供在 GPU 上运行的机器学习推理作业。 我们已经看到多个组织将这些媒体文件作为 BINARY 记录存储在 Delta 表中，并利用 Delta 进行更快的推理查询、流处理和 ACID 事务。 例如，领先的电子商务和旅游公司正在使用这种方法来管理数百万用户上传的图像。</p><h1 id="6-PERFORMANCE-EXPERIMENTS"><a href="#6-PERFORMANCE-EXPERIMENTS" class="headerlink" title="6. PERFORMANCE EXPERIMENTS"></a>6. PERFORMANCE EXPERIMENTS</h1><p>…</p><h1 id="7-DISCUSSION-AND-LIMITATIONS"><a href="#7-DISCUSSION-AND-LIMITATIONS" class="headerlink" title="7. DISCUSSION AND LIMITATIONS"></a>7. DISCUSSION AND LIMITATIONS</h1><p>我们在 Delta Lake 的经验表明，ACID 事务可以通过云对象存储实现，用于许多企业数据处理工作负载，并且它们可以支持大规模流、批处理和交互式工作负载。 Delta Lake 的设计特别有吸引力，因为它不需要任何其他重量级系统来调解对云存储的访问，这使得部署和从支持 Parquet 的各种查询引擎直接访问变得微不足道。 Delta Lake 对 ACID 的支持可以实现其他强大的性能和管理功能。<br>尽管如此，Delta Lake 的设计和当前的实现有一些限制，这些限制是未来工作的有趣途径。首先，Delta Lake 目前只提供单表内可序列化的事务，因为每张表都有自己的事务日志。跨多个表共享事务日志将消除此限制，但可能会增加通过乐观并发追加日志记录的争用。对于非常大的事务量，协调器还可以调解对日志的写入访问，而无需成为数据对象的读写路径的一部分。其次，对于流式工作负载，Delta Lake 受到底层云对象存储延迟的限制。例如，使用对象存储操作很难实现毫秒级的流延迟。但是，我们发现，对于用户希望运行并行作业的大型企业工作负载，使用 Delta Lake 表的几秒级延迟是可以接受的。第三，Delta Lake 目前不支持二级索引（每个数据对象的 min-max 统计除外），但我们已经开始对基于 Bloom 过滤器的索引进行原型设计。 Delta 的 ACID 事务允许我们通过更改基本数据以事务方式更新此类索引。</p><h1 id="8-Related-Work"><a href="#8-Related-Work" class="headerlink" title="8. Related Work"></a>8. Related Work</h1><p>多个研究和行业项目试图使数据管理系统适应云环境。例如，布兰特纳等人。探索在 S3 上构建 OLTP 数据库系统 [20]；附加一致性 [19] 在最终一致的键值存储之上实现因果一致性； AWS Aurora [49] 是一个商业 OLTP DBMS，具有单独扩展的计算和存储层；和 Google BigQuery [29]、AWS Redshift Spectrum [39] 和 Snowflake [23] 是 OLAP DBMS，它们可以独立于存储扩展计算集群，并且可以从云对象存储中读取数据。其他工作，例如关系云项目 [22]，考虑如何自动调整 DBMS 引擎以适应弹性、多租户工作负载。<br>Delta Lake 分享了这些作品的愿景，即利用广泛可用的云基础设施，但针对一组不同的要求。具体来说，大多数以前的 DBMS-on-cloud-storage 系统都需要 DBMS 来调解客户端和存储之间的交互（例如，通过让客户端连接到 Aurora 或 Redshift 前端服务器）。这会产生额外的操作负担（前端节点必须始终运行），以及在通过前端节点流式传输大量数据时可能出现的可扩展性、可用性或成本问题。相比之下，我们设计了 Delta Lake 以便许多独立运行的客户端可以通过云对象存储操作直接协调对表的访问，在大多数情况下不需要单独运行的服务（除了 S3 上日志记录 ID 的轻量级协调器，如上所述在第 3.2.2 节）。这种设计使 Delta Lake 对用户来说操作简单，并确保以与底层对象存储相同的成本进行高度可扩展的读取和写入。此外，该系统与底层云对象存储一样具有高可用性：无需加固或重新启动其他组件即可进行灾难恢复。当然，由于 Delta Lake 目标工作负载的性质，这种设计在这里是可行的：每秒写入事务相对较少但事务大小较大的 OLAP 工作负载，这与我们的乐观并发方法配合得很好。<br>与 Delta Lake 的设计和目标最接近的系统是 Apache Hudi [8] 和 Apache Iceberg [10]，它们都定义了数据格式和访问 protocol ，以在云对象存储上实现事务性操作。这些系统是与 Delta Lake 同时开发的，并不提供其所有功能。例如，这两个系统都没有提供数据布局优化，例如 Delta Lake 的 ZORDER BY（第 4.4 节），应用程序可以使用流输入源有效扫描添加到表中的新记录（第 4.3 节），或支持本地缓存，如Databricks 服务（第 4.5 节）。此外，Apache Hudi 一次仅支持一个写入器（但支持多个读取器）[9]。这两个项目都提供连接到开源引擎（包括 Spark 和 Presto）的连接器，但缺乏连接到商业数据仓库（如 Redshift 和 Snowflake，我们使用清单文件（第 4.8 节）实现）和商业 ETL 工具的连接器。<br>Apache Hive ACID [32] 也通过对象存储或分布式文件系统实现事务，但它依赖于 Hive 元存储（在 OLTP DBMS 中运行）来跟踪每个表的状态。这会在具有数百万个分区的表中造成瓶颈，并增加用户的操作负担。 Hive ACID 也缺乏对time travel 的支持（第 4.1 节）。 HDFS 上的低延迟存储，例如 HBase [7] 和 Kudu [6]，也可以在写入 HDFS 之前组合少量写入，但需要运行单独的分布式系统。<br>将高性能事务处理和分析处理结合起来还有很长的工作要做，例如 C-Store [43] 和 HTAP 系统。这些系统通常有一个单独的、针对 OLTP 优化的可写存储和一个针对分析优化的长期存储。在我们的工作中，我们试图通过设计直接针对对象存储的并发 protocol 来支持适度的事务率，而无需运行单独的高可用写入存储。</p><h1 id="9-Conclusion"><a href="#9-Conclusion" class="headerlink" title="9. Conclusion"></a>9. Conclusion</h1><p>我们已经介绍了 Delta Lake，这是一个基于云对象存储的 ACID 表存储层，它为低成本云存储中的数据提供了广泛的类似于 DBMS 的性能和管理功能。 Delta Lake 仅作为一种存储格式和一组客户端访问 protocol 实现，使其操作简单且高度可用，并为客户端提供对对象存储的直接、高带宽访问。 Delta Lake 被数以千计的组织用于每天处理 EB 级的数据，通常会取代涉及多个数据管理系统的更复杂的架构。 它在 <a href="https://delta.io/">https://delta.io</a> 上的 Apache 2 许可下是开源的。</p><h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><ul><li>paper <a href="https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf">https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf</a></li><li>GitHub repo <a href="https://github.com/delta-io/delta">https://github.com/delta-io/delta</a></li><li>databricks 的 video，介绍 delta lake 里的操作<a href="https://databricks.com/discover/diving-into-delta-lake-talks/unpacking-transaction-log">https://databricks.com/discover/diving-into-delta-lake-talks/unpacking-transaction-log</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020总结</title>
    <link href="/2020/12/30/2020%E6%80%BB%E7%BB%93/"/>
    <url>/2020/12/30/2020%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>在北京的寒风里，这个世界的2020马上结束了，我的2020也即将结束。</p><span id="more"></span><p>2020 真是魔幻的一年啊，对我的人生而言也是很重要的一年，为了不让我的人生糊里糊涂的就这么过去，此刻的我，坐在地毯上，用文字记录下我的2020。</p><p>真像一场梦啊。</p><p>回忆去年的年尾，我在在哪里呢？去年这个时候我的刚刚结束紧张的考研，和女朋友自己做了一顿冬天里的火锅，随后去了合肥和曾碧飞哥一起，听陈绮贞的演唱会跨年。当时记得在地铁上，听说武汉出现了不明的肺炎，当时还不以为然，总觉得疫情、天灾人祸距离自己很远，没想到这就是魔幻的2020的开头。</p><p>一月回家之后，怎么也没想到疫情发展的这么严重，身处疫情最严重的的湖北，当时草木皆兵的情景仍然历历在目。村与村之间都隔离起来了，县城里也出现了几例病情，每天和父母在家，吃着早已经囤好的年货和家里种的菜，现在回忆起来竟然还觉得不错。过年那段时间，由于父亲闲不住的性格，于是他和我在院子里新修了一个花坛。当时还记得有邻居调侃，这应该是今后我在家和父母在一起呆的最长一段时间了，真的是这样吗？直到自己现在又开始求学，才意识到这也许就是真的。疫情刚刚一结束，全家就去了几个姨妈家，大家团聚的时候才真正感觉到亲人的珍贵啊，人活着努力奋斗不都就是为了自己和家人吗？</p><p>在家呆着一直没剃胡子，直到考研出分的那一天，自己的成绩中规中矩，还算不错，于是开始联系老师面试才刮去了胡子。学校一直拖到了最后才出面试时间，中途复习面试也真的是煎熬啊，每天不敢不复习，复习却又找不到重点，到处都是重点，所幸最后取得了还不错的成绩，成功上岸。</p><p>上岸之后开始准备毕设的事情，这是今年另外一个最大的挫折吧。对于大多数人来说，毕设可能是一个没花多少时间和心思的事情，我和另外几个同学却因为导师给我们选题的问题，导致我们组的所有人答辩都被怼的一无是处，但是本科毕业设计的定位，我们最终都都还是顺利毕业了。</p><p>感谢西安的疫情防控一直做得很好，我们都能够回到学校顺利参加毕业典礼，也能够拍摄我们的毕业照以及大家做一个告别。我的大学生涯就这么结束了！把握好自己拥有的所有吧！毕业之后又在西安呆了一个月，和女朋友一起还体验了一波有猫有狗的日子。</p><p>回家之前又去了恩施，没想到又遇到了百年难遇的大洪水，在恩施困了好几天。回到宜昌之后，就做了手术把陪伴自己二十年的头顶上的包切除了，剃了自己有记忆以来的第一个光头，还顶着绷带过了好一段时间。怎么也想不到，暑假学车的一个月就把自己大学四年养成的有些许进步的稍微白点的皮肤又晒回去了，哎，这真是最大的一个失误。</p><p>来北京了！九月中旬开学，和父母第一次出远门，这次也是我带着他们在北京的几个大景点草草的转了一圈，时间太短玩的不够尽兴，希望毕业之后能够有钱有时间带着父母再来好好玩一次。于是就开学了，非常幸运的是，学校的环境非常舒适，很符合自己的预期，宿舍能够洗澡了，宿舍的舍友也很好，老师也很好，达到了我对于研究生生活的期待。来到北京之后，和高中同学见了一面，高中啊，那能够算是人生开始改变的起点吗？BUAA 6系的强人的比例确实比本科的时候上升了，这是我比较高兴的一个点，但是研究生的课程却没有超出我的预期，还是中规中矩的很多课程，所以我也和本科一样，继续的水过这些课程。</p><p>女朋友今年也很顺利，顺利的出分开始申请学校，还找到了一份大厂的非常好的实习，这也是终于她对自己能力的认可了吧！希望以后能和她一起加油，继续为未来奋斗。</p><p>记录是为了不遗忘。希望自己的2021可以有更多突破自己的点，能够更加努力成为更好的自己，能够顺利的找到实习，在技术方面有更多的提升，写作水平更进一步。</p><p>就这样吧！再见了2020！</p>]]></content>
    
    
    <categories>
      
      <category>年度总结</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>为什么重开一个博客</title>
    <link href="/2020/12/10/%E9%87%8D%E5%90%AF%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/12/10/%E9%87%8D%E5%90%AF%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>兜兜转转，竟然又开始写博客了…之前的那个博客不太好看，也记录了太多乱七八糟的东西，所以索性直接放弃，重新维护一个新的博客；另外一个原因，之前在博客园也尝试记录过博客（那个竟然有几万的阅读量…），但是最终都没有坚持下来，而且博客园感觉是第三方平台，就没有太自由的感觉，所以重新，整一波博客。</p><p>为什么要写博客呢？写博客主要是给自己记录笔记，之前想着每次写博客都觉得比较烦，因为得把一个事情的来龙去脉全部都讲清楚，而那些细节我可能已经很清楚了。所以这次开始重新写博客，以记录为主！好好写给自己看，就当写在 Notion 里面的笔记放到博客了，</p><p>最近事情比较多，希望能坚持下来，就每天晚上学不动干不动的时候记录博客吧！</p>]]></content>
    
    
    <categories>
      
      <category>记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
